{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a7c0c5",
   "metadata": {},
   "source": [
    "# EfficientNet with Squeeze Excitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd871ce6",
   "metadata": {},
   "source": [
    "In this model, our team builds of the efficientNet infrastructure, where we have introduced a Squeeze Excitation block in the classifier head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65feca1",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Used to handle the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b45d0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-20 10:30:48,569\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-20 10:30:48,697\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Callable\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b5, EfficientNet_B5_Weights\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from utils.DataHandler import download_dataset, create_full_data_loaders, create_tuning_data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804ebec",
   "metadata": {},
   "source": [
    "## Loading Dataset and File Location Variables\n",
    "All code that handles renaming of file location save files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffb1e0",
   "metadata": {},
   "source": [
    "Ability to set the following variables\n",
    "- **model_name**: Name the current model that is being created\n",
    "- **batch_size**: The size of each batch of data\n",
    "- **root**: Declare the root folder of the project (should usually be ..)\n",
    "- **model_directory**: Declare the folder that models will be saved in (should be standardised to models)\n",
    "- **dataset_folder**: Location for the dataset folder (should be \"../../input/final_split_15Apr2025\")\n",
    "\n",
    "\n",
    "\n",
    "Which creates the following global variables\n",
    "- PROJECT_ROOT:\n",
    "- LOG_DIR:\n",
    "- MODEL_SAVE_DIR:\n",
    "- DEVICE:\n",
    "- SPLIT_DATASET:\n",
    "- BATCH_SIZE: \n",
    "- TRAIN_LOADER, VAL_LOADER, TEST_LOADER:\n",
    "- FULL_TRAIN_LOADER, FULL_VAL_LOADER, FULL_TEST_LOADER: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21489441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variables to control where files are saved\n",
    "\n",
    "model_name = \"ENSE\" # Name of the model that the file is training in abbreviation\n",
    "batch_size = 64 # State the number of datapoints in each batch size\n",
    "root = \"..\" # Os path to the root of the project\n",
    "model_directory = \"models\" # Name of the folder where the models will be stored\n",
    "dataset_folder = \"../input/final_split_15Apr2025\" # Abs Location of the folder with the data split into train eval test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35721868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA (GPU)\n"
     ]
    }
   ],
   "source": [
    "# Determine the project root - required to import DataHandler from utils folder\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), root))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Configuring log file\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "log_filename = os.path.join(LOG_DIR, F\"{model_name}_training_log.txt\")\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    filemode=\"w\",  #NOTE: previous logs would be overwritten\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Creating the models save location\n",
    "MODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, model_directory, model_name)\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, F\"{model_directory}/hyptune/{model_name}\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, F\"{model_name}_best_model.pt\")\n",
    "\n",
    "# Gets the device to be used\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using device: CUDA (GPU)\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using device: MPS (Apple Silicon GPU)\")\n",
    "        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"Using device: CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d100d88",
   "metadata": {},
   "source": [
    "Loading the datasets for both hyperparameter tuning as well as actual trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9555097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-split datasets: train 8025, val 579, test 572\n",
      "Created tuning data loaders with subset fraction: 0.5\n",
      "Created subset datasets for hyperparameter tuning: train 4012, val 289, test 572\n",
      "Class Distribution for Subset Training:\n",
      "  angry     : 983\n",
      "  happy     : 1039\n",
      "  relaxed   : 1024\n",
      "  sad       : 966\n",
      "Class Distribution for Subset Validation:\n",
      "  angry     : 71\n",
      "  happy     : 75\n",
      "  relaxed   : 74\n",
      "  sad       : 69\n",
      "Using pre-split datasets: train 8025, val 579, test 572\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATASET = os.path.abspath(dataset_folder)\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "# Define data transformation \n",
    "# in this notebook, we are doing it for EfficientNetB5, so we resize to ~456x456\n",
    "weights = EfficientNet_B5_Weights.DEFAULT\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((456, 456)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    weights.transforms()\n",
    "])\n",
    "\n",
    "# Creating dataloader to load 50% of the dataset with the same proportions used for hyper parameter tuning\n",
    "TRAIN_LOADER, VAL_LOADER, TEST_LOADER = create_tuning_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset_fraction=0.5,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Creating dataloader to load the full dataset for training the actual models and testing\n",
    "FULL_TRAIN_LOADER, FULL_VAL_LOADER, FULL_TEST_LOADER = create_full_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9eb0a",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "This part of the file will change largely based on each of the models that are being created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0105ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation (SE) block for channel-wise feature recalibration.\n",
    "\n",
    "    This block implements the SE mechanism which adaptively recalibrates \n",
    "    channel-wise feature responses by explicitly modeling interdependencies \n",
    "    between channels.\n",
    "\n",
    "    Args:\n",
    "        channels (int): Number of input and output channels.\n",
    "        reduction_ratio (int): Reduction ratio for the intermediate hidden layer. \n",
    "            Controls the bottleneck in the SE block (default: 16).\n",
    "\n",
    "    Forward Input:\n",
    "        x (Tensor): Input feature map of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor: Output feature map of the same shape, with recalibrated channel responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, reduction_ratio=16):\n",
    "        super(SqueezeExcitationBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.interaction = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction_ratio, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction_ratio, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.interaction(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class BaseEfficientNetB5(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNetB5-based classifier with optional Squeeze-and-Excitation (SE) block, \n",
    "    customizable classifier head, and partial layer unfreezing for fine-tuning.\n",
    "\n",
    "    Designed for flexibility during hyperparameter tuning (e.g., with Ray Tune).\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes. Default is 4.\n",
    "        dropout (float): Dropout probability used in the classifier head. Default is 0.2.\n",
    "        hidden_sizes (Optional[List[int]]): List of hidden layer sizes for the classifier head. \n",
    "            If None, no additional hidden layers are added beyond the SE block (if used).\n",
    "        activation (str): Activation function to use in the classifier head. \n",
    "            Supported values: 'relu', 'tanh', 'sigmoid'. Default is 'relu'.\n",
    "        use_se (bool): Whether to include a Squeeze-and-Excitation (SE) block before the classifier. Default is True.\n",
    "        unfreeze_blocks (Optional[List[int]]): List of block indices in EfficientNetB5's features to unfreeze \n",
    "            for fine-tuning. If None, the entire backbone remains frozen.\n",
    "\n",
    "    Notes:\n",
    "        - The SE block is applied to the flattened output of the backbone if `use_se` is True.\n",
    "        - The classifier head is built dynamically based on `hidden_sizes` and activation.\n",
    "        - The EfficientNetB5 backbone is loaded with pretrained weights by default.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes: int = 4,\n",
    "                 dropout: float = 0.2,\n",
    "                 hidden_sizes: Optional[List[int]] = None,\n",
    "                 activation: str = 'relu',\n",
    "                 use_se: bool = True,\n",
    "                 unfreeze_blocks: Optional[List[int]] = None) -> None:\n",
    "        super(BaseEfficientNetB5, self).__init__()\n",
    "        weights = EfficientNet_B5_Weights.DEFAULT\n",
    "        self.backbone = efficientnet_b5(weights=weights)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "\n",
    "        # Setting the values layers in the efficientnet backbone that will be frozen\n",
    "        for param in self.backbone.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if unfreeze_blocks:\n",
    "            for idx in unfreeze_blocks:\n",
    "                for param in self.backbone.features[idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Build the classifier head with the SE and additional layers according to the number of layers in the hidden state\n",
    "        layers = []\n",
    "        if use_se:\n",
    "            layers.extend([\n",
    "                nn.Unflatten(1, (in_features, 1, 1)),\n",
    "                SqueezeExcitationBlock(in_features),\n",
    "                nn.Flatten()\n",
    "            ])\n",
    "        \n",
    "        input_dim = in_features\n",
    "        if hidden_sizes:\n",
    "            for hidden_dim in hidden_sizes:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(self._get_activation(activation))\n",
    "                input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Dropout(p=dropout))\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "\n",
    "        self.backbone.classifier[1] = nn.Sequential(*layers)\n",
    "\n",
    "    def _get_activation(self, activation: str) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns the specified activation function from PyTorch's nn module.\n",
    "\n",
    "        Args:\n",
    "            activation (str): Name of the activation function. Supported values are\n",
    "                'relu', 'tanh', and 'sigmoid' (case-insensitive).\n",
    "\n",
    "        Returns:\n",
    "            Callable: The corresponding activation function module (e.g., nn.ReLU()).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unsupported activation function name is provided.\n",
    "        \"\"\"\n",
    "        if activation.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation.lower() == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb027fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model instantiated: BaseEfficientNetB5\n",
      "BaseEfficientNetB5(\n",
      "  (backbone): EfficientNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "              (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.005128205128205128, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.010256410256410256, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.015384615384615387, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.020512820512820513, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.02564102564102564, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.030769230769230774, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0358974358974359, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.041025641025641026, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.046153846153846156, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05128205128205128, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05641025641025642, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06153846153846155, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06666666666666667, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0717948717948718, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08205128205128205, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08717948717948719, mode=row)\n",
      "        )\n",
      "        (5): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09230769230769231, mode=row)\n",
      "        )\n",
      "        (6): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09743589743589744, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.10256410256410256, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.11282051282051284, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.11794871794871796, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1230769230769231, mode=row)\n",
      "        )\n",
      "        (5): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1282051282051282, mode=row)\n",
      "        )\n",
      "        (6): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.13333333333333333, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1435897435897436, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14871794871794874, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.158974358974359, mode=row)\n",
      "        )\n",
      "        (5): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1641025641025641, mode=row)\n",
      "        )\n",
      "        (6): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
      "        )\n",
      "        (7): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.17435897435897438, mode=row)\n",
      "        )\n",
      "        (8): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1794871794871795, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18461538461538463, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18974358974358976, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.19487179487179487, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2dNormActivation(\n",
      "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.4, inplace=True)\n",
      "      (1): Sequential(\n",
      "        (0): Unflatten(dim=1, unflattened_size=(2048, 1, 1))\n",
      "        (1): SqueezeExcitationBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (interaction): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=128, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=128, out_features=2048, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (2): Flatten(start_dim=1, end_dim=-1)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): Dropout(p=0.3, inplace=False)\n",
      "        (7): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (8): ReLU()\n",
      "        (9): Dropout(p=0.3, inplace=False)\n",
      "        (10): Linear(in_features=128, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "model = BaseEfficientNetB5(num_classes=4, dropout=0.3, hidden_sizes=[256, 128], activation='relu', unfreeze_blocks=[7]).to(DEVICE)\n",
    "print(\"Model instantiated:\", model.__class__.__name__)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e203777",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ba66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "\n",
    "def train_model(config, checkpoint_dir=CHECKPOINT_DIR, data_dir=None):\n",
    "    \"\"\"Training function for Ray Tune hyperparameter tuning.\n",
    "\n",
    "    This function instantiates the model with hyperparameters\n",
    "    specified in the config dictionary, trains the model on the global TRAIN_LOADER,\n",
    "    evaluates on VAL_LOADER, and reports the validation loss to Ray Tune.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Hyperparameter configuration. Expected keys include:\n",
    "            - lr (float): Learning rate.\n",
    "            - weight_decay (float): Weight decay for the optimizer.\n",
    "            - dropout (float): Dropout rate for the classifier.\n",
    "            - hidden_sizes (list or None): List of hidden layer sizes in the classifier.\n",
    "            - activation (str): Activation function to use ('relu', 'tanh', etc.).\n",
    "            - freeze_backbone (bool): Whether to freeze the model backbone.\n",
    "            - num_epochs (int): Number of training epochs.\n",
    "            - optimiser (callable, optional): Optimiser class. Default is optim.Adam.\n",
    "            - criterion (callable, optional): Loss function instance. Default is nn.CrossEntropyLoss().\n",
    "        checkpoint_dir (str, optional): Directory for checkpointing (if applicable).\n",
    "        data_dir (str, optional): Not used here; included for compatibility.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Checkpoint Folder exists\")\n",
    "    \n",
    "    # instantiate model with hyperparameters from config\n",
    "    model = BaseEfficientNetB5(\n",
    "        num_classes=4,\n",
    "        dropout=config.get(\"dropout\", 0.3),\n",
    "        hidden_sizes=config.get(\"hidden_sizes\", None),\n",
    "        activation=config.get(\"activation\", \"relu\"),\n",
    "        unfreeze_blocks = config.get(\"unfreeze_blocks\", [7])\n",
    "    ).to(device)\n",
    "\n",
    "    optimiser = config[\"optimiser\"](model.parameters(), config[\"lr\"], config[\"weight_decay\"])\n",
    "    criterion = config[\"criterion\"]()\n",
    "\n",
    "    num_epochs = config.get(\"num_epochs\", 2)  # a low number for quick tuning, but update accordingly\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in tqdm(TRAIN_LOADER, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(TRAIN_LOADER.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # optionally, can checkpoint the model\n",
    "        if checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "    \n",
    "    # evaluation on the subset validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in VAL_LOADER:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_val_loss = total_loss / len(VAL_LOADER.dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Report the metric to Ray Tune\n",
    "    tune.report({\"loss\": avg_val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f8fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-20 10:34:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:50.61        </td></tr>\n",
       "<tr><td>Memory:      </td><td>12.1/31.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Logical resource usage: 12.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name  </th><th>status  </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  dropout</th><th>hidden_sizes  </th><th style=\"text-align: right;\">         lr</th><th>optimiser           </th><th>unfreeze_blocks  </th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>t_bd650a46  </td><td>RUNNING </td><td>127.0.0.1:24276</td><td>tanh        </td><td style=\"text-align: right;\"> 0.40464 </td><td>[256, 128]    </td><td style=\"text-align: right;\">1.82723e-05</td><td>&lt;function &lt;lamb_3f40</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.93283e-05</td></tr>\n",
       "<tr><td>t_3352da0a  </td><td>PENDING </td><td>               </td><td>sigmoid     </td><td style=\"text-align: right;\"> 0.393636</td><td>              </td><td style=\"text-align: right;\">3.81815e-05</td><td>&lt;function &lt;lamb_3f40</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.15857e-05</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 1/10:   2%|         | 1/63 [00:01<02:00,  1.94s/it]\n",
      "Epoch 1/10:   3%|         | 2/63 [00:02<01:26,  1.41s/it]\n",
      "Epoch 1/10:   5%|         | 3/63 [00:04<01:14,  1.24s/it]\n",
      "Epoch 1/10:   6%|         | 4/63 [00:05<01:08,  1.17s/it]\n",
      "Epoch 1/10:   8%|         | 5/63 [00:06<01:05,  1.13s/it]\n",
      "Epoch 1/10:  10%|         | 6/63 [00:07<01:03,  1.11s/it]\n",
      "Epoch 1/10:  11%|         | 7/63 [00:08<01:01,  1.09s/it]\n",
      "Epoch 1/10:  13%|        | 8/63 [00:09<00:59,  1.09s/it]\n",
      "Epoch 1/10:  14%|        | 9/63 [00:10<00:58,  1.08s/it]\n",
      "Epoch 1/10:  16%|        | 10/63 [00:11<00:57,  1.08s/it]\n",
      "Epoch 1/10:  17%|        | 11/63 [00:12<00:55,  1.07s/it]\n",
      "Epoch 1/10:  19%|        | 12/63 [00:13<00:53,  1.06s/it]\n",
      "Epoch 1/10:  21%|        | 13/63 [00:14<00:52,  1.06s/it]\n",
      "Epoch 1/10:  22%|       | 14/63 [00:15<00:51,  1.05s/it]\n",
      "Epoch 1/10:  24%|       | 15/63 [00:16<00:49,  1.04s/it]\n",
      "Epoch 1/10:  25%|       | 16/63 [00:17<00:48,  1.03s/it]\n",
      "Epoch 1/10:  27%|       | 17/63 [00:18<00:47,  1.03s/it]\n",
      "Epoch 1/10:  29%|       | 18/63 [00:19<00:46,  1.04s/it]\n",
      "Epoch 1/10:  30%|       | 19/63 [00:20<00:45,  1.04s/it]\n",
      "Epoch 1/10:  32%|      | 20/63 [00:21<00:44,  1.03s/it]\n",
      "Epoch 1/10:  33%|      | 21/63 [00:22<00:43,  1.03s/it]\n",
      "Epoch 1/10:  35%|      | 22/63 [00:23<00:42,  1.03s/it]\n",
      "Epoch 1/10:  37%|      | 23/63 [00:24<00:41,  1.03s/it]\n",
      "Epoch 1/10:  38%|      | 24/63 [00:26<00:40,  1.05s/it]\n",
      "Epoch 1/10:  40%|      | 25/63 [00:27<00:40,  1.08s/it]\n",
      "Epoch 1/10:  41%|     | 26/63 [00:28<00:39,  1.07s/it]\n",
      "Epoch 1/10:  43%|     | 27/63 [00:29<00:38,  1.06s/it]\n",
      "Epoch 1/10:  44%|     | 28/63 [00:30<00:36,  1.05s/it]\n",
      "Epoch 1/10:  46%|     | 29/63 [00:31<00:35,  1.05s/it]\n",
      "Epoch 1/10:  48%|     | 30/63 [00:32<00:34,  1.04s/it]\n",
      "Epoch 1/10:  49%|     | 31/63 [00:33<00:33,  1.04s/it]\n",
      "Epoch 1/10:  51%|     | 32/63 [00:34<00:32,  1.05s/it]\n",
      "Epoch 1/10:  52%|    | 33/63 [00:35<00:32,  1.07s/it]\n",
      "Epoch 1/10:  54%|    | 34/63 [00:36<00:31,  1.08s/it]\n",
      "Epoch 1/10:  56%|    | 35/63 [00:37<00:30,  1.09s/it]\n",
      "Epoch 1/10:  57%|    | 36/63 [00:38<00:29,  1.08s/it]\n",
      "Epoch 1/10:  59%|    | 37/63 [00:39<00:28,  1.08s/it]\n",
      "Epoch 1/10:  60%|    | 38/63 [00:40<00:26,  1.07s/it]\n",
      "Epoch 1/10:  62%|   | 39/63 [00:42<00:25,  1.06s/it]\n",
      "Epoch 1/10:  63%|   | 40/63 [00:43<00:24,  1.06s/it]\n",
      "Epoch 1/10:  65%|   | 41/63 [00:44<00:23,  1.06s/it]\n",
      "Epoch 1/10:  67%|   | 42/63 [00:45<00:22,  1.06s/it]\n",
      "Epoch 1/10:  68%|   | 43/63 [00:46<00:21,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    max_t=100,           # max training iterations per trial\n",
    "    grace_period=10,     # min iterations before stopping\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"loss\", mode=\"min\", seed=42)\n",
    "\n",
    "# define search space in a config dictionary, i.e what are the values you want to try, this is just example of format\n",
    "'''\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"freeze_backbone\": tune.choice([True, False]),\n",
    "    \"num_epochs\": 2, \n",
    "    \"optimiser\": tune.choice([optim.Adam, optim.SGD]),\n",
    "    \"criterion\": tune.choice([nn.CrossEntropyLoss, nn.NLLLoss]),\n",
    "    \"unfreeze_blocks\": tune.choice([[7], [6, 7], [5, 6, 7]]),\n",
    "}\n",
    "'''\n",
    "\n",
    "# this is what i specified for the example because i am running on cpu\n",
    "# config = {\n",
    "#     \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "#     \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "#     \"dropout\": tune.uniform(0.1, 0.5),\n",
    "#     \"freeze_backbone\": tune.choice([True]),\n",
    "#     \"num_epochs\": 2,\n",
    "# }\n",
    "\n",
    "\n",
    "# for efficientnetb5 gpu\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 5e-5),    \n",
    "    \"weight_decay\": tune.loguniform(1e-5, 2e-5),\n",
    "    \"dropout\": tune.uniform(0.39, 0.41),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\", \"leakyrelu\"]), #Removed gelu\n",
    "    \"unfreeze_blocks\": tune.choice([[6, 7]]),\n",
    "    \"num_epochs\": 10,\n",
    "    \"optimiser\": tune.choice([\n",
    "        lambda params, lr, wd: optim.Adam(params, lr=lr, weight_decay=wd),\n",
    "        lambda params, lr, wd: optim.SGD(params, lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    ]),\n",
    "    \"criterion\": lambda: nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    # this is meant to create short and unique directory name for each trial bc else too long for windows\n",
    "    # include a unique identifier (like trial.trial_id).\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "def trial_name_creator(trial):\n",
    "    return f\"t_{trial.trial_id}\"\n",
    "\n",
    "# tuner object\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {\"cpu\": 12, \"gpu\": 1}), #NOTE:specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=optuna_search,\n",
    "        num_samples=15,  # number of trials to run\n",
    "        trial_dirname_creator=trial_dirname_creator,\n",
    "        trial_name_creator=trial_name_creator, \n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        name=model_name,\n",
    "        storage_path=F\"C:/ray_results/{model_name}\", #NOTE: not required for macos\n",
    "        log_to_file=True,\n",
    "        verbose=1),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "print(\"Best config:\", results.get_best_result(metric=\"loss\", mode=\"min\", filter_nan_and_inf=False).config)\n",
    "\n",
    "# Extracting the best config from the training from above\n",
    "best_config = results.get_best_result(metric=\"loss\", mode=\"min\").config\n",
    "print(\"Full best config:\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641aeac",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4762ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Good configs, saved after various experimentations\n",
    "# This cell can be skipped if hyper parameter tuning was run\n",
    "best_config = {\n",
    "    'lr': 2.5883109107619077e-05,\n",
    "    'weight_decay': 1.5994372155012393e-05,\n",
    "    'dropout': 0.4016587828927856,\n",
    "    'hidden_sizes': None,\n",
    "    'activation': 'leakyrelu',\n",
    "    'unfreeze_blocks': [6, 7],\n",
    "    'num_epochs': 10,\n",
    "    'optimiser': lambda params, lr, wd: optim.Adam(params, lr=lr, weight_decay=wd),\n",
    "    'criterion': lambda: nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7018de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = BaseEfficientNetB5(\n",
    "    num_classes=4,\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    hidden_sizes=best_config[\"hidden_sizes\"],\n",
    "    activation=best_config[\"activation\"],\n",
    "    unfreeze_blocks=best_config[\"unfreeze_blocks\"]\n",
    ")\n",
    "\n",
    "optimiser = best_config[\"optimiser\"](final_model.parameters(), lr=best_config[\"lr\"], wd=best_config[\"weight_decay\"])\n",
    "criterion = best_config[\"criterion\"]()\n",
    "\n",
    "final_model = final_model.to(DEVICE)\n",
    "logging.info(\"Model instantiated on device: %s\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c7292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping parameters\n",
    "patience = 5  # the number of epochs to wait to see if got improvements\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "num_epochs_full = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop with early stopping\n",
    "for epoch in range(num_epochs_full):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in tqdm(FULL_TRAIN_LOADER, desc=f\"Epoch {epoch+1}/{num_epochs_full}\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimiser.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_loss / len(FULL_TRAIN_LOADER.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    logging.info(\"Epoch %d: Training Loss: %.4f\", epoch+1, epoch_train_loss)\n",
    "    \n",
    "    # evaluate on the validation set\n",
    "    final_model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in FULL_VAL_LOADER:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = final_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_val_loss = running_val_loss / len(FULL_VAL_LOADER.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    logging.info(\"Epoch %d: Validation Loss: %.4f\", epoch+1, epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_full} - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "    # if no improvement then early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # save the best model\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        logging.info(\"Model saved to: %s\",  {BEST_MODEL_PATH})\n",
    "        logging.info(\"Epoch %d: New best model saved.\", epoch+1)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        logging.info(\"Early stopping triggered at epoch %d\", epoch+1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e01e8",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a3b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model from training \n",
    "final_model.load_state_dict(torch.load(BEST_MODEL_PATH)) \n",
    "final_model.to(DEVICE) \n",
    "final_model.eval() \n",
    " \n",
    "# initialise metrics \n",
    "correct = 0 \n",
    "total = 0 \n",
    "test_loss = 0.0 \n",
    "all_preds = [] \n",
    "all_labels = [] \n",
    " \n",
    "with torch.no_grad(): \n",
    "    for images, labels in FULL_TEST_LOADER: \n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE) \n",
    "        outputs = final_model(images) \n",
    "        # for test loss \n",
    "        loss = criterion(outputs, labels) \n",
    "        test_loss += loss.item() * images.size(0) \n",
    "        # for accuracy \n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0) \n",
    "        correct += (predicted == labels).sum().item() \n",
    " \n",
    "        all_preds.extend(predicted.cpu().numpy()) # move to cpu to ensure compatibility because numpy only operates on cpu \n",
    "        all_labels.extend(labels.cpu().numpy()) \n",
    " \n",
    "test_loss = test_loss / len(FULL_TEST_LOADER.dataset) \n",
    "test_accuracy = correct / total \n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\") \n",
    "logging.info(\"Test Loss: %.4f, Test Accuracy: %.2f%%\", test_loss, test_accuracy * 100) \n",
    " \n",
    "# plotting training and validation loss curves \n",
    "if 'train_losses' in globals() and 'val_losses' in globals(): \n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Training Loss\") \n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Validation Loss\") \n",
    "    plt.xlabel(\"Epoch\") \n",
    "    plt.ylabel(\"Loss\") \n",
    "    plt.title(\"Training and Validation Loss Curves\") \n",
    "    plt.legend() \n",
    "    plt.grid(True) \n",
    "    loss_curve_path = os.path.join(\"..\", \"results\", {model_name}, \"loss_curves.png\") #NOTE: specify by model name \n",
    "    plt.savefig(loss_curve_path) \n",
    "    plt.show() \n",
    "    logging.info(\"Training and Validation Loss curves saved to %s\", loss_curve_path) \n",
    "else: \n",
    "    logging.warning(\"train_losses and val_losses are not defined. Skipping loss curves plot.\") \n",
    " \n",
    "# generate and log classification report \n",
    "class_names = FULL_TEST_LOADER.dataset.classes \n",
    "report = classification_report(all_labels, all_preds, target_names=class_names) \n",
    "print(\"\\nClassification Report:\\n\", report) \n",
    "logging.info(\"Classification Report:\\n%s\", report) \n",
    " \n",
    "# generate confusion matrix \n",
    "cm = confusion_matrix(all_labels, all_preds) \n",
    " \n",
    "# save confusion matrix as image \n",
    "fig, ax = plt.subplots(figsize=(8, 6)) \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax) \n",
    "plt.xlabel(\"Predicted\") \n",
    "plt.ylabel(\"True\") \n",
    "plt.title(\"Confusion Matrix\") \n",
    "plt.tight_layout() \n",
    " \n",
    "conf_matrix_path = os.path.join(\"..\", \"results\", {model_name}, \"confusion_matrix.png\") #NOTE: specify by model name \n",
    "fig.savefig(conf_matrix_path) \n",
    "plt.show(fig) \n",
    " \n",
    "logging.info(\"Confusion matrix saved to: %s\", conf_matrix_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADLTEAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
