{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72a7c0c5",
   "metadata": {},
   "source": [
    "# EfficientNet with Squeeze Excitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd871ce6",
   "metadata": {},
   "source": [
    "In this model, our team builds of the efficientNet infrastructure, where we have introduced a Squeeze Excitation block in the classifier head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65feca1",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Used to handle the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45d0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-20 12:30:05,228\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-04-20 12:30:05,365\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msearch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OptunaSearch\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschedulers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ASHAScheduler\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDataHandler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_dataset, create_full_data_loaders, create_tuning_data_loaders\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Callable\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b5, EfficientNet_B5_Weights\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "from utils.DataHandler import download_dataset, create_full_data_loaders, create_tuning_data_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804ebec",
   "metadata": {},
   "source": [
    "## Loading Dataset and File Location Variables\n",
    "All code that handles renaming of file location save files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffb1e0",
   "metadata": {},
   "source": [
    "Ability to set the following variables\n",
    "- **model_name**: Name the current model that is being created\n",
    "- **batch_size**: The size of each batch of data\n",
    "- **root**: Declare the root folder of the project (should usually be ..)\n",
    "- **model_directory**: Declare the folder that models will be saved in (should be standardised to models)\n",
    "- **dataset_folder**: Location for the dataset folder (should be \"../../input/final_split_15Apr2025\")\n",
    "\n",
    "\n",
    "\n",
    "Which creates the following global variables\n",
    "- PROJECT_ROOT:\n",
    "- LOG_DIR:\n",
    "- MODEL_SAVE_DIR:\n",
    "- DEVICE:\n",
    "- SPLIT_DATASET:\n",
    "- BATCH_SIZE: \n",
    "- TRAIN_LOADER, VAL_LOADER, TEST_LOADER:\n",
    "- FULL_TRAIN_LOADER, FULL_VAL_LOADER, FULL_TEST_LOADER: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21489441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variables to control where files are saved\n",
    "\n",
    "model_name = \"ENSE\" # Name of the model that the file is training in abbreviation\n",
    "batch_size = 64 # State the number of datapoints in each batch size\n",
    "root = \"..\" # Os path to the root of the project\n",
    "model_directory = \"models\" # Name of the folder where the models will be stored\n",
    "dataset_folder = \"../input/final_split_15Apr2025\" # Abs Location of the folder with the data split into train eval test\n",
    "root_result_folder = \"results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35721868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yongl\\OneDrive\\Documents\\GitHub\\DogEmotionADL\\results/ENSE\n",
      "Using device: CUDA (GPU)\n"
     ]
    }
   ],
   "source": [
    "# Determine the project root - required to import DataHandler from utils folder\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), root))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Configuring log file\n",
    "LOG_DIR = os.path.join(PROJECT_ROOT, \"logs\")\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "log_filename = os.path.join(LOG_DIR, F\"{model_name}_training_log.txt\")\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    filemode=\"w\",  #NOTE: previous logs would be overwritten\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Creating the models save location\n",
    "MODEL_SAVE_DIR = os.path.join(PROJECT_ROOT, model_directory, model_name)\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, F\"{model_directory}/hyptune/{model_name}\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, F\"{root_result_folder}/{model_name}\")\n",
    "print(RESULTS_DIR)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, F\"{model_name}_best_model.pt\")\n",
    "\n",
    "# Gets the device to be used\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using device: CUDA (GPU)\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using device: MPS (Apple Silicon GPU)\")\n",
    "        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"Using device: CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d100d88",
   "metadata": {},
   "source": [
    "Loading the datasets for both hyperparameter tuning as well as actual trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9555097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-split datasets: train 8025, val 579, test 572\n",
      "Created tuning data loaders with subset fraction: 0.5\n",
      "Created subset datasets for hyperparameter tuning: train 4012, val 289, test 572\n",
      "Class Distribution for Subset Training:\n",
      "  angry     : 983\n",
      "  happy     : 1039\n",
      "  relaxed   : 1024\n",
      "  sad       : 966\n",
      "Class Distribution for Subset Validation:\n",
      "  angry     : 71\n",
      "  happy     : 75\n",
      "  relaxed   : 74\n",
      "  sad       : 69\n",
      "Using pre-split datasets: train 8025, val 579, test 572\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATASET = os.path.abspath(dataset_folder)\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "# Define data transformation \n",
    "# in this notebook, we are doing it for EfficientNetB5, so we resize to ~456x456\n",
    "weights = EfficientNet_B5_Weights.DEFAULT\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((456, 456)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    weights.transforms()\n",
    "])\n",
    "\n",
    "# Creating dataloader to load 50% of the dataset with the same proportions used for hyper parameter tuning\n",
    "TRAIN_LOADER, VAL_LOADER, TEST_LOADER = create_tuning_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset_fraction=0.5,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Creating dataloader to load the full dataset for training the actual models and testing\n",
    "FULL_TRAIN_LOADER, FULL_VAL_LOADER, FULL_TEST_LOADER = create_full_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9eb0a",
   "metadata": {},
   "source": [
    "## Model Specification\n",
    "This part of the file will change largely based on each of the models that are being created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c0105ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcitationBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeeze-and-Excitation (SE) block for channel-wise feature recalibration.\n",
    "\n",
    "    This block implements the SE mechanism which adaptively recalibrates \n",
    "    channel-wise feature responses by explicitly modeling interdependencies \n",
    "    between channels.\n",
    "\n",
    "    Args:\n",
    "        channels (int): Number of input and output channels.\n",
    "        reduction_ratio (int): Reduction ratio for the intermediate hidden layer. \n",
    "            Controls the bottleneck in the SE block (default: 16).\n",
    "\n",
    "    Forward Input:\n",
    "        x (Tensor): Input feature map of shape (batch_size, channels, height, width).\n",
    "\n",
    "    Forward Output:\n",
    "        Tensor: Output feature map of the same shape, with recalibrated channel responses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, reduction_ratio=16):\n",
    "        super(SqueezeExcitationBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.interaction = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction_ratio, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction_ratio, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.interaction(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class BaseEfficientNetB5(nn.Module):\n",
    "    \"\"\"\n",
    "    EfficientNetB5-based classifier with optional Squeeze-and-Excitation (SE) block, \n",
    "    customizable classifier head, and partial layer unfreezing for fine-tuning.\n",
    "\n",
    "    Designed for flexibility during hyperparameter tuning (e.g., with Ray Tune).\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of output classes. Default is 4.\n",
    "        dropout (float): Dropout probability used in the classifier head. Default is 0.2.\n",
    "        hidden_sizes (Optional[List[int]]): List of hidden layer sizes for the classifier head. \n",
    "            If None, no additional hidden layers are added beyond the SE block (if used).\n",
    "        activation (str): Activation function to use in the classifier head. \n",
    "            Supported values: 'relu', 'tanh', 'sigmoid'. Default is 'relu'.\n",
    "        use_se (bool): Whether to include a Squeeze-and-Excitation (SE) block before the classifier. Default is True.\n",
    "        unfreeze_blocks (Optional[List[int]]): List of block indices in EfficientNetB5's features to unfreeze \n",
    "            for fine-tuning. If None, the entire backbone remains frozen.\n",
    "\n",
    "    Notes:\n",
    "        - The SE block is applied to the flattened output of the backbone if `use_se` is True.\n",
    "        - The classifier head is built dynamically based on `hidden_sizes` and activation.\n",
    "        - The EfficientNetB5 backbone is loaded with pretrained weights by default.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes: int = 4,\n",
    "                 dropout: float = 0.2,\n",
    "                 hidden_sizes: Optional[List[int]] = None,\n",
    "                 activation: str = 'relu',\n",
    "                 use_se: bool = True,\n",
    "                 unfreeze_blocks: Optional[List[int]] = None) -> None:\n",
    "        super(BaseEfficientNetB5, self).__init__()\n",
    "        weights = EfficientNet_B5_Weights.DEFAULT\n",
    "        self.backbone = efficientnet_b5(weights=weights)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "\n",
    "        # Setting the values layers in the efficientnet backbone that will be frozen\n",
    "        for param in self.backbone.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if unfreeze_blocks:\n",
    "            for idx in unfreeze_blocks:\n",
    "                for param in self.backbone.features[idx].parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        # Build the classifier head with the SE and additional layers according to the number of layers in the hidden state\n",
    "        layers = []\n",
    "        if use_se:\n",
    "            layers.extend([\n",
    "                nn.Unflatten(1, (in_features, 1, 1)),\n",
    "                SqueezeExcitationBlock(in_features),\n",
    "                nn.Flatten()\n",
    "            ])\n",
    "        \n",
    "        input_dim = in_features\n",
    "        if hidden_sizes:\n",
    "            for hidden_dim in hidden_sizes:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(self._get_activation(activation))\n",
    "                input_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Dropout(p=dropout))\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "\n",
    "        self.backbone.classifier[1] = nn.Sequential(*layers)\n",
    "\n",
    "    def _get_activation(self, activation: str) -> Callable:\n",
    "        \"\"\"\n",
    "        Returns the specified activation function from PyTorch's nn module.\n",
    "\n",
    "        Args:\n",
    "            activation (str): Name of the activation function. Supported values are\n",
    "                'relu', 'tanh', and 'sigmoid' (case-insensitive).\n",
    "\n",
    "        Returns:\n",
    "            Callable: The corresponding activation function module (e.g., nn.ReLU()).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If an unsupported activation function name is provided.\n",
    "        \"\"\"\n",
    "        if activation.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation.lower() == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb027fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Model instantiated: BaseEfficientNetB5\n",
      "BaseEfficientNetB5(\n",
      "  (backbone): EfficientNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
      "              (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(12, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.005128205128205128, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(24, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (2): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.010256410256410256, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "              (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.015384615384615387, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.020512820512820513, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.02564102564102564, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.030769230769230774, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0358974358974359, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)\n",
      "              (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.041025641025641026, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.046153846153846156, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05128205128205128, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.05641025641025642, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06153846153846155, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=384, bias=False)\n",
      "              (1): BatchNorm2d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(384, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(16, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.06666666666666667, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.0717948717948718, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.07692307692307693, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08205128205128205, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.08717948717948719, mode=row)\n",
      "        )\n",
      "        (5): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09230769230769231, mode=row)\n",
      "        )\n",
      "        (6): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.09743589743589744, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
      "              (1): BatchNorm2d(768, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(768, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.10256410256410256, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1076923076923077, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.11282051282051284, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.11794871794871796, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1230769230769231, mode=row)\n",
      "        )\n",
      "        (5): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1282051282051282, mode=row)\n",
      "        )\n",
      "        (6): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 176, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(176, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.13333333333333333, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(176, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 1056, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=1056, bias=False)\n",
      "              (1): BatchNorm2d(1056, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1056, 44, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(44, 1056, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1056, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.13846153846153847, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1435897435897436, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.14871794871794874, mode=row)\n",
      "        )\n",
      "        (3): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.15384615384615385, mode=row)\n",
      "        )\n",
      "        (4): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.158974358974359, mode=row)\n",
      "        )\n",
      "        (5): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1641025641025641, mode=row)\n",
      "        )\n",
      "        (6): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.16923076923076924, mode=row)\n",
      "        )\n",
      "        (7): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.17435897435897438, mode=row)\n",
      "        )\n",
      "        (8): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 304, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(304, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.1794871794871795, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential(\n",
      "        (0): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(304, 1824, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 1824, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1824, bias=False)\n",
      "              (1): BatchNorm2d(1824, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(1824, 76, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(76, 1824, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(1824, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18461538461538463, mode=row)\n",
      "        )\n",
      "        (1): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.18974358974358976, mode=row)\n",
      "        )\n",
      "        (2): MBConv(\n",
      "          (block): Sequential(\n",
      "            (0): Conv2dNormActivation(\n",
      "              (0): Conv2d(512, 3072, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 3072, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3072, bias=False)\n",
      "              (1): BatchNorm2d(3072, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "              (2): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): SqueezeExcitation(\n",
      "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "              (fc1): Conv2d(3072, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (fc2): Conv2d(128, 3072, kernel_size=(1, 1), stride=(1, 1))\n",
      "              (activation): SiLU(inplace=True)\n",
      "              (scale_activation): Sigmoid()\n",
      "            )\n",
      "            (3): Conv2dNormActivation(\n",
      "              (0): Conv2d(3072, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (stochastic_depth): StochasticDepth(p=0.19487179487179487, mode=row)\n",
      "        )\n",
      "      )\n",
      "      (8): Conv2dNormActivation(\n",
      "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): SiLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.4, inplace=True)\n",
      "      (1): Sequential(\n",
      "        (0): Unflatten(dim=1, unflattened_size=(2048, 1, 1))\n",
      "        (1): SqueezeExcitationBlock(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (interaction): Sequential(\n",
      "            (0): Linear(in_features=2048, out_features=128, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=128, out_features=2048, bias=False)\n",
      "            (3): Sigmoid()\n",
      "          )\n",
      "        )\n",
      "        (2): Flatten(start_dim=1, end_dim=-1)\n",
      "        (3): Dropout(p=0.3, inplace=False)\n",
      "        (4): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): Dropout(p=0.3, inplace=False)\n",
      "        (7): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (8): ReLU()\n",
      "        (9): Dropout(p=0.3, inplace=False)\n",
      "        (10): Linear(in_features=128, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "model = BaseEfficientNetB5(num_classes=4, dropout=0.3, hidden_sizes=[256, 128], activation='relu', unfreeze_blocks=[7]).to(DEVICE)\n",
    "print(\"Model instantiated:\", model.__class__.__name__)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e203777",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ba66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "\n",
    "def train_model(config, checkpoint_dir=CHECKPOINT_DIR, data_dir=None):\n",
    "    \"\"\"Training function for Ray Tune hyperparameter tuning.\n",
    "\n",
    "    This function instantiates the model with hyperparameters\n",
    "    specified in the config dictionary, trains the model on the global TRAIN_LOADER,\n",
    "    evaluates on VAL_LOADER, and reports the validation loss to Ray Tune.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Hyperparameter configuration. Expected keys include:\n",
    "            - lr (float): Learning rate.\n",
    "            - weight_decay (float): Weight decay for the optimizer.\n",
    "            - dropout (float): Dropout rate for the classifier.\n",
    "            - hidden_sizes (list or None): List of hidden layer sizes in the classifier.\n",
    "            - activation (str): Activation function to use ('relu', 'tanh', etc.).\n",
    "            - freeze_backbone (bool): Whether to freeze the model backbone.\n",
    "            - num_epochs (int): Number of training epochs.\n",
    "            - optimiser (callable, optional): Optimiser class. Default is optim.Adam.\n",
    "            - criterion (callable, optional): Loss function instance. Default is nn.CrossEntropyLoss().\n",
    "        checkpoint_dir (str, optional): Directory for checkpointing (if applicable).\n",
    "        data_dir (str, optional): Not used here; included for compatibility.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Checkpoint Folder exists\")\n",
    "    \n",
    "    # instantiate model with hyperparameters from config\n",
    "    model = BaseEfficientNetB5(\n",
    "        num_classes=4,\n",
    "        dropout=config.get(\"dropout\", 0.3),\n",
    "        hidden_sizes=config.get(\"hidden_sizes\", None),\n",
    "        activation=config.get(\"activation\", \"relu\"),\n",
    "        unfreeze_blocks = config.get(\"unfreeze_blocks\", [7])\n",
    "    ).to(device)\n",
    "\n",
    "    optimiser = config[\"optimiser\"](model.parameters(), config[\"lr\"], config[\"weight_decay\"])\n",
    "    criterion = config[\"criterion\"]()\n",
    "\n",
    "    num_epochs = config.get(\"num_epochs\", 2)  # a low number for quick tuning, but update accordingly\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in tqdm(TRAIN_LOADER, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(TRAIN_LOADER.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # optionally, can checkpoint the model\n",
    "        if checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "    \n",
    "    # evaluation on the subset validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in VAL_LOADER:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_val_loss = total_loss / len(VAL_LOADER.dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Report the metric to Ray Tune\n",
    "    tune.report({\"loss\": avg_val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276f8fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-20 11:09:40</td></tr>\n",
       "<tr><td>Running for: </td><td>00:35:38.24        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.2/31.1 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Logical resource usage: 12.0/16 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>t_c8c52f13  </td><td style=\"text-align: right;\">           1</td><td>C:/Users/yongl/AppData/Local/Temp/ray/session_2025-04-20_10-33-59_893052_4820/artifacts/2025-04-20_10-34-02/ENSE/driver_artifacts/trial_c8c52f13/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name  </th><th>status    </th><th>loc            </th><th>activation  </th><th style=\"text-align: right;\">  dropout</th><th>hidden_sizes  </th><th style=\"text-align: right;\">         lr</th><th>optimiser           </th><th>unfreeze_blocks  </th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>t_d2c5ba5c  </td><td>RUNNING   </td><td>127.0.0.1:22184</td><td>tanh        </td><td style=\"text-align: right;\"> 0.409392</td><td>[512, 256]    </td><td style=\"text-align: right;\">2.41065e-05</td><td>&lt;function &lt;lamb_3f40</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.1367e-05 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>t_d9a12698  </td><td>PENDING   </td><td>               </td><td>leakyrelu   </td><td style=\"text-align: right;\"> 0.406575</td><td>              </td><td style=\"text-align: right;\">1.86928e-05</td><td>&lt;function &lt;lamb_d900</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.20694e-05</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "<tr><td>t_bd650a46  </td><td>TERMINATED</td><td>127.0.0.1:24276</td><td>tanh        </td><td style=\"text-align: right;\"> 0.40464 </td><td>[256, 128]    </td><td style=\"text-align: right;\">1.82723e-05</td><td>&lt;function &lt;lamb_3f40</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.93283e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         657.01 </td><td style=\"text-align: right;\">1.38829</td></tr>\n",
       "<tr><td>t_3352da0a  </td><td>TERMINATED</td><td>127.0.0.1:21864</td><td>sigmoid     </td><td style=\"text-align: right;\"> 0.393636</td><td>              </td><td style=\"text-align: right;\">3.81815e-05</td><td>&lt;function &lt;lamb_3f40</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.15857e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         649.193</td><td style=\"text-align: right;\">1.37907</td></tr>\n",
       "<tr><td>t_8ae9eb94  </td><td>TERMINATED</td><td>127.0.0.1:7692 </td><td>tanh        </td><td style=\"text-align: right;\"> 0.403685</td><td>              </td><td style=\"text-align: right;\">1.63274e-05</td><td>&lt;function &lt;lamb_3f40</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.07005e-05</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         651.311</td><td style=\"text-align: right;\">1.38331</td></tr>\n",
       "<tr><td>t_c8c52f13  </td><td>ERROR     </td><td>127.0.0.1:20840</td><td>leakyrelu   </td><td style=\"text-align: right;\"> 0.393993</td><td>[512, 256]    </td><td style=\"text-align: right;\">2.08343e-05</td><td>&lt;function &lt;lamb_d900</td><td>[6, 7]           </td><td style=\"text-align: right;\">   1.7233e-05 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 1/10:   2%|▏         | 1/63 [00:01<02:00,  1.94s/it]\n",
      "Epoch 1/10:   3%|▎         | 2/63 [00:02<01:26,  1.41s/it]\n",
      "Epoch 1/10:   5%|▍         | 3/63 [00:04<01:14,  1.24s/it]\n",
      "Epoch 1/10:   6%|▋         | 4/63 [00:05<01:08,  1.17s/it]\n",
      "Epoch 1/10:   8%|▊         | 5/63 [00:06<01:05,  1.13s/it]\n",
      "Epoch 1/10:  10%|▉         | 6/63 [00:07<01:03,  1.11s/it]\n",
      "Epoch 1/10:  11%|█         | 7/63 [00:08<01:01,  1.09s/it]\n",
      "Epoch 1/10:  13%|█▎        | 8/63 [00:09<00:59,  1.09s/it]\n",
      "Epoch 1/10:  14%|█▍        | 9/63 [00:10<00:58,  1.08s/it]\n",
      "Epoch 1/10:  16%|█▌        | 10/63 [00:11<00:57,  1.08s/it]\n",
      "Epoch 1/10:  17%|█▋        | 11/63 [00:12<00:55,  1.07s/it]\n",
      "Epoch 1/10:  19%|█▉        | 12/63 [00:13<00:53,  1.06s/it]\n",
      "Epoch 1/10:  21%|██        | 13/63 [00:14<00:52,  1.06s/it]\n",
      "Epoch 1/10:  22%|██▏       | 14/63 [00:15<00:51,  1.05s/it]\n",
      "Epoch 1/10:  24%|██▍       | 15/63 [00:16<00:49,  1.04s/it]\n",
      "Epoch 1/10:  25%|██▌       | 16/63 [00:17<00:48,  1.03s/it]\n",
      "Epoch 1/10:  27%|██▋       | 17/63 [00:18<00:47,  1.03s/it]\n",
      "Epoch 1/10:  29%|██▊       | 18/63 [00:19<00:46,  1.04s/it]\n",
      "Epoch 1/10:  30%|███       | 19/63 [00:20<00:45,  1.04s/it]\n",
      "Epoch 1/10:  32%|███▏      | 20/63 [00:21<00:44,  1.03s/it]\n",
      "Epoch 1/10:  33%|███▎      | 21/63 [00:22<00:43,  1.03s/it]\n",
      "Epoch 1/10:  35%|███▍      | 22/63 [00:23<00:42,  1.03s/it]\n",
      "Epoch 1/10:  37%|███▋      | 23/63 [00:24<00:41,  1.03s/it]\n",
      "Epoch 1/10:  38%|███▊      | 24/63 [00:26<00:40,  1.05s/it]\n",
      "Epoch 1/10:  40%|███▉      | 25/63 [00:27<00:40,  1.08s/it]\n",
      "Epoch 1/10:  41%|████▏     | 26/63 [00:28<00:39,  1.07s/it]\n",
      "Epoch 1/10:  43%|████▎     | 27/63 [00:29<00:38,  1.06s/it]\n",
      "Epoch 1/10:  44%|████▍     | 28/63 [00:30<00:36,  1.05s/it]\n",
      "Epoch 1/10:  46%|████▌     | 29/63 [00:31<00:35,  1.05s/it]\n",
      "Epoch 1/10:  48%|████▊     | 30/63 [00:32<00:34,  1.04s/it]\n",
      "Epoch 1/10:  49%|████▉     | 31/63 [00:33<00:33,  1.04s/it]\n",
      "Epoch 1/10:  51%|█████     | 32/63 [00:34<00:32,  1.05s/it]\n",
      "Epoch 1/10:  52%|█████▏    | 33/63 [00:35<00:32,  1.07s/it]\n",
      "Epoch 1/10:  54%|█████▍    | 34/63 [00:36<00:31,  1.08s/it]\n",
      "Epoch 1/10:  56%|█████▌    | 35/63 [00:37<00:30,  1.09s/it]\n",
      "Epoch 1/10:  57%|█████▋    | 36/63 [00:38<00:29,  1.08s/it]\n",
      "Epoch 1/10:  59%|█████▊    | 37/63 [00:39<00:28,  1.08s/it]\n",
      "Epoch 1/10:  60%|██████    | 38/63 [00:40<00:26,  1.07s/it]\n",
      "Epoch 1/10:  62%|██████▏   | 39/63 [00:42<00:25,  1.06s/it]\n",
      "Epoch 1/10:  63%|██████▎   | 40/63 [00:43<00:24,  1.06s/it]\n",
      "Epoch 1/10:  65%|██████▌   | 41/63 [00:44<00:23,  1.06s/it]\n",
      "Epoch 1/10:  67%|██████▋   | 42/63 [00:45<00:22,  1.06s/it]\n",
      "Epoch 1/10:  68%|██████▊   | 43/63 [00:46<00:21,  1.06s/it]\n",
      "Epoch 1/10:  70%|██████▉   | 44/63 [00:47<00:20,  1.07s/it]\n",
      "Epoch 1/10:  71%|███████▏  | 45/63 [00:48<00:19,  1.07s/it]\n",
      "Epoch 1/10:  73%|███████▎  | 46/63 [00:49<00:18,  1.06s/it]\n",
      "Epoch 1/10:  75%|███████▍  | 47/63 [00:50<00:16,  1.06s/it]\n",
      "Epoch 1/10:  76%|███████▌  | 48/63 [00:51<00:15,  1.05s/it]\n",
      "Epoch 1/10:  78%|███████▊  | 49/63 [00:52<00:14,  1.05s/it]\n",
      "Epoch 1/10:  79%|███████▉  | 50/63 [00:53<00:13,  1.05s/it]\n",
      "Epoch 1/10:  81%|████████  | 51/63 [00:54<00:12,  1.05s/it]\n",
      "Epoch 1/10:  83%|████████▎ | 52/63 [00:55<00:11,  1.05s/it]\n",
      "Epoch 1/10:  84%|████████▍ | 53/63 [00:56<00:10,  1.06s/it]\n",
      "Epoch 1/10:  86%|████████▌ | 54/63 [00:57<00:09,  1.06s/it]\n",
      "Epoch 1/10:  87%|████████▋ | 55/63 [00:58<00:08,  1.06s/it]\n",
      "Epoch 1/10:  89%|████████▉ | 56/63 [00:59<00:07,  1.06s/it]\n",
      "Epoch 1/10:  90%|█████████ | 57/63 [01:01<00:06,  1.06s/it]\n",
      "Epoch 1/10:  92%|█████████▏| 58/63 [01:02<00:05,  1.05s/it]\n",
      "Epoch 1/10:  94%|█████████▎| 59/63 [01:03<00:04,  1.05s/it]\n",
      "Epoch 1/10:  95%|█████████▌| 60/63 [01:04<00:03,  1.04s/it]\n",
      "Epoch 1/10:  97%|█████████▋| 61/63 [01:05<00:02,  1.05s/it]\n",
      "Epoch 1/10:  98%|█████████▊| 62/63 [01:06<00:01,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 1/10, Training Loss: 1.3918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 63/63 [01:06<00:00,  1.06s/it]\n",
      "Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 2/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 2/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 2/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 2/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 2/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 2/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 2/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 2/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 2/10:  14%|█▍        | 9/63 [00:09<00:55,  1.04s/it]\n",
      "Epoch 2/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 2/10:  17%|█▋        | 11/63 [00:11<00:53,  1.04s/it]\n",
      "Epoch 2/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 2/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 2/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 2/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 2/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 2/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 2/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 2/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 2/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 2/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 2/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 2/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 2/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 2/10:  40%|███▉      | 25/63 [00:25<00:39,  1.04s/it]\n",
      "Epoch 2/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 2/10:  43%|████▎     | 27/63 [00:27<00:37,  1.04s/it]\n",
      "Epoch 2/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 2/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 2/10:  48%|████▊     | 30/63 [00:31<00:34,  1.04s/it]\n",
      "Epoch 2/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 2/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 2/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.03s/it]\n",
      "Epoch 2/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 2/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.02s/it]\n",
      "Epoch 2/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 2/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 2/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 2/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.02s/it]\n",
      "Epoch 2/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.02s/it]\n",
      "Epoch 2/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 2/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 2/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.01s/it]\n",
      "Epoch 2/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 2/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 2/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 2/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 2/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 2/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 2/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 2/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 2/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 2/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 2/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 2/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 2/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 2/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 2/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.02s/it]\n",
      "Epoch 2/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 2/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 2/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 2/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 2/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 2/10, Training Loss: 1.3922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 3/10:   3%|▎         | 2/63 [00:02<01:03,  1.05s/it]\n",
      "Epoch 3/10:   5%|▍         | 3/63 [00:03<01:03,  1.05s/it]\n",
      "Epoch 3/10:   6%|▋         | 4/63 [00:04<01:01,  1.05s/it]\n",
      "Epoch 3/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 3/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 3/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 3/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 3/10:  14%|█▍        | 9/63 [00:09<00:56,  1.04s/it]\n",
      "Epoch 3/10:  16%|█▌        | 10/63 [00:10<00:55,  1.04s/it]\n",
      "Epoch 3/10:  17%|█▋        | 11/63 [00:11<00:53,  1.04s/it]\n",
      "Epoch 3/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 3/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 3/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 3/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 3/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 3/10:  27%|██▋       | 17/63 [00:17<00:47,  1.04s/it]\n",
      "Epoch 3/10:  29%|██▊       | 18/63 [00:18<00:46,  1.04s/it]\n",
      "Epoch 3/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 3/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 3/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 3/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 3/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 3/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 3/10:  40%|███▉      | 25/63 [00:25<00:39,  1.05s/it]\n",
      "Epoch 3/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 3/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 3/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 3/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 3/10:  48%|████▊     | 30/63 [00:31<00:34,  1.04s/it]\n",
      "Epoch 3/10:  49%|████▉     | 31/63 [00:32<00:33,  1.05s/it]\n",
      "Epoch 3/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 3/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.05s/it]\n",
      "Epoch 3/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 3/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 3/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 3/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.04s/it]\n",
      "Epoch 3/10:  60%|██████    | 38/63 [00:39<00:25,  1.04s/it]\n",
      "Epoch 3/10:  62%|██████▏   | 39/63 [00:40<00:25,  1.04s/it]\n",
      "Epoch 3/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 3/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.04s/it]\n",
      "Epoch 3/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 3/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 3/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 3/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 3/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 3/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 3/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.03s/it]\n",
      "Epoch 3/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.03s/it]\n",
      "Epoch 3/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 3/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 3/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 3/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 3/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 3/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 3/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.02s/it]\n",
      "Epoch 3/10:  90%|█████████ | 57/63 [00:59<00:06,  1.02s/it]\n",
      "Epoch 3/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 3/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 3/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.02s/it]\n",
      "Epoch 3/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 3/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 3/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 3/10, Training Loss: 1.3902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 4/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 4/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 4/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 4/10:   8%|▊         | 5/63 [00:05<00:59,  1.03s/it]\n",
      "Epoch 4/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 4/10:  11%|█         | 7/63 [00:07<00:58,  1.05s/it]\n",
      "Epoch 4/10:  13%|█▎        | 8/63 [00:08<00:57,  1.05s/it]\n",
      "Epoch 4/10:  14%|█▍        | 9/63 [00:09<00:56,  1.05s/it]\n",
      "Epoch 4/10:  16%|█▌        | 10/63 [00:10<00:55,  1.05s/it]\n",
      "Epoch 4/10:  17%|█▋        | 11/63 [00:11<00:54,  1.05s/it]\n",
      "Epoch 4/10:  19%|█▉        | 12/63 [00:12<00:53,  1.05s/it]\n",
      "Epoch 4/10:  21%|██        | 13/63 [00:13<00:52,  1.05s/it]\n",
      "Epoch 4/10:  22%|██▏       | 14/63 [00:14<00:51,  1.05s/it]\n",
      "Epoch 4/10:  24%|██▍       | 15/63 [00:15<00:50,  1.05s/it]\n",
      "Epoch 4/10:  25%|██▌       | 16/63 [00:16<00:49,  1.05s/it]\n",
      "Epoch 4/10:  27%|██▋       | 17/63 [00:17<00:48,  1.05s/it]\n",
      "Epoch 4/10:  29%|██▊       | 18/63 [00:18<00:47,  1.05s/it]\n",
      "Epoch 4/10:  30%|███       | 19/63 [00:19<00:46,  1.05s/it]\n",
      "Epoch 4/10:  32%|███▏      | 20/63 [00:20<00:45,  1.05s/it]\n",
      "Epoch 4/10:  33%|███▎      | 21/63 [00:22<00:44,  1.05s/it]\n",
      "Epoch 4/10:  35%|███▍      | 22/63 [00:23<00:42,  1.05s/it]\n",
      "Epoch 4/10:  37%|███▋      | 23/63 [00:24<00:41,  1.04s/it]\n",
      "Epoch 4/10:  38%|███▊      | 24/63 [00:25<00:40,  1.04s/it]\n",
      "Epoch 4/10:  40%|███▉      | 25/63 [00:26<00:39,  1.04s/it]\n",
      "Epoch 4/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 4/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 4/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 4/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 4/10:  48%|████▊     | 30/63 [00:31<00:34,  1.04s/it]\n",
      "Epoch 4/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 4/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 4/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.03s/it]\n",
      "Epoch 4/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 4/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 4/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 4/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.04s/it]\n",
      "Epoch 4/10:  60%|██████    | 38/63 [00:39<00:25,  1.04s/it]\n",
      "Epoch 4/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 4/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 4/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 4/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 4/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 4/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 4/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 4/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 4/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 4/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.03s/it]\n",
      "Epoch 4/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 4/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 4/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 4/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 4/10:  84%|████████▍ | 53/63 [00:55<00:10,  1.02s/it]\n",
      "Epoch 4/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.02s/it]\n",
      "Epoch 4/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.02s/it]\n",
      "Epoch 4/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.02s/it]\n",
      "Epoch 4/10:  90%|█████████ | 57/63 [00:59<00:06,  1.02s/it]\n",
      "Epoch 4/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.02s/it]\n",
      "Epoch 4/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.02s/it]\n",
      "Epoch 4/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.02s/it]\n",
      "Epoch 4/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.02s/it]\n",
      "Epoch 4/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.02s/it]\n",
      "Epoch 4/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 4/10, Training Loss: 1.3916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 5/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 5/10:   3%|▎         | 2/63 [00:02<01:04,  1.06s/it]\n",
      "Epoch 5/10:   5%|▍         | 3/63 [00:03<01:03,  1.05s/it]\n",
      "Epoch 5/10:   6%|▋         | 4/63 [00:04<01:02,  1.05s/it]\n",
      "Epoch 5/10:   8%|▊         | 5/63 [00:05<01:00,  1.05s/it]\n",
      "Epoch 5/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 5/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 5/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 5/10:  14%|█▍        | 9/63 [00:09<00:56,  1.04s/it]\n",
      "Epoch 5/10:  16%|█▌        | 10/63 [00:10<00:55,  1.04s/it]\n",
      "Epoch 5/10:  17%|█▋        | 11/63 [00:11<00:54,  1.04s/it]\n",
      "Epoch 5/10:  19%|█▉        | 12/63 [00:12<00:52,  1.04s/it]\n",
      "Epoch 5/10:  21%|██        | 13/63 [00:13<00:52,  1.04s/it]\n",
      "Epoch 5/10:  22%|██▏       | 14/63 [00:14<00:50,  1.04s/it]\n",
      "Epoch 5/10:  24%|██▍       | 15/63 [00:15<00:49,  1.04s/it]\n",
      "Epoch 5/10:  25%|██▌       | 16/63 [00:16<00:49,  1.04s/it]\n",
      "Epoch 5/10:  27%|██▋       | 17/63 [00:17<00:48,  1.04s/it]\n",
      "Epoch 5/10:  29%|██▊       | 18/63 [00:18<00:46,  1.04s/it]\n",
      "Epoch 5/10:  30%|███       | 19/63 [00:19<00:45,  1.05s/it]\n",
      "Epoch 5/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 5/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 5/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 5/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 5/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 5/10:  40%|███▉      | 25/63 [00:26<00:39,  1.04s/it]\n",
      "Epoch 5/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 5/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 5/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 5/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 5/10:  48%|████▊     | 30/63 [00:31<00:34,  1.03s/it]\n",
      "Epoch 5/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 5/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 5/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 5/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 5/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 5/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 5/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.05s/it]\n",
      "Epoch 5/10:  60%|██████    | 38/63 [00:39<00:26,  1.05s/it]\n",
      "Epoch 5/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 5/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 5/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 5/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 5/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 5/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 5/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 5/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 5/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 5/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.03s/it]\n",
      "Epoch 5/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 5/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 5/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 5/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 5/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 5/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 5/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.03s/it]\n",
      "Epoch 5/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.02s/it]\n",
      "Epoch 5/10:  90%|█████████ | 57/63 [00:59<00:06,  1.03s/it]\n",
      "Epoch 5/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 5/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 5/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.03s/it]\n",
      "Epoch 5/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 5/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 5/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 5/10, Training Loss: 1.3910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 6/10:   3%|▎         | 2/63 [00:02<01:04,  1.05s/it]\n",
      "Epoch 6/10:   5%|▍         | 3/63 [00:03<01:02,  1.05s/it]\n",
      "Epoch 6/10:   6%|▋         | 4/63 [00:04<01:01,  1.05s/it]\n",
      "Epoch 6/10:   8%|▊         | 5/63 [00:05<01:00,  1.05s/it]\n",
      "Epoch 6/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 6/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 6/10:  13%|█▎        | 8/63 [00:08<00:56,  1.04s/it]\n",
      "Epoch 6/10:  14%|█▍        | 9/63 [00:09<00:55,  1.03s/it]\n",
      "Epoch 6/10:  16%|█▌        | 10/63 [00:10<00:55,  1.04s/it]\n",
      "Epoch 6/10:  17%|█▋        | 11/63 [00:11<00:53,  1.04s/it]\n",
      "Epoch 6/10:  19%|█▉        | 12/63 [00:12<00:53,  1.05s/it]\n",
      "Epoch 6/10:  21%|██        | 13/63 [00:13<00:52,  1.04s/it]\n",
      "Epoch 6/10:  22%|██▏       | 14/63 [00:14<00:50,  1.04s/it]\n",
      "Epoch 6/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 6/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 6/10:  27%|██▋       | 17/63 [00:17<00:47,  1.04s/it]\n",
      "Epoch 6/10:  29%|██▊       | 18/63 [00:18<00:47,  1.04s/it]\n",
      "Epoch 6/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 6/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 6/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 6/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 6/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 6/10:  38%|███▊      | 24/63 [00:24<00:40,  1.03s/it]\n",
      "Epoch 6/10:  40%|███▉      | 25/63 [00:25<00:39,  1.04s/it]\n",
      "Epoch 6/10:  41%|████▏     | 26/63 [00:27<00:39,  1.06s/it]\n",
      "Epoch 6/10:  43%|████▎     | 27/63 [00:28<00:38,  1.06s/it]\n",
      "Epoch 6/10:  44%|████▍     | 28/63 [00:29<00:37,  1.06s/it]\n",
      "Epoch 6/10:  46%|████▌     | 29/63 [00:30<00:35,  1.06s/it]\n",
      "Epoch 6/10:  48%|████▊     | 30/63 [00:31<00:34,  1.05s/it]\n",
      "Epoch 6/10:  49%|████▉     | 31/63 [00:32<00:33,  1.05s/it]\n",
      "Epoch 6/10:  51%|█████     | 32/63 [00:33<00:32,  1.05s/it]\n",
      "Epoch 6/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 6/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 6/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 6/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.04s/it]\n",
      "Epoch 6/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.04s/it]\n",
      "Epoch 6/10:  60%|██████    | 38/63 [00:39<00:26,  1.04s/it]\n",
      "Epoch 6/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 6/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 6/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 6/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.01s/it]\n",
      "Epoch 6/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 6/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.01s/it]\n",
      "Epoch 6/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 6/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 6/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.01s/it]\n",
      "Epoch 6/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.01s/it]\n",
      "Epoch 6/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.01s/it]\n",
      "Epoch 6/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.01s/it]\n",
      "Epoch 6/10:  81%|████████  | 51/63 [00:52<00:12,  1.01s/it]\n",
      "Epoch 6/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.01s/it]\n",
      "Epoch 6/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.01s/it]\n",
      "Epoch 6/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.01s/it]\n",
      "Epoch 6/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 6/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 6/10:  90%|█████████ | 57/63 [00:58<00:06,  1.02s/it]\n",
      "Epoch 6/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.02s/it]\n",
      "Epoch 6/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 6/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 6/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.01s/it]\n",
      "Epoch 6/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 6/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 6/10, Training Loss: 1.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 7/10:   3%|▎         | 2/63 [00:02<01:03,  1.03s/it]\n",
      "Epoch 7/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 7/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 7/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 7/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 7/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 7/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 7/10:  14%|█▍        | 9/63 [00:09<00:55,  1.04s/it]\n",
      "Epoch 7/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 7/10:  17%|█▋        | 11/63 [00:11<00:53,  1.04s/it]\n",
      "Epoch 7/10:  19%|█▉        | 12/63 [00:12<00:52,  1.04s/it]\n",
      "Epoch 7/10:  21%|██        | 13/63 [00:13<00:52,  1.04s/it]\n",
      "Epoch 7/10:  22%|██▏       | 14/63 [00:14<00:50,  1.04s/it]\n",
      "Epoch 7/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 7/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 7/10:  27%|██▋       | 17/63 [00:17<00:47,  1.04s/it]\n",
      "Epoch 7/10:  29%|██▊       | 18/63 [00:18<00:46,  1.04s/it]\n",
      "Epoch 7/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 7/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 7/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 7/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 7/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 7/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 7/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 7/10:  41%|████▏     | 26/63 [00:26<00:38,  1.03s/it]\n",
      "Epoch 7/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 7/10:  44%|████▍     | 28/63 [00:29<00:36,  1.03s/it]\n",
      "Epoch 7/10:  46%|████▌     | 29/63 [00:30<00:35,  1.03s/it]\n",
      "Epoch 7/10:  48%|████▊     | 30/63 [00:31<00:33,  1.03s/it]\n",
      "Epoch 7/10:  49%|████▉     | 31/63 [00:32<00:32,  1.03s/it]\n",
      "Epoch 7/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 7/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.03s/it]\n",
      "Epoch 7/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 7/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.02s/it]\n",
      "Epoch 7/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 7/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 7/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 7/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 7/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 7/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 7/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 7/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 7/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 7/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 7/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 7/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.01s/it]\n",
      "Epoch 7/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.01s/it]\n",
      "Epoch 7/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.01s/it]\n",
      "Epoch 7/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.01s/it]\n",
      "Epoch 7/10:  81%|████████  | 51/63 [00:52<00:12,  1.01s/it]\n",
      "Epoch 7/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 7/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 7/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 7/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 7/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 7/10:  90%|█████████ | 57/63 [00:58<00:06,  1.01s/it]\n",
      "Epoch 7/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.02s/it]\n",
      "Epoch 7/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.01s/it]\n",
      "Epoch 7/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 7/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.02s/it]\n",
      "Epoch 7/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 7/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 7/10, Training Loss: 1.3913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 8/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 8/10:   3%|▎         | 2/63 [00:02<01:04,  1.05s/it]\n",
      "Epoch 8/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 8/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 8/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 8/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 8/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 8/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 8/10:  14%|█▍        | 9/63 [00:09<00:55,  1.04s/it]\n",
      "Epoch 8/10:  16%|█▌        | 10/63 [00:10<00:55,  1.04s/it]\n",
      "Epoch 8/10:  17%|█▋        | 11/63 [00:11<00:54,  1.04s/it]\n",
      "Epoch 8/10:  19%|█▉        | 12/63 [00:12<00:53,  1.05s/it]\n",
      "Epoch 8/10:  21%|██        | 13/63 [00:13<00:52,  1.04s/it]\n",
      "Epoch 8/10:  22%|██▏       | 14/63 [00:14<00:51,  1.04s/it]\n",
      "Epoch 8/10:  24%|██▍       | 15/63 [00:15<00:49,  1.04s/it]\n",
      "Epoch 8/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 8/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 8/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 8/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 8/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 8/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 8/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 8/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 8/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 8/10:  40%|███▉      | 25/63 [00:25<00:39,  1.04s/it]\n",
      "Epoch 8/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 8/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 8/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 8/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 8/10:  48%|████▊     | 30/63 [00:31<00:34,  1.04s/it]\n",
      "Epoch 8/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 8/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 8/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 8/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.06s/it]\n",
      "Epoch 8/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.06s/it]\n",
      "Epoch 8/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.05s/it]\n",
      "Epoch 8/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.05s/it]\n",
      "Epoch 8/10:  60%|██████    | 38/63 [00:39<00:26,  1.05s/it]\n",
      "Epoch 8/10:  62%|██████▏   | 39/63 [00:40<00:25,  1.05s/it]\n",
      "Epoch 8/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 8/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.04s/it]\n",
      "Epoch 8/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 8/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.04s/it]\n",
      "Epoch 8/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 8/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 8/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 8/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 8/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.03s/it]\n",
      "Epoch 8/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 8/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 8/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 8/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 8/10:  84%|████████▍ | 53/63 [00:55<00:10,  1.03s/it]\n",
      "Epoch 8/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.03s/it]\n",
      "Epoch 8/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.03s/it]\n",
      "Epoch 8/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.03s/it]\n",
      "Epoch 8/10:  90%|█████████ | 57/63 [00:59<00:06,  1.03s/it]\n",
      "Epoch 8/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 8/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 8/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.03s/it]\n",
      "Epoch 8/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 8/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 8/10: 100%|██████████| 63/63 [01:05<00:00,  1.03s/it]\n",
      "Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 8/10, Training Loss: 1.3902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   2%|▏         | 1/63 [00:01<01:05,  1.05s/it]\n",
      "Epoch 9/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 9/10:   5%|▍         | 3/63 [00:03<01:03,  1.05s/it]\n",
      "Epoch 9/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 9/10:   8%|▊         | 5/63 [00:05<01:00,  1.05s/it]\n",
      "Epoch 9/10:  10%|▉         | 6/63 [00:06<00:59,  1.05s/it]\n",
      "Epoch 9/10:  11%|█         | 7/63 [00:07<00:58,  1.05s/it]\n",
      "Epoch 9/10:  13%|█▎        | 8/63 [00:08<00:58,  1.05s/it]\n",
      "Epoch 9/10:  14%|█▍        | 9/63 [00:09<00:56,  1.05s/it]\n",
      "Epoch 9/10:  16%|█▌        | 10/63 [00:10<00:55,  1.05s/it]\n",
      "Epoch 9/10:  17%|█▋        | 11/63 [00:11<00:54,  1.06s/it]\n",
      "Epoch 9/10:  19%|█▉        | 12/63 [00:12<00:53,  1.05s/it]\n",
      "Epoch 9/10:  21%|██        | 13/63 [00:13<00:52,  1.05s/it]\n",
      "Epoch 9/10:  22%|██▏       | 14/63 [00:14<00:51,  1.05s/it]\n",
      "Epoch 9/10:  24%|██▍       | 15/63 [00:15<00:50,  1.05s/it]\n",
      "Epoch 9/10:  25%|██▌       | 16/63 [00:16<00:49,  1.05s/it]\n",
      "Epoch 9/10:  27%|██▋       | 17/63 [00:17<00:47,  1.04s/it]\n",
      "Epoch 9/10:  29%|██▊       | 18/63 [00:18<00:47,  1.05s/it]\n",
      "Epoch 9/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 9/10:  32%|███▏      | 20/63 [00:20<00:45,  1.05s/it]\n",
      "Epoch 9/10:  33%|███▎      | 21/63 [00:22<00:43,  1.05s/it]\n",
      "Epoch 9/10:  35%|███▍      | 22/63 [00:23<00:43,  1.05s/it]\n",
      "Epoch 9/10:  37%|███▋      | 23/63 [00:24<00:41,  1.05s/it]\n",
      "Epoch 9/10:  38%|███▊      | 24/63 [00:25<00:40,  1.04s/it]\n",
      "Epoch 9/10:  40%|███▉      | 25/63 [00:26<00:39,  1.04s/it]\n",
      "Epoch 9/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 9/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 9/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 9/10:  46%|████▌     | 29/63 [00:30<00:35,  1.05s/it]\n",
      "Epoch 9/10:  48%|████▊     | 30/63 [00:31<00:34,  1.05s/it]\n",
      "Epoch 9/10:  49%|████▉     | 31/63 [00:32<00:33,  1.05s/it]\n",
      "Epoch 9/10:  51%|█████     | 32/63 [00:33<00:32,  1.05s/it]\n",
      "Epoch 9/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 9/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 9/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.05s/it]\n",
      "Epoch 9/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.05s/it]\n",
      "Epoch 9/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.05s/it]\n",
      "Epoch 9/10:  60%|██████    | 38/63 [00:39<00:26,  1.04s/it]\n",
      "Epoch 9/10:  62%|██████▏   | 39/63 [00:40<00:25,  1.05s/it]\n",
      "Epoch 9/10:  63%|██████▎   | 40/63 [00:41<00:24,  1.05s/it]\n",
      "Epoch 9/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.04s/it]\n",
      "Epoch 9/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 9/10:  68%|██████▊   | 43/63 [00:45<00:20,  1.04s/it]\n",
      "Epoch 9/10:  70%|██████▉   | 44/63 [00:46<00:19,  1.04s/it]\n",
      "Epoch 9/10:  71%|███████▏  | 45/63 [00:47<00:18,  1.04s/it]\n",
      "Epoch 9/10:  73%|███████▎  | 46/63 [00:48<00:17,  1.03s/it]\n",
      "Epoch 9/10:  75%|███████▍  | 47/63 [00:49<00:16,  1.03s/it]\n",
      "Epoch 9/10:  76%|███████▌  | 48/63 [00:50<00:15,  1.03s/it]\n",
      "Epoch 9/10:  78%|███████▊  | 49/63 [00:51<00:14,  1.03s/it]\n",
      "Epoch 9/10:  79%|███████▉  | 50/63 [00:52<00:13,  1.03s/it]\n",
      "Epoch 9/10:  81%|████████  | 51/63 [00:53<00:12,  1.03s/it]\n",
      "Epoch 9/10:  83%|████████▎ | 52/63 [00:54<00:11,  1.03s/it]\n",
      "Epoch 9/10:  84%|████████▍ | 53/63 [00:55<00:10,  1.03s/it]\n",
      "Epoch 9/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.03s/it]\n",
      "Epoch 9/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.02s/it]\n",
      "Epoch 9/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.02s/it]\n",
      "Epoch 9/10:  90%|█████████ | 57/63 [00:59<00:06,  1.03s/it]\n",
      "Epoch 9/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 9/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 9/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.03s/it]\n",
      "Epoch 9/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 9/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.04s/it]\n",
      "Epoch 9/10: 100%|██████████| 63/63 [01:05<00:00,  1.04s/it]\n",
      "Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 9/10, Training Loss: 1.3916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   2%|▏         | 1/63 [00:01<01:05,  1.05s/it]\n",
      "Epoch 10/10:   3%|▎         | 2/63 [00:02<01:04,  1.06s/it]\n",
      "Epoch 10/10:   5%|▍         | 3/63 [00:03<01:03,  1.06s/it]\n",
      "Epoch 10/10:   6%|▋         | 4/63 [00:04<01:02,  1.05s/it]\n",
      "Epoch 10/10:   8%|▊         | 5/63 [00:05<01:00,  1.05s/it]\n",
      "Epoch 10/10:  10%|▉         | 6/63 [00:06<00:59,  1.05s/it]\n",
      "Epoch 10/10:  11%|█         | 7/63 [00:07<00:58,  1.05s/it]\n",
      "Epoch 10/10:  13%|█▎        | 8/63 [00:08<00:57,  1.05s/it]\n",
      "Epoch 10/10:  14%|█▍        | 9/63 [00:09<00:56,  1.04s/it]\n",
      "Epoch 10/10:  16%|█▌        | 10/63 [00:10<00:55,  1.04s/it]\n",
      "Epoch 10/10:  17%|█▋        | 11/63 [00:11<00:54,  1.05s/it]\n",
      "Epoch 10/10:  19%|█▉        | 12/63 [00:12<00:53,  1.05s/it]\n",
      "Epoch 10/10:  21%|██        | 13/63 [00:13<00:52,  1.04s/it]\n",
      "Epoch 10/10:  22%|██▏       | 14/63 [00:14<00:51,  1.05s/it]\n",
      "Epoch 10/10:  24%|██▍       | 15/63 [00:15<00:50,  1.05s/it]\n",
      "Epoch 10/10:  25%|██▌       | 16/63 [00:16<00:49,  1.05s/it]\n",
      "Epoch 10/10:  27%|██▋       | 17/63 [00:17<00:48,  1.04s/it]\n",
      "Epoch 10/10:  29%|██▊       | 18/63 [00:18<00:47,  1.05s/it]\n",
      "Epoch 10/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 10/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 10/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 10/10:  35%|███▍      | 22/63 [00:23<00:42,  1.04s/it]\n",
      "Epoch 10/10:  37%|███▋      | 23/63 [00:24<00:41,  1.04s/it]\n",
      "Epoch 10/10:  38%|███▊      | 24/63 [00:25<00:40,  1.04s/it]\n",
      "Epoch 10/10:  40%|███▉      | 25/63 [00:26<00:39,  1.05s/it]\n",
      "Epoch 10/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 10/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 10/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 10/10:  46%|████▌     | 29/63 [00:30<00:35,  1.05s/it]\n",
      "Epoch 10/10:  48%|████▊     | 30/63 [00:31<00:34,  1.05s/it]\n",
      "Epoch 10/10:  49%|████▉     | 31/63 [00:32<00:33,  1.05s/it]\n",
      "Epoch 10/10:  51%|█████     | 32/63 [00:33<00:32,  1.05s/it]\n",
      "Epoch 10/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.05s/it]\n",
      "Epoch 10/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.05s/it]\n",
      "Epoch 10/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.05s/it]\n",
      "Epoch 10/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 10/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.04s/it]\n",
      "Epoch 10/10:  60%|██████    | 38/63 [00:39<00:26,  1.04s/it]\n",
      "Epoch 10/10:  62%|██████▏   | 39/63 [00:40<00:25,  1.04s/it]\n",
      "Epoch 10/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 10/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.04s/it]\n",
      "Epoch 10/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 10/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.04s/it]\n",
      "Epoch 10/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 10/10:  71%|███████▏  | 45/63 [00:47<00:18,  1.03s/it]\n",
      "Epoch 10/10:  73%|███████▎  | 46/63 [00:48<00:17,  1.03s/it]\n",
      "Epoch 10/10:  75%|███████▍  | 47/63 [00:49<00:16,  1.03s/it]\n",
      "Epoch 10/10:  76%|███████▌  | 48/63 [00:50<00:15,  1.03s/it]\n",
      "Epoch 10/10:  78%|███████▊  | 49/63 [00:51<00:14,  1.04s/it]\n",
      "Epoch 10/10:  79%|███████▉  | 50/63 [00:52<00:13,  1.04s/it]\n",
      "Epoch 10/10:  81%|████████  | 51/63 [00:53<00:12,  1.04s/it]\n",
      "Epoch 10/10:  83%|████████▎ | 52/63 [00:54<00:11,  1.04s/it]\n",
      "Epoch 10/10:  84%|████████▍ | 53/63 [00:55<00:10,  1.03s/it]\n",
      "Epoch 10/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.04s/it]\n",
      "Epoch 10/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.04s/it]\n",
      "Epoch 10/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.03s/it]\n",
      "Epoch 10/10:  90%|█████████ | 57/63 [00:59<00:06,  1.04s/it]\n",
      "Epoch 10/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 10/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.04s/it]\n",
      "Epoch 10/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.03s/it]\n",
      "Epoch 10/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.04s/it]\n",
      "Epoch 10/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 10/10: 100%|██████████| 63/63 [01:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Epoch 10/10, Training Loss: 1.3900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 10:45:02,347\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'optimiser': ('__ref_ph', '5e5b722a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=24276)\u001b[0m Validation Loss: 1.3883\n",
      "\u001b[36m(train_model pid=21864)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 1/10:   2%|▏         | 1/63 [00:01<01:25,  1.38s/it]\n",
      "Epoch 1/10:   3%|▎         | 2/63 [00:02<01:12,  1.18s/it]\n",
      "Epoch 1/10:   5%|▍         | 3/63 [00:03<01:07,  1.12s/it]\n",
      "Epoch 1/10:   6%|▋         | 4/63 [00:04<01:04,  1.09s/it]\n",
      "Epoch 1/10:   8%|▊         | 5/63 [00:05<01:02,  1.07s/it]\n",
      "Epoch 1/10:  10%|▉         | 6/63 [00:06<01:00,  1.06s/it]\n",
      "Epoch 1/10:  11%|█         | 7/63 [00:07<00:59,  1.06s/it]\n",
      "Epoch 1/10:  13%|█▎        | 8/63 [00:08<00:58,  1.06s/it]\n",
      "Epoch 1/10:  14%|█▍        | 9/63 [00:09<00:57,  1.06s/it]\n",
      "Epoch 1/10:  16%|█▌        | 10/63 [00:10<00:55,  1.05s/it]\n",
      "Epoch 1/10:  17%|█▋        | 11/63 [00:11<00:54,  1.04s/it]\n",
      "Epoch 1/10:  19%|█▉        | 12/63 [00:12<00:53,  1.04s/it]\n",
      "Epoch 1/10:  21%|██        | 13/63 [00:13<00:51,  1.04s/it]\n",
      "Epoch 1/10:  22%|██▏       | 14/63 [00:14<00:51,  1.04s/it]\n",
      "Epoch 1/10:  24%|██▍       | 15/63 [00:15<00:49,  1.04s/it]\n",
      "Epoch 1/10:  25%|██▌       | 16/63 [00:17<00:48,  1.04s/it]\n",
      "Epoch 1/10:  27%|██▋       | 17/63 [00:18<00:47,  1.04s/it]\n",
      "Epoch 1/10:  29%|██▊       | 18/63 [00:19<00:46,  1.04s/it]\n",
      "Epoch 1/10:  30%|███       | 19/63 [00:20<00:45,  1.04s/it]\n",
      "Epoch 1/10:  32%|███▏      | 20/63 [00:21<00:44,  1.04s/it]\n",
      "Epoch 1/10:  33%|███▎      | 21/63 [00:22<00:43,  1.04s/it]\n",
      "Epoch 1/10:  35%|███▍      | 22/63 [00:23<00:42,  1.04s/it]\n",
      "Epoch 1/10:  37%|███▋      | 23/63 [00:24<00:41,  1.04s/it]\n",
      "Epoch 1/10:  38%|███▊      | 24/63 [00:25<00:40,  1.05s/it]\n",
      "Epoch 1/10:  40%|███▉      | 25/63 [00:26<00:39,  1.05s/it]\n",
      "Epoch 1/10:  41%|████▏     | 26/63 [00:27<00:38,  1.05s/it]\n",
      "Epoch 1/10:  43%|████▎     | 27/63 [00:28<00:37,  1.05s/it]\n",
      "Epoch 1/10:  44%|████▍     | 28/63 [00:29<00:36,  1.04s/it]\n",
      "Epoch 1/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 1/10:  48%|████▊     | 30/63 [00:31<00:34,  1.05s/it]\n",
      "Epoch 1/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 1/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 1/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 1/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 1/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 1/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.05s/it]\n",
      "Epoch 1/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.05s/it]\n",
      "Epoch 1/10:  60%|██████    | 38/63 [00:39<00:26,  1.04s/it]\n",
      "Epoch 1/10:  62%|██████▏   | 39/63 [00:41<00:24,  1.04s/it]\n",
      "Epoch 1/10:  63%|██████▎   | 40/63 [00:42<00:23,  1.04s/it]\n",
      "Epoch 1/10:  65%|██████▌   | 41/63 [00:43<00:22,  1.03s/it]\n",
      "Epoch 1/10:  67%|██████▋   | 42/63 [00:44<00:21,  1.03s/it]\n",
      "Epoch 1/10:  68%|██████▊   | 43/63 [00:45<00:20,  1.03s/it]\n",
      "Epoch 1/10:  70%|██████▉   | 44/63 [00:46<00:19,  1.03s/it]\n",
      "Epoch 1/10:  71%|███████▏  | 45/63 [00:47<00:18,  1.03s/it]\n",
      "Epoch 1/10:  73%|███████▎  | 46/63 [00:48<00:17,  1.03s/it]\n",
      "Epoch 1/10:  75%|███████▍  | 47/63 [00:49<00:16,  1.03s/it]\n",
      "Epoch 1/10:  76%|███████▌  | 48/63 [00:50<00:15,  1.03s/it]\n",
      "Epoch 1/10:  78%|███████▊  | 49/63 [00:51<00:14,  1.03s/it]\n",
      "Epoch 1/10:  79%|███████▉  | 50/63 [00:52<00:13,  1.03s/it]\n",
      "Epoch 1/10:  81%|████████  | 51/63 [00:53<00:12,  1.03s/it]\n",
      "Epoch 1/10:  83%|████████▎ | 52/63 [00:54<00:11,  1.03s/it]\n",
      "Epoch 1/10:  84%|████████▍ | 53/63 [00:55<00:10,  1.03s/it]\n",
      "Epoch 1/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.03s/it]\n",
      "Epoch 1/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.04s/it]\n",
      "Epoch 1/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.03s/it]\n",
      "Epoch 1/10:  90%|█████████ | 57/63 [00:59<00:06,  1.03s/it]\n",
      "Epoch 1/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 1/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.02s/it]\n",
      "Epoch 1/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.02s/it]\n",
      "Epoch 1/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.02s/it]\n",
      "Epoch 1/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.02s/it]\n",
      "Epoch 1/10: 100%|██████████| 63/63 [01:05<00:00,  1.04s/it]\n",
      "Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 1/10, Training Loss: 1.3888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 2/10:   3%|▎         | 2/63 [00:02<01:04,  1.05s/it]\n",
      "Epoch 2/10:   5%|▍         | 3/63 [00:03<01:03,  1.05s/it]\n",
      "Epoch 2/10:   6%|▋         | 4/63 [00:04<01:01,  1.05s/it]\n",
      "Epoch 2/10:   8%|▊         | 5/63 [00:05<01:00,  1.05s/it]\n",
      "Epoch 2/10:  10%|▉         | 6/63 [00:06<00:59,  1.05s/it]\n",
      "Epoch 2/10:  11%|█         | 7/63 [00:07<00:58,  1.05s/it]\n",
      "Epoch 2/10:  13%|█▎        | 8/63 [00:08<00:57,  1.05s/it]\n",
      "Epoch 2/10:  14%|█▍        | 9/63 [00:09<00:56,  1.05s/it]\n",
      "Epoch 2/10:  16%|█▌        | 10/63 [00:10<00:55,  1.05s/it]\n",
      "Epoch 2/10:  17%|█▋        | 11/63 [00:11<00:54,  1.04s/it]\n",
      "Epoch 2/10:  19%|█▉        | 12/63 [00:12<00:53,  1.04s/it]\n",
      "Epoch 2/10:  21%|██        | 13/63 [00:13<00:52,  1.04s/it]\n",
      "Epoch 2/10:  22%|██▏       | 14/63 [00:14<00:50,  1.04s/it]\n",
      "Epoch 2/10:  24%|██▍       | 15/63 [00:15<00:49,  1.04s/it]\n",
      "Epoch 2/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 2/10:  27%|██▋       | 17/63 [00:17<00:47,  1.04s/it]\n",
      "Epoch 2/10:  29%|██▊       | 18/63 [00:18<00:46,  1.04s/it]\n",
      "Epoch 2/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 2/10:  32%|███▏      | 20/63 [00:20<00:44,  1.05s/it]\n",
      "Epoch 2/10:  33%|███▎      | 21/63 [00:21<00:43,  1.05s/it]\n",
      "Epoch 2/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 2/10:  37%|███▋      | 23/63 [00:24<00:41,  1.04s/it]\n",
      "Epoch 2/10:  38%|███▊      | 24/63 [00:25<00:40,  1.04s/it]\n",
      "Epoch 2/10:  40%|███▉      | 25/63 [00:26<00:39,  1.04s/it]\n",
      "Epoch 2/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 2/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 2/10:  44%|████▍     | 28/63 [00:29<00:36,  1.03s/it]\n",
      "Epoch 2/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 2/10:  48%|████▊     | 30/63 [00:31<00:34,  1.04s/it]\n",
      "Epoch 2/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 2/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 2/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 2/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 2/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 2/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 2/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.04s/it]\n",
      "Epoch 2/10:  60%|██████    | 38/63 [00:39<00:26,  1.04s/it]\n",
      "Epoch 2/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 2/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 2/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.04s/it]\n",
      "Epoch 2/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 2/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 2/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 2/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 2/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 2/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 2/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 2/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.03s/it]\n",
      "Epoch 2/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 2/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 2/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 2/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 2/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.03s/it]\n",
      "Epoch 2/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.03s/it]\n",
      "Epoch 2/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.03s/it]\n",
      "Epoch 2/10:  90%|█████████ | 57/63 [00:59<00:06,  1.03s/it]\n",
      "Epoch 2/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 2/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.05s/it]\n",
      "Epoch 2/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.04s/it]\n",
      "Epoch 2/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 2/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 2/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 2/10, Training Loss: 1.3889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 3/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 3/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 3/10:   6%|▋         | 4/63 [00:04<01:01,  1.03s/it]\n",
      "Epoch 3/10:   8%|▊         | 5/63 [00:05<00:59,  1.03s/it]\n",
      "Epoch 3/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 3/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 3/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 3/10:  14%|█▍        | 9/63 [00:09<00:55,  1.04s/it]\n",
      "Epoch 3/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 3/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 3/10:  19%|█▉        | 12/63 [00:12<00:52,  1.04s/it]\n",
      "Epoch 3/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 3/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 3/10:  24%|██▍       | 15/63 [00:15<00:49,  1.04s/it]\n",
      "Epoch 3/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 3/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 3/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 3/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 3/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 3/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 3/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 3/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 3/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 3/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 3/10:  41%|████▏     | 26/63 [00:26<00:38,  1.03s/it]\n",
      "Epoch 3/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 3/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 3/10:  46%|████▌     | 29/63 [00:30<00:35,  1.04s/it]\n",
      "Epoch 3/10:  48%|████▊     | 30/63 [00:31<00:34,  1.04s/it]\n",
      "Epoch 3/10:  49%|████▉     | 31/63 [00:32<00:33,  1.03s/it]\n",
      "Epoch 3/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 3/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.03s/it]\n",
      "Epoch 3/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 3/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 3/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.02s/it]\n",
      "Epoch 3/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 3/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 3/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 3/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 3/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 3/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 3/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 3/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 3/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 3/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 3/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 3/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 3/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 3/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 3/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 3/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 3/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 3/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 3/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 3/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 3/10:  90%|█████████ | 57/63 [00:58<00:06,  1.02s/it]\n",
      "Epoch 3/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.02s/it]\n",
      "Epoch 3/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 3/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 3/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.01s/it]\n",
      "Epoch 3/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.01s/it]\n",
      "Epoch 3/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 3/10, Training Loss: 1.3888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   2%|▏         | 1/63 [00:01<01:04,  1.03s/it]\n",
      "Epoch 4/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 4/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 4/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 4/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 4/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 4/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 4/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 4/10:  14%|█▍        | 9/63 [00:09<00:56,  1.04s/it]\n",
      "Epoch 4/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 4/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 4/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 4/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 4/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 4/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 4/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 4/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 4/10:  29%|██▊       | 18/63 [00:18<00:46,  1.04s/it]\n",
      "Epoch 4/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 4/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 4/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 4/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 4/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 4/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 4/10:  40%|███▉      | 25/63 [00:25<00:39,  1.04s/it]\n",
      "Epoch 4/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 4/10:  43%|████▎     | 27/63 [00:27<00:37,  1.04s/it]\n",
      "Epoch 4/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 4/10:  46%|████▌     | 29/63 [00:29<00:34,  1.03s/it]\n",
      "Epoch 4/10:  48%|████▊     | 30/63 [00:31<00:34,  1.03s/it]\n",
      "Epoch 4/10:  49%|████▉     | 31/63 [00:32<00:32,  1.03s/it]\n",
      "Epoch 4/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 4/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.03s/it]\n",
      "Epoch 4/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 4/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 4/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 4/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.04s/it]\n",
      "Epoch 4/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 4/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 4/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 4/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 4/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 4/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 4/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 4/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 4/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 4/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 4/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 4/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.01s/it]\n",
      "Epoch 4/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.01s/it]\n",
      "Epoch 4/10:  81%|████████  | 51/63 [00:52<00:12,  1.01s/it]\n",
      "Epoch 4/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 4/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 4/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.04s/it]\n",
      "Epoch 4/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 4/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.03s/it]\n",
      "Epoch 4/10:  90%|█████████ | 57/63 [00:58<00:06,  1.04s/it]\n",
      "Epoch 4/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.04s/it]\n",
      "Epoch 4/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.05s/it]\n",
      "Epoch 4/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.05s/it]\n",
      "Epoch 4/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.06s/it]\n",
      "Epoch 4/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.06s/it]\n",
      "Epoch 4/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 4/10, Training Loss: 1.3868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   2%|▏         | 1/63 [00:01<01:05,  1.06s/it]\n",
      "Epoch 5/10:   3%|▎         | 2/63 [00:02<01:04,  1.05s/it]\n",
      "Epoch 5/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 5/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 5/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 5/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 5/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 5/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 5/10:  14%|█▍        | 9/63 [00:09<00:56,  1.04s/it]\n",
      "Epoch 5/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 5/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 5/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 5/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 5/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 5/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 5/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 5/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 5/10:  29%|██▊       | 18/63 [00:18<00:46,  1.02s/it]\n",
      "Epoch 5/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 5/10:  32%|███▏      | 20/63 [00:20<00:44,  1.02s/it]\n",
      "Epoch 5/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 5/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 5/10:  37%|███▋      | 23/63 [00:23<00:41,  1.03s/it]\n",
      "Epoch 5/10:  38%|███▊      | 24/63 [00:24<00:40,  1.03s/it]\n",
      "Epoch 5/10:  40%|███▉      | 25/63 [00:25<00:38,  1.03s/it]\n",
      "Epoch 5/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 5/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 5/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 5/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 5/10:  48%|████▊     | 30/63 [00:30<00:33,  1.02s/it]\n",
      "Epoch 5/10:  49%|████▉     | 31/63 [00:31<00:32,  1.02s/it]\n",
      "Epoch 5/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 5/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.02s/it]\n",
      "Epoch 5/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.03s/it]\n",
      "Epoch 5/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.02s/it]\n",
      "Epoch 5/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.02s/it]\n",
      "Epoch 5/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.02s/it]\n",
      "Epoch 5/10:  60%|██████    | 38/63 [00:38<00:25,  1.01s/it]\n",
      "Epoch 5/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.02s/it]\n",
      "Epoch 5/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.01s/it]\n",
      "Epoch 5/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 5/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 5/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 5/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.01s/it]\n",
      "Epoch 5/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 5/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 5/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 5/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 5/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 5/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 5/10:  81%|████████  | 51/63 [00:52<00:12,  1.04s/it]\n",
      "Epoch 5/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.04s/it]\n",
      "Epoch 5/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 5/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 5/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 5/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 5/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 5/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 5/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 5/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it]\n",
      "Epoch 5/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 5/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.03s/it]\n",
      "Epoch 5/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 5/10, Training Loss: 1.3846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   2%|▏         | 1/63 [00:01<01:03,  1.02s/it]\n",
      "Epoch 6/10:   3%|▎         | 2/63 [00:02<01:02,  1.02s/it]\n",
      "Epoch 6/10:   5%|▍         | 3/63 [00:03<01:01,  1.02s/it]\n",
      "Epoch 6/10:   6%|▋         | 4/63 [00:04<01:00,  1.02s/it]\n",
      "Epoch 6/10:   8%|▊         | 5/63 [00:05<00:59,  1.02s/it]\n",
      "Epoch 6/10:  10%|▉         | 6/63 [00:06<00:57,  1.01s/it]\n",
      "Epoch 6/10:  11%|█         | 7/63 [00:07<00:57,  1.02s/it]\n",
      "Epoch 6/10:  13%|█▎        | 8/63 [00:08<00:56,  1.02s/it]\n",
      "Epoch 6/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 6/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 6/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 6/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 6/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 6/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 6/10:  24%|██▍       | 15/63 [00:15<00:49,  1.02s/it]\n",
      "Epoch 6/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 6/10:  27%|██▋       | 17/63 [00:17<00:47,  1.02s/it]\n",
      "Epoch 6/10:  29%|██▊       | 18/63 [00:18<00:46,  1.02s/it]\n",
      "Epoch 6/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 6/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 6/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 6/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 6/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 6/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 6/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 6/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 6/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 6/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 6/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 6/10:  48%|████▊     | 30/63 [00:30<00:33,  1.02s/it]\n",
      "Epoch 6/10:  49%|████▉     | 31/63 [00:31<00:32,  1.02s/it]\n",
      "Epoch 6/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 6/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.03s/it]\n",
      "Epoch 6/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.02s/it]\n",
      "Epoch 6/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.02s/it]\n",
      "Epoch 6/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.02s/it]\n",
      "Epoch 6/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.03s/it]\n",
      "Epoch 6/10:  60%|██████    | 38/63 [00:38<00:25,  1.03s/it]\n",
      "Epoch 6/10:  62%|██████▏   | 39/63 [00:39<00:24,  1.03s/it]\n",
      "Epoch 6/10:  63%|██████▎   | 40/63 [00:40<00:23,  1.03s/it]\n",
      "Epoch 6/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 6/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 6/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 6/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 6/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 6/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 6/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 6/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 6/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 6/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 6/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 6/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 6/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 6/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 6/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 6/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 6/10:  90%|█████████ | 57/63 [00:58<00:06,  1.02s/it]\n",
      "Epoch 6/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.02s/it]\n",
      "Epoch 6/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 6/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 6/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.02s/it]\n",
      "Epoch 6/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 6/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 6/10, Training Loss: 1.3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 7/10:   2%|▏         | 1/63 [00:01<01:02,  1.00s/it]\n",
      "Epoch 7/10:   3%|▎         | 2/63 [00:02<01:02,  1.02s/it]\n",
      "Epoch 7/10:   5%|▍         | 3/63 [00:03<01:01,  1.02s/it]\n",
      "Epoch 7/10:   6%|▋         | 4/63 [00:04<01:00,  1.02s/it]\n",
      "Epoch 7/10:   8%|▊         | 5/63 [00:05<00:58,  1.01s/it]\n",
      "Epoch 7/10:  10%|▉         | 6/63 [00:06<00:58,  1.02s/it]\n",
      "Epoch 7/10:  11%|█         | 7/63 [00:07<00:57,  1.02s/it]\n",
      "Epoch 7/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 7/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 7/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 7/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 7/10:  19%|█▉        | 12/63 [00:12<00:52,  1.02s/it]\n",
      "Epoch 7/10:  21%|██        | 13/63 [00:13<00:51,  1.02s/it]\n",
      "Epoch 7/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 7/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 7/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 7/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 7/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 7/10:  30%|███       | 19/63 [00:19<00:44,  1.02s/it]\n",
      "Epoch 7/10:  32%|███▏      | 20/63 [00:20<00:43,  1.02s/it]\n",
      "Epoch 7/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 7/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 7/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 7/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 7/10:  40%|███▉      | 25/63 [00:25<00:38,  1.03s/it]\n",
      "Epoch 7/10:  41%|████▏     | 26/63 [00:26<00:38,  1.03s/it]\n",
      "Epoch 7/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 7/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 7/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 7/10:  48%|████▊     | 30/63 [00:30<00:33,  1.03s/it]\n",
      "Epoch 7/10:  49%|████▉     | 31/63 [00:31<00:32,  1.02s/it]\n",
      "Epoch 7/10:  51%|█████     | 32/63 [00:32<00:31,  1.03s/it]\n",
      "Epoch 7/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.02s/it]\n",
      "Epoch 7/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.03s/it]\n",
      "Epoch 7/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.02s/it]\n",
      "Epoch 7/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.03s/it]\n",
      "Epoch 7/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.02s/it]\n",
      "Epoch 7/10:  60%|██████    | 38/63 [00:38<00:25,  1.02s/it]\n",
      "Epoch 7/10:  62%|██████▏   | 39/63 [00:39<00:24,  1.02s/it]\n",
      "Epoch 7/10:  63%|██████▎   | 40/63 [00:40<00:23,  1.02s/it]\n",
      "Epoch 7/10:  65%|██████▌   | 41/63 [00:41<00:22,  1.02s/it]\n",
      "Epoch 7/10:  67%|██████▋   | 42/63 [00:42<00:21,  1.02s/it]\n",
      "Epoch 7/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 7/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 7/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 7/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 7/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 7/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 7/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.01s/it]\n",
      "Epoch 7/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 7/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 7/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 7/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 7/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 7/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 7/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.03s/it]\n",
      "Epoch 7/10:  90%|█████████ | 57/63 [00:58<00:06,  1.02s/it]\n",
      "Epoch 7/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 7/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 7/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 7/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.02s/it]\n",
      "Epoch 7/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.01s/it]\n",
      "Epoch 7/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 7/10, Training Loss: 1.3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 8/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 8/10:   5%|▍         | 3/63 [00:03<01:01,  1.03s/it]\n",
      "Epoch 8/10:   6%|▋         | 4/63 [00:04<01:00,  1.02s/it]\n",
      "Epoch 8/10:   8%|▊         | 5/63 [00:05<00:59,  1.02s/it]\n",
      "Epoch 8/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 8/10:  11%|█         | 7/63 [00:07<00:57,  1.02s/it]\n",
      "Epoch 8/10:  13%|█▎        | 8/63 [00:08<00:56,  1.02s/it]\n",
      "Epoch 8/10:  14%|█▍        | 9/63 [00:09<00:55,  1.03s/it]\n",
      "Epoch 8/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 8/10:  17%|█▋        | 11/63 [00:11<00:53,  1.02s/it]\n",
      "Epoch 8/10:  19%|█▉        | 12/63 [00:12<00:51,  1.02s/it]\n",
      "Epoch 8/10:  21%|██        | 13/63 [00:13<00:50,  1.01s/it]\n",
      "Epoch 8/10:  22%|██▏       | 14/63 [00:14<00:50,  1.02s/it]\n",
      "Epoch 8/10:  24%|██▍       | 15/63 [00:15<00:49,  1.02s/it]\n",
      "Epoch 8/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 8/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 8/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 8/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 8/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 8/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 8/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 8/10:  37%|███▋      | 23/63 [00:23<00:41,  1.03s/it]\n",
      "Epoch 8/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 8/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 8/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 8/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 8/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 8/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 8/10:  48%|████▊     | 30/63 [00:30<00:33,  1.02s/it]\n",
      "Epoch 8/10:  49%|████▉     | 31/63 [00:31<00:32,  1.03s/it]\n",
      "Epoch 8/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 8/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.02s/it]\n",
      "Epoch 8/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.03s/it]\n",
      "Epoch 8/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.02s/it]\n",
      "Epoch 8/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.02s/it]\n",
      "Epoch 8/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.02s/it]\n",
      "Epoch 8/10:  60%|██████    | 38/63 [00:38<00:25,  1.02s/it]\n",
      "Epoch 8/10:  62%|██████▏   | 39/63 [00:39<00:24,  1.02s/it]\n",
      "Epoch 8/10:  63%|██████▎   | 40/63 [00:40<00:23,  1.03s/it]\n",
      "Epoch 8/10:  65%|██████▌   | 41/63 [00:41<00:22,  1.03s/it]\n",
      "Epoch 8/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 8/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 8/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 8/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 8/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 8/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 8/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.04s/it]\n",
      "Epoch 8/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 8/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.04s/it]\n",
      "Epoch 8/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 8/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 8/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 8/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 8/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 8/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 8/10:  90%|█████████ | 57/63 [00:58<00:06,  1.02s/it]\n",
      "Epoch 8/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.02s/it]\n",
      "Epoch 8/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 8/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 8/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.02s/it]\n",
      "Epoch 8/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 8/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 8/10, Training Loss: 1.3841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   2%|▏         | 1/63 [00:01<01:03,  1.02s/it]\n",
      "Epoch 9/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 9/10:   5%|▍         | 3/63 [00:03<01:01,  1.03s/it]\n",
      "Epoch 9/10:   6%|▋         | 4/63 [00:04<01:00,  1.03s/it]\n",
      "Epoch 9/10:   8%|▊         | 5/63 [00:05<00:59,  1.03s/it]\n",
      "Epoch 9/10:  10%|▉         | 6/63 [00:06<00:58,  1.02s/it]\n",
      "Epoch 9/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 9/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 9/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 9/10:  16%|█▌        | 10/63 [00:10<00:54,  1.02s/it]\n",
      "Epoch 9/10:  17%|█▋        | 11/63 [00:11<00:53,  1.02s/it]\n",
      "Epoch 9/10:  19%|█▉        | 12/63 [00:12<00:52,  1.02s/it]\n",
      "Epoch 9/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 9/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 9/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 9/10:  25%|██▌       | 16/63 [00:16<00:48,  1.02s/it]\n",
      "Epoch 9/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 9/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 9/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 9/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 9/10:  33%|███▎      | 21/63 [00:21<00:42,  1.02s/it]\n",
      "Epoch 9/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 9/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 9/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 9/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 9/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 9/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 9/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 9/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 9/10:  48%|████▊     | 30/63 [00:30<00:33,  1.02s/it]\n",
      "Epoch 9/10:  49%|████▉     | 31/63 [00:31<00:32,  1.02s/it]\n",
      "Epoch 9/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 9/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.02s/it]\n",
      "Epoch 9/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.02s/it]\n",
      "Epoch 9/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.02s/it]\n",
      "Epoch 9/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.03s/it]\n",
      "Epoch 9/10:  59%|█████▊    | 37/63 [00:37<00:27,  1.06s/it]\n",
      "Epoch 9/10:  60%|██████    | 38/63 [00:38<00:26,  1.05s/it]\n",
      "Epoch 9/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 9/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 9/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 9/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 9/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.04s/it]\n",
      "Epoch 9/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 9/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 9/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 9/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 9/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.01s/it]\n",
      "Epoch 9/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.01s/it]\n",
      "Epoch 9/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.01s/it]\n",
      "Epoch 9/10:  81%|████████  | 51/63 [00:52<00:12,  1.01s/it]\n",
      "Epoch 9/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.01s/it]\n",
      "Epoch 9/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.01s/it]\n",
      "Epoch 9/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.01s/it]\n",
      "Epoch 9/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.01s/it]\n",
      "Epoch 9/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.01s/it]\n",
      "Epoch 9/10:  90%|█████████ | 57/63 [00:58<00:06,  1.01s/it]\n",
      "Epoch 9/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.01s/it]\n",
      "Epoch 9/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.01s/it]\n",
      "Epoch 9/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.00s/it]\n",
      "Epoch 9/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.00s/it]\n",
      "Epoch 9/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.00s/it]\n",
      "Epoch 9/10: 100%|██████████| 63/63 [01:03<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 9/10, Training Loss: 1.3820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 10/10:   2%|▏         | 1/63 [00:01<01:03,  1.02s/it]\n",
      "Epoch 10/10:   3%|▎         | 2/63 [00:02<01:02,  1.02s/it]\n",
      "Epoch 10/10:   5%|▍         | 3/63 [00:03<01:01,  1.02s/it]\n",
      "Epoch 10/10:   6%|▋         | 4/63 [00:04<00:59,  1.01s/it]\n",
      "Epoch 10/10:   8%|▊         | 5/63 [00:05<00:59,  1.02s/it]\n",
      "Epoch 10/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 10/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 10/10:  13%|█▎        | 8/63 [00:08<00:56,  1.02s/it]\n",
      "Epoch 10/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 10/10:  16%|█▌        | 10/63 [00:10<00:53,  1.02s/it]\n",
      "Epoch 10/10:  17%|█▋        | 11/63 [00:11<00:52,  1.01s/it]\n",
      "Epoch 10/10:  19%|█▉        | 12/63 [00:12<00:51,  1.02s/it]\n",
      "Epoch 10/10:  21%|██        | 13/63 [00:13<00:50,  1.01s/it]\n",
      "Epoch 10/10:  22%|██▏       | 14/63 [00:14<00:49,  1.01s/it]\n",
      "Epoch 10/10:  24%|██▍       | 15/63 [00:15<00:48,  1.01s/it]\n",
      "Epoch 10/10:  25%|██▌       | 16/63 [00:16<00:47,  1.01s/it]\n",
      "Epoch 10/10:  27%|██▋       | 17/63 [00:17<00:46,  1.01s/it]\n",
      "Epoch 10/10:  29%|██▊       | 18/63 [00:18<00:45,  1.01s/it]\n",
      "Epoch 10/10:  30%|███       | 19/63 [00:19<00:44,  1.01s/it]\n",
      "Epoch 10/10:  32%|███▏      | 20/63 [00:20<00:43,  1.01s/it]\n",
      "Epoch 10/10:  33%|███▎      | 21/63 [00:21<00:42,  1.01s/it]\n",
      "Epoch 10/10:  35%|███▍      | 22/63 [00:22<00:41,  1.01s/it]\n",
      "Epoch 10/10:  37%|███▋      | 23/63 [00:23<00:40,  1.01s/it]\n",
      "Epoch 10/10:  38%|███▊      | 24/63 [00:24<00:39,  1.01s/it]\n",
      "Epoch 10/10:  40%|███▉      | 25/63 [00:25<00:38,  1.00s/it]\n",
      "Epoch 10/10:  41%|████▏     | 26/63 [00:26<00:37,  1.01s/it]\n",
      "Epoch 10/10:  43%|████▎     | 27/63 [00:27<00:36,  1.01s/it]\n",
      "Epoch 10/10:  44%|████▍     | 28/63 [00:28<00:35,  1.01s/it]\n",
      "Epoch 10/10:  46%|████▌     | 29/63 [00:29<00:34,  1.01s/it]\n",
      "Epoch 10/10:  48%|████▊     | 30/63 [00:30<00:33,  1.01s/it]\n",
      "Epoch 10/10:  49%|████▉     | 31/63 [00:31<00:32,  1.01s/it]\n",
      "Epoch 10/10:  51%|█████     | 32/63 [00:32<00:31,  1.01s/it]\n",
      "Epoch 10/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.01s/it]\n",
      "Epoch 10/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.01s/it]\n",
      "Epoch 10/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.02s/it]\n",
      "Epoch 10/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.01s/it]\n",
      "Epoch 10/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.02s/it]\n",
      "Epoch 10/10:  60%|██████    | 38/63 [00:38<00:25,  1.02s/it]\n",
      "Epoch 10/10:  62%|██████▏   | 39/63 [00:39<00:24,  1.01s/it]\n",
      "Epoch 10/10:  63%|██████▎   | 40/63 [00:40<00:23,  1.01s/it]\n",
      "Epoch 10/10:  65%|██████▌   | 41/63 [00:41<00:22,  1.01s/it]\n",
      "Epoch 10/10:  67%|██████▋   | 42/63 [00:42<00:21,  1.01s/it]\n",
      "Epoch 10/10:  68%|██████▊   | 43/63 [00:43<00:20,  1.00s/it]\n",
      "Epoch 10/10:  70%|██████▉   | 44/63 [00:44<00:19,  1.00s/it]\n",
      "Epoch 10/10:  71%|███████▏  | 45/63 [00:45<00:18,  1.01s/it]\n",
      "Epoch 10/10:  73%|███████▎  | 46/63 [00:46<00:17,  1.01s/it]\n",
      "Epoch 10/10:  75%|███████▍  | 47/63 [00:47<00:16,  1.01s/it]\n",
      "Epoch 10/10:  76%|███████▌  | 48/63 [00:48<00:15,  1.00s/it]\n",
      "Epoch 10/10:  78%|███████▊  | 49/63 [00:49<00:13,  1.00it/s]\n",
      "Epoch 10/10:  79%|███████▉  | 50/63 [00:50<00:13,  1.00s/it]\n",
      "Epoch 10/10:  81%|████████  | 51/63 [00:51<00:12,  1.00s/it]\n",
      "Epoch 10/10:  83%|████████▎ | 52/63 [00:52<00:11,  1.00s/it]\n",
      "Epoch 10/10:  84%|████████▍ | 53/63 [00:53<00:10,  1.01s/it]\n",
      "Epoch 10/10:  86%|████████▌ | 54/63 [00:54<00:09,  1.01s/it]\n",
      "Epoch 10/10:  87%|████████▋ | 55/63 [00:55<00:08,  1.01s/it]\n",
      "Epoch 10/10:  89%|████████▉ | 56/63 [00:56<00:07,  1.01s/it]\n",
      "Epoch 10/10:  90%|█████████ | 57/63 [00:57<00:06,  1.00s/it]\n",
      "Epoch 10/10:  92%|█████████▏| 58/63 [00:58<00:05,  1.00s/it]\n",
      "Epoch 10/10:  94%|█████████▎| 59/63 [00:59<00:04,  1.00s/it]\n",
      "Epoch 10/10:  95%|█████████▌| 60/63 [01:00<00:03,  1.00s/it]\n",
      "Epoch 10/10:  97%|█████████▋| 61/63 [01:01<00:02,  1.01s/it]\n",
      "Epoch 10/10:  98%|█████████▊| 62/63 [01:02<00:01,  1.02s/it]\n",
      "Epoch 10/10: 100%|██████████| 63/63 [01:03<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Epoch 10/10, Training Loss: 1.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 10:55:55,249\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'optimiser': ('__ref_ph', '5e5b722a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=21864)\u001b[0m Validation Loss: 1.3791\n",
      "\u001b[36m(train_model pid=20840)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 10:55:58,973\tERROR tune_controller.py:1331 -- Trial task failed for trial t_c8c52f13\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\_private\\worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\_private\\worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ValueError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=20840, ip=127.0.0.1, actor_id=c458bd4fb5d7731e27e0f1d601000000, repr=train_model)\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1895, in ray._raylet.execute_task\n",
      "  File \"python\\\\ray\\\\_raylet.pyx\", line 1835, in ray._raylet.execute_task.function_executor\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\_private\\function_manager.py\", line 689, in actor_method_executor\n",
      "    return method(__ray_actor, *args, **kwargs)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 463, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py\", line 330, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\air\\_internal\\util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\util\\tracing\\tracing_helper.py\", line 463, in _resume_span\n",
      "    return method(self, *_args, **_kwargs)\n",
      "  File \"c:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\ray\\tune\\trainable\\function_trainable.py\", line 261, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"C:\\Users\\yongl\\AppData\\Local\\Temp\\ipykernel_4820\\2516378235.py\", line 35, in train_model\n",
      "  File \"C:\\Users\\yongl\\AppData\\Local\\Temp\\ipykernel_4820\\4122356310.py\", line 96, in __init__\n",
      "  File \"C:\\Users\\yongl\\AppData\\Local\\Temp\\ipykernel_4820\\4122356310.py\", line 125, in _get_activation\n",
      "ValueError: Unsupported activation function: leakyrelu\n",
      "Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 1/10:   2%|▏         | 1/63 [00:01<01:23,  1.35s/it]\n",
      "Epoch 1/10:   3%|▎         | 2/63 [00:02<01:10,  1.16s/it]\n",
      "Epoch 1/10:   5%|▍         | 3/63 [00:03<01:06,  1.10s/it]\n",
      "Epoch 1/10:   6%|▋         | 4/63 [00:04<01:02,  1.07s/it]\n",
      "Epoch 1/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 1/10:  10%|▉         | 6/63 [00:06<00:58,  1.02s/it]\n",
      "Epoch 1/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 1/10:  13%|█▎        | 8/63 [00:08<00:56,  1.02s/it]\n",
      "Epoch 1/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 1/10:  16%|█▌        | 10/63 [00:10<00:54,  1.02s/it]\n",
      "Epoch 1/10:  17%|█▋        | 11/63 [00:11<00:53,  1.02s/it]\n",
      "Epoch 1/10:  19%|█▉        | 12/63 [00:12<00:52,  1.02s/it]\n",
      "Epoch 1/10:  21%|██        | 13/63 [00:13<00:51,  1.02s/it]\n",
      "Epoch 1/10:  22%|██▏       | 14/63 [00:14<00:49,  1.02s/it]\n",
      "Epoch 1/10:  24%|██▍       | 15/63 [00:15<00:48,  1.02s/it]\n",
      "Epoch 1/10:  25%|██▌       | 16/63 [00:16<00:47,  1.02s/it]\n",
      "Epoch 1/10:  27%|██▋       | 17/63 [00:17<00:46,  1.02s/it]\n",
      "Epoch 1/10:  29%|██▊       | 18/63 [00:18<00:45,  1.02s/it]\n",
      "Epoch 1/10:  30%|███       | 19/63 [00:19<00:44,  1.02s/it]\n",
      "Epoch 1/10:  32%|███▏      | 20/63 [00:20<00:43,  1.01s/it]\n",
      "Epoch 1/10:  33%|███▎      | 21/63 [00:21<00:42,  1.01s/it]\n",
      "Epoch 1/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 1/10:  37%|███▋      | 23/63 [00:23<00:41,  1.03s/it]\n",
      "Epoch 1/10:  38%|███▊      | 24/63 [00:24<00:40,  1.03s/it]\n",
      "Epoch 1/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 1/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 1/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 1/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 1/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 1/10:  48%|████▊     | 30/63 [00:30<00:34,  1.03s/it]\n",
      "Epoch 1/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 1/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 1/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 1/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 1/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 1/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 1/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.04s/it]\n",
      "Epoch 1/10:  60%|██████    | 38/63 [00:39<00:26,  1.04s/it]\n",
      "Epoch 1/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 1/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 1/10:  65%|██████▌   | 41/63 [00:42<00:23,  1.05s/it]\n",
      "Epoch 1/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.05s/it]\n",
      "Epoch 1/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.05s/it]\n",
      "Epoch 1/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.05s/it]\n",
      "Epoch 1/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.05s/it]\n",
      "Epoch 1/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.05s/it]\n",
      "Epoch 1/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.05s/it]\n",
      "Epoch 1/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.05s/it]\n",
      "Epoch 1/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 1/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.05s/it]\n",
      "Epoch 1/10:  81%|████████  | 51/63 [00:52<00:12,  1.04s/it]\n",
      "Epoch 1/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.04s/it]\n",
      "Epoch 1/10:  84%|████████▍ | 53/63 [00:55<00:10,  1.05s/it]\n",
      "Epoch 1/10:  86%|████████▌ | 54/63 [00:56<00:09,  1.05s/it]\n",
      "Epoch 1/10:  87%|████████▋ | 55/63 [00:57<00:08,  1.04s/it]\n",
      "Epoch 1/10:  89%|████████▉ | 56/63 [00:58<00:07,  1.05s/it]\n",
      "Epoch 1/10:  90%|█████████ | 57/63 [00:59<00:06,  1.05s/it]\n",
      "Epoch 1/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.05s/it]\n",
      "Epoch 1/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.04s/it]\n",
      "Epoch 1/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.04s/it]\n",
      "Epoch 1/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.04s/it]\n",
      "Epoch 1/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.04s/it]\n",
      "Epoch 1/10: 100%|██████████| 63/63 [01:05<00:00,  1.03s/it]\n",
      "Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 1/10, Training Loss: 1.3906\n",
      "\u001b[36m(train_model pid=7692)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   2%|▏         | 1/63 [00:01<01:04,  1.04s/it]\n",
      "Epoch 2/10:   3%|▎         | 2/63 [00:02<01:03,  1.05s/it]\n",
      "Epoch 2/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 2/10:   6%|▋         | 4/63 [00:04<01:02,  1.05s/it]\n",
      "Epoch 2/10:   8%|▊         | 5/63 [00:05<01:01,  1.05s/it]\n",
      "Epoch 2/10:  10%|▉         | 6/63 [00:06<00:59,  1.05s/it]\n",
      "Epoch 2/10:  11%|█         | 7/63 [00:07<00:58,  1.05s/it]\n",
      "Epoch 2/10:  13%|█▎        | 8/63 [00:08<00:57,  1.05s/it]\n",
      "Epoch 2/10:  14%|█▍        | 9/63 [00:09<00:56,  1.04s/it]\n",
      "Epoch 2/10:  16%|█▌        | 10/63 [00:10<00:54,  1.04s/it]\n",
      "Epoch 2/10:  17%|█▋        | 11/63 [00:11<00:53,  1.04s/it]\n",
      "Epoch 2/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 2/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 2/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 2/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 2/10:  25%|██▌       | 16/63 [00:16<00:48,  1.02s/it]\n",
      "Epoch 2/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 2/10:  29%|██▊       | 18/63 [00:18<00:46,  1.02s/it]\n",
      "Epoch 2/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 2/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 2/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 2/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 2/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 2/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 2/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 2/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 2/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 2/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 2/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 2/10:  48%|████▊     | 30/63 [00:30<00:33,  1.03s/it]\n",
      "Epoch 2/10:  49%|████▉     | 31/63 [00:31<00:32,  1.03s/it]\n",
      "Epoch 2/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 2/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.02s/it]\n",
      "Epoch 2/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.02s/it]\n",
      "Epoch 2/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.02s/it]\n",
      "Epoch 2/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.02s/it]\n",
      "Epoch 2/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.02s/it]\n",
      "Epoch 2/10:  60%|██████    | 38/63 [00:39<00:25,  1.02s/it]\n",
      "Epoch 2/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.02s/it]\n",
      "Epoch 2/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.02s/it]\n",
      "Epoch 2/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 2/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 2/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 2/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 2/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 2/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 2/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 2/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.01s/it]\n",
      "Epoch 2/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 2/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 2/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 2/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 2/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 2/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 2/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 2/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.03s/it]\n",
      "Epoch 2/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 2/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 2/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 2/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it]\n",
      "Epoch 2/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 2/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.03s/it]\n",
      "Epoch 2/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 2/10, Training Loss: 1.3885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   2%|▏         | 1/63 [00:01<01:03,  1.02s/it]\n",
      "Epoch 3/10:   3%|▎         | 2/63 [00:02<01:02,  1.03s/it]\n",
      "Epoch 3/10:   5%|▍         | 3/63 [00:03<01:01,  1.03s/it]\n",
      "Epoch 3/10:   6%|▋         | 4/63 [00:04<01:00,  1.03s/it]\n",
      "Epoch 3/10:   8%|▊         | 5/63 [00:05<00:59,  1.02s/it]\n",
      "Epoch 3/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 3/10:  11%|█         | 7/63 [00:07<00:57,  1.02s/it]\n",
      "Epoch 3/10:  13%|█▎        | 8/63 [00:08<00:56,  1.02s/it]\n",
      "Epoch 3/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 3/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 3/10:  17%|█▋        | 11/63 [00:11<00:53,  1.04s/it]\n",
      "Epoch 3/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 3/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 3/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 3/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 3/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 3/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 3/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 3/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 3/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 3/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 3/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 3/10:  37%|███▋      | 23/63 [00:23<00:41,  1.03s/it]\n",
      "Epoch 3/10:  38%|███▊      | 24/63 [00:24<00:39,  1.03s/it]\n",
      "Epoch 3/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 3/10:  41%|████▏     | 26/63 [00:26<00:38,  1.03s/it]\n",
      "Epoch 3/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 3/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 3/10:  46%|████▌     | 29/63 [00:29<00:35,  1.03s/it]\n",
      "Epoch 3/10:  48%|████▊     | 30/63 [00:30<00:33,  1.03s/it]\n",
      "Epoch 3/10:  49%|████▉     | 31/63 [00:31<00:32,  1.02s/it]\n",
      "Epoch 3/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 3/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.02s/it]\n",
      "Epoch 3/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.02s/it]\n",
      "Epoch 3/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.03s/it]\n",
      "Epoch 3/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.03s/it]\n",
      "Epoch 3/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.03s/it]\n",
      "Epoch 3/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 3/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.02s/it]\n",
      "Epoch 3/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.02s/it]\n",
      "Epoch 3/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 3/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.02s/it]\n",
      "Epoch 3/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 3/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 3/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 3/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 3/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 3/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 3/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.03s/it]\n",
      "Epoch 3/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 3/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 3/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 3/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.01s/it]\n",
      "Epoch 3/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.01s/it]\n",
      "Epoch 3/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 3/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 3/10:  90%|█████████ | 57/63 [00:58<00:06,  1.02s/it]\n",
      "Epoch 3/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 3/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 3/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.02s/it]\n",
      "Epoch 3/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 3/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 3/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 3/10, Training Loss: 1.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:   2%|▏         | 1/63 [00:01<01:02,  1.00s/it]\n",
      "Epoch 4/10:   3%|▎         | 2/63 [00:02<01:01,  1.02s/it]\n",
      "Epoch 4/10:   5%|▍         | 3/63 [00:03<01:01,  1.02s/it]\n",
      "Epoch 4/10:   6%|▋         | 4/63 [00:04<01:00,  1.02s/it]\n",
      "Epoch 4/10:   8%|▊         | 5/63 [00:05<00:59,  1.02s/it]\n",
      "Epoch 4/10:  10%|▉         | 6/63 [00:06<00:57,  1.02s/it]\n",
      "Epoch 4/10:  11%|█         | 7/63 [00:07<00:57,  1.02s/it]\n",
      "Epoch 4/10:  13%|█▎        | 8/63 [00:08<00:55,  1.02s/it]\n",
      "Epoch 4/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 4/10:  16%|█▌        | 10/63 [00:10<00:53,  1.02s/it]\n",
      "Epoch 4/10:  17%|█▋        | 11/63 [00:11<00:52,  1.02s/it]\n",
      "Epoch 4/10:  19%|█▉        | 12/63 [00:12<00:52,  1.02s/it]\n",
      "Epoch 4/10:  21%|██        | 13/63 [00:13<00:51,  1.02s/it]\n",
      "Epoch 4/10:  22%|██▏       | 14/63 [00:14<00:50,  1.02s/it]\n",
      "Epoch 4/10:  24%|██▍       | 15/63 [00:15<00:49,  1.02s/it]\n",
      "Epoch 4/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 4/10:  27%|██▋       | 17/63 [00:17<00:46,  1.02s/it]\n",
      "Epoch 4/10:  29%|██▊       | 18/63 [00:18<00:45,  1.02s/it]\n",
      "Epoch 4/10:  30%|███       | 19/63 [00:19<00:44,  1.02s/it]\n",
      "Epoch 4/10:  32%|███▏      | 20/63 [00:20<00:43,  1.02s/it]\n",
      "Epoch 4/10:  33%|███▎      | 21/63 [00:21<00:42,  1.02s/it]\n",
      "Epoch 4/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 4/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 4/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 4/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 4/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 4/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 4/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 4/10:  46%|████▌     | 29/63 [00:29<00:34,  1.02s/it]\n",
      "Epoch 4/10:  48%|████▊     | 30/63 [00:30<00:33,  1.03s/it]\n",
      "Epoch 4/10:  49%|████▉     | 31/63 [00:31<00:32,  1.02s/it]\n",
      "Epoch 4/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 4/10:  52%|█████▏    | 33/63 [00:33<00:30,  1.03s/it]\n",
      "Epoch 4/10:  54%|█████▍    | 34/63 [00:34<00:29,  1.02s/it]\n",
      "Epoch 4/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.03s/it]\n",
      "Epoch 4/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.02s/it]\n",
      "Epoch 4/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.02s/it]\n",
      "Epoch 4/10:  60%|██████    | 38/63 [00:38<00:25,  1.02s/it]\n",
      "Epoch 4/10:  62%|██████▏   | 39/63 [00:39<00:24,  1.02s/it]\n",
      "Epoch 4/10:  63%|██████▎   | 40/63 [00:40<00:23,  1.02s/it]\n",
      "Epoch 4/10:  65%|██████▌   | 41/63 [00:41<00:22,  1.02s/it]\n",
      "Epoch 4/10:  67%|██████▋   | 42/63 [00:42<00:21,  1.02s/it]\n",
      "Epoch 4/10:  68%|██████▊   | 43/63 [00:43<00:20,  1.02s/it]\n",
      "Epoch 4/10:  70%|██████▉   | 44/63 [00:44<00:19,  1.03s/it]\n",
      "Epoch 4/10:  71%|███████▏  | 45/63 [00:45<00:18,  1.02s/it]\n",
      "Epoch 4/10:  73%|███████▎  | 46/63 [00:46<00:17,  1.02s/it]\n",
      "Epoch 4/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.04s/it]\n",
      "Epoch 4/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.05s/it]\n",
      "Epoch 4/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.05s/it]\n",
      "Epoch 4/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.05s/it]\n",
      "Epoch 4/10:  81%|████████  | 51/63 [00:52<00:12,  1.05s/it]\n",
      "Epoch 4/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.04s/it]\n",
      "Epoch 4/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.05s/it]\n",
      "Epoch 4/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.05s/it]\n",
      "Epoch 4/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.05s/it]\n",
      "Epoch 4/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.04s/it]\n",
      "Epoch 4/10:  90%|█████████ | 57/63 [00:58<00:06,  1.05s/it]\n",
      "Epoch 4/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.05s/it]\n",
      "Epoch 4/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.04s/it]\n",
      "Epoch 4/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.04s/it]\n",
      "Epoch 4/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 4/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.03s/it]\n",
      "Epoch 4/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 4/10, Training Loss: 1.3880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:   2%|▏         | 1/63 [00:01<01:03,  1.02s/it]\n",
      "Epoch 5/10:   3%|▎         | 2/63 [00:02<01:02,  1.02s/it]\n",
      "Epoch 5/10:   5%|▍         | 3/63 [00:03<01:01,  1.03s/it]\n",
      "Epoch 5/10:   6%|▋         | 4/63 [00:04<01:00,  1.03s/it]\n",
      "Epoch 5/10:   8%|▊         | 5/63 [00:05<00:59,  1.03s/it]\n",
      "Epoch 5/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 5/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 5/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 5/10:  14%|█▍        | 9/63 [00:09<00:55,  1.04s/it]\n",
      "Epoch 5/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 5/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 5/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 5/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 5/10:  22%|██▏       | 14/63 [00:14<00:50,  1.04s/it]\n",
      "Epoch 5/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 5/10:  25%|██▌       | 16/63 [00:16<00:48,  1.04s/it]\n",
      "Epoch 5/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 5/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 5/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 5/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 5/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 5/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 5/10:  37%|███▋      | 23/63 [00:23<00:41,  1.03s/it]\n",
      "Epoch 5/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 5/10:  40%|███▉      | 25/63 [00:25<00:39,  1.04s/it]\n",
      "Epoch 5/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 5/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 5/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 5/10:  46%|████▌     | 29/63 [00:29<00:35,  1.03s/it]\n",
      "Epoch 5/10:  48%|████▊     | 30/63 [00:30<00:34,  1.03s/it]\n",
      "Epoch 5/10:  49%|████▉     | 31/63 [00:32<00:33,  1.03s/it]\n",
      "Epoch 5/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 5/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.03s/it]\n",
      "Epoch 5/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 5/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 5/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 5/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 5/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 5/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 5/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 5/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.04s/it]\n",
      "Epoch 5/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 5/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 5/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 5/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 5/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 5/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 5/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 5/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.03s/it]\n",
      "Epoch 5/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 5/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 5/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.04s/it]\n",
      "Epoch 5/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 5/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 5/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 5/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.03s/it]\n",
      "Epoch 5/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 5/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 5/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 5/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it]\n",
      "Epoch 5/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.02s/it]\n",
      "Epoch 5/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 5/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 5/10, Training Loss: 1.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 6/10:   3%|▎         | 2/63 [00:02<01:03,  1.05s/it]\n",
      "Epoch 6/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 6/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 6/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 6/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 6/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 6/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 6/10:  14%|█▍        | 9/63 [00:09<00:55,  1.03s/it]\n",
      "Epoch 6/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 6/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 6/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 6/10:  21%|██        | 13/63 [00:13<00:51,  1.02s/it]\n",
      "Epoch 6/10:  22%|██▏       | 14/63 [00:14<00:50,  1.02s/it]\n",
      "Epoch 6/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 6/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 6/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 6/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 6/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 6/10:  32%|███▏      | 20/63 [00:20<00:44,  1.04s/it]\n",
      "Epoch 6/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 6/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 6/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 6/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 6/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 6/10:  41%|████▏     | 26/63 [00:26<00:38,  1.03s/it]\n",
      "Epoch 6/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 6/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 6/10:  46%|████▌     | 29/63 [00:29<00:35,  1.03s/it]\n",
      "Epoch 6/10:  48%|████▊     | 30/63 [00:30<00:34,  1.04s/it]\n",
      "Epoch 6/10:  49%|████▉     | 31/63 [00:32<00:33,  1.04s/it]\n",
      "Epoch 6/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 6/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 6/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 6/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 6/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 6/10:  59%|█████▊    | 37/63 [00:38<00:27,  1.04s/it]\n",
      "Epoch 6/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 6/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 6/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 6/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 6/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 6/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 6/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 6/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 6/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 6/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 6/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.03s/it]\n",
      "Epoch 6/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 6/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 6/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 6/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 6/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 6/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 6/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 6/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.03s/it]\n",
      "Epoch 6/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 6/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 6/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 6/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it]\n",
      "Epoch 6/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 6/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.03s/it]\n",
      "Epoch 6/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 6/10, Training Loss: 1.3883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 7/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 7/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 7/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 7/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 7/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 7/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 7/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 7/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 7/10:  14%|█▍        | 9/63 [00:09<00:55,  1.03s/it]\n",
      "Epoch 7/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 7/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 7/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 7/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 7/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 7/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 7/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 7/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 7/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 7/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 7/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 7/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 7/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 7/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 7/10:  38%|███▊      | 24/63 [00:24<00:40,  1.03s/it]\n",
      "Epoch 7/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 7/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 7/10:  43%|████▎     | 27/63 [00:27<00:37,  1.04s/it]\n",
      "Epoch 7/10:  44%|████▍     | 28/63 [00:28<00:36,  1.04s/it]\n",
      "Epoch 7/10:  46%|████▌     | 29/63 [00:29<00:35,  1.03s/it]\n",
      "Epoch 7/10:  48%|████▊     | 30/63 [00:31<00:34,  1.03s/it]\n",
      "Epoch 7/10:  49%|████▉     | 31/63 [00:32<00:33,  1.03s/it]\n",
      "Epoch 7/10:  51%|█████     | 32/63 [00:33<00:32,  1.03s/it]\n",
      "Epoch 7/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.03s/it]\n",
      "Epoch 7/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 7/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 7/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 7/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 7/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 7/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 7/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.04s/it]\n",
      "Epoch 7/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 7/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 7/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.04s/it]\n",
      "Epoch 7/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.04s/it]\n",
      "Epoch 7/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 7/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 7/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.04s/it]\n",
      "Epoch 7/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.04s/it]\n",
      "Epoch 7/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 7/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.04s/it]\n",
      "Epoch 7/10:  81%|████████  | 51/63 [00:52<00:12,  1.04s/it]\n",
      "Epoch 7/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 7/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.04s/it]\n",
      "Epoch 7/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.04s/it]\n",
      "Epoch 7/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.04s/it]\n",
      "Epoch 7/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.04s/it]\n",
      "Epoch 7/10:  90%|█████████ | 57/63 [00:58<00:06,  1.04s/it]\n",
      "Epoch 7/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.04s/it]\n",
      "Epoch 7/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 7/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.03s/it]\n",
      "Epoch 7/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 7/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.04s/it]\n",
      "Epoch 7/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 7/10, Training Loss: 1.3870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:   2%|▏         | 1/63 [00:01<01:03,  1.02s/it]\n",
      "Epoch 8/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 8/10:   5%|▍         | 3/63 [00:03<01:01,  1.02s/it]\n",
      "Epoch 8/10:   6%|▋         | 4/63 [00:04<01:00,  1.03s/it]\n",
      "Epoch 8/10:   8%|▊         | 5/63 [00:05<01:00,  1.03s/it]\n",
      "Epoch 8/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 8/10:  11%|█         | 7/63 [00:07<00:58,  1.04s/it]\n",
      "Epoch 8/10:  13%|█▎        | 8/63 [00:08<00:57,  1.04s/it]\n",
      "Epoch 8/10:  14%|█▍        | 9/63 [00:09<00:55,  1.04s/it]\n",
      "Epoch 8/10:  16%|█▌        | 10/63 [00:10<00:54,  1.04s/it]\n",
      "Epoch 8/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 8/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 8/10:  21%|██        | 13/63 [00:13<00:51,  1.04s/it]\n",
      "Epoch 8/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 8/10:  24%|██▍       | 15/63 [00:15<00:49,  1.02s/it]\n",
      "Epoch 8/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 8/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 8/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 8/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 8/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 8/10:  33%|███▎      | 21/63 [00:21<00:43,  1.03s/it]\n",
      "Epoch 8/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 8/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 8/10:  38%|███▊      | 24/63 [00:24<00:40,  1.03s/it]\n",
      "Epoch 8/10:  40%|███▉      | 25/63 [00:25<00:39,  1.03s/it]\n",
      "Epoch 8/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 8/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 8/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 8/10:  46%|████▌     | 29/63 [00:29<00:35,  1.04s/it]\n",
      "Epoch 8/10:  48%|████▊     | 30/63 [00:30<00:34,  1.03s/it]\n",
      "Epoch 8/10:  49%|████▉     | 31/63 [00:31<00:33,  1.04s/it]\n",
      "Epoch 8/10:  51%|█████     | 32/63 [00:33<00:32,  1.04s/it]\n",
      "Epoch 8/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.04s/it]\n",
      "Epoch 8/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.04s/it]\n",
      "Epoch 8/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.04s/it]\n",
      "Epoch 8/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 8/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.04s/it]\n",
      "Epoch 8/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 8/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 8/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 8/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 8/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 8/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 8/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 8/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 8/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.04s/it]\n",
      "Epoch 8/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.04s/it]\n",
      "Epoch 8/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.04s/it]\n",
      "Epoch 8/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 8/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 8/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 8/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 8/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 8/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 8/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 8/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.04s/it]\n",
      "Epoch 8/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 8/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 8/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 8/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it]\n",
      "Epoch 8/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 8/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 8/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 8/10, Training Loss: 1.3873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 9/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 9/10:   3%|▎         | 2/63 [00:02<01:03,  1.04s/it]\n",
      "Epoch 9/10:   5%|▍         | 3/63 [00:03<01:01,  1.03s/it]\n",
      "Epoch 9/10:   6%|▋         | 4/63 [00:04<01:00,  1.03s/it]\n",
      "Epoch 9/10:   8%|▊         | 5/63 [00:05<00:59,  1.02s/it]\n",
      "Epoch 9/10:  10%|▉         | 6/63 [00:06<00:58,  1.02s/it]\n",
      "Epoch 9/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 9/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 9/10:  14%|█▍        | 9/63 [00:09<00:55,  1.03s/it]\n",
      "Epoch 9/10:  16%|█▌        | 10/63 [00:10<00:54,  1.02s/it]\n",
      "Epoch 9/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 9/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 9/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 9/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 9/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 9/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 9/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 9/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 9/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 9/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 9/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 9/10:  35%|███▍      | 22/63 [00:22<00:42,  1.04s/it]\n",
      "Epoch 9/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 9/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 9/10:  40%|███▉      | 25/63 [00:25<00:39,  1.05s/it]\n",
      "Epoch 9/10:  41%|████▏     | 26/63 [00:26<00:38,  1.04s/it]\n",
      "Epoch 9/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 9/10:  44%|████▍     | 28/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 9/10:  46%|████▌     | 29/63 [00:29<00:34,  1.03s/it]\n",
      "Epoch 9/10:  48%|████▊     | 30/63 [00:30<00:33,  1.02s/it]\n",
      "Epoch 9/10:  49%|████▉     | 31/63 [00:31<00:32,  1.03s/it]\n",
      "Epoch 9/10:  51%|█████     | 32/63 [00:32<00:31,  1.02s/it]\n",
      "Epoch 9/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.02s/it]\n",
      "Epoch 9/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.02s/it]\n",
      "Epoch 9/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.02s/it]\n",
      "Epoch 9/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.02s/it]\n",
      "Epoch 9/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.02s/it]\n",
      "Epoch 9/10:  60%|██████    | 38/63 [00:39<00:25,  1.02s/it]\n",
      "Epoch 9/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.01s/it]\n",
      "Epoch 9/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.02s/it]\n",
      "Epoch 9/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.02s/it]\n",
      "Epoch 9/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.03s/it]\n",
      "Epoch 9/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.02s/it]\n",
      "Epoch 9/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.02s/it]\n",
      "Epoch 9/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.02s/it]\n",
      "Epoch 9/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.02s/it]\n",
      "Epoch 9/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.02s/it]\n",
      "Epoch 9/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 9/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 9/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 9/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 9/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 9/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 9/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.02s/it]\n",
      "Epoch 9/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.02s/it]\n",
      "Epoch 9/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 9/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 9/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 9/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.02s/it]\n",
      "Epoch 9/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.01s/it]\n",
      "Epoch 9/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.02s/it]\n",
      "Epoch 9/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.02s/it]\n",
      "Epoch 9/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n",
      "Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 9/10, Training Loss: 1.3863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:   2%|▏         | 1/63 [00:01<01:02,  1.01s/it]\n",
      "Epoch 10/10:   3%|▎         | 2/63 [00:02<01:02,  1.02s/it]\n",
      "Epoch 10/10:   5%|▍         | 3/63 [00:03<01:00,  1.01s/it]\n",
      "Epoch 10/10:   6%|▋         | 4/63 [00:04<00:59,  1.01s/it]\n",
      "Epoch 10/10:   8%|▊         | 5/63 [00:05<00:58,  1.01s/it]\n",
      "Epoch 10/10:  10%|▉         | 6/63 [00:06<00:57,  1.01s/it]\n",
      "Epoch 10/10:  11%|█         | 7/63 [00:07<00:56,  1.01s/it]\n",
      "Epoch 10/10:  13%|█▎        | 8/63 [00:08<00:55,  1.02s/it]\n",
      "Epoch 10/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 10/10:  16%|█▌        | 10/63 [00:10<00:53,  1.01s/it]\n",
      "Epoch 10/10:  17%|█▋        | 11/63 [00:11<00:52,  1.01s/it]\n",
      "Epoch 10/10:  19%|█▉        | 12/63 [00:12<00:52,  1.02s/it]\n",
      "Epoch 10/10:  21%|██        | 13/63 [00:13<00:50,  1.02s/it]\n",
      "Epoch 10/10:  22%|██▏       | 14/63 [00:14<00:49,  1.02s/it]\n",
      "Epoch 10/10:  24%|██▍       | 15/63 [00:15<00:48,  1.01s/it]\n",
      "Epoch 10/10:  25%|██▌       | 16/63 [00:16<00:47,  1.02s/it]\n",
      "Epoch 10/10:  27%|██▋       | 17/63 [00:17<00:46,  1.02s/it]\n",
      "Epoch 10/10:  29%|██▊       | 18/63 [00:18<00:45,  1.01s/it]\n",
      "Epoch 10/10:  30%|███       | 19/63 [00:19<00:44,  1.02s/it]\n",
      "Epoch 10/10:  32%|███▏      | 20/63 [00:20<00:43,  1.02s/it]\n",
      "Epoch 10/10:  33%|███▎      | 21/63 [00:21<00:42,  1.02s/it]\n",
      "Epoch 10/10:  35%|███▍      | 22/63 [00:22<00:41,  1.02s/it]\n",
      "Epoch 10/10:  37%|███▋      | 23/63 [00:23<00:40,  1.02s/it]\n",
      "Epoch 10/10:  38%|███▊      | 24/63 [00:24<00:39,  1.02s/it]\n",
      "Epoch 10/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 10/10:  41%|████▏     | 26/63 [00:26<00:37,  1.02s/it]\n",
      "Epoch 10/10:  43%|████▎     | 27/63 [00:27<00:36,  1.02s/it]\n",
      "Epoch 10/10:  44%|████▍     | 28/63 [00:28<00:35,  1.02s/it]\n",
      "Epoch 10/10:  46%|████▌     | 29/63 [00:29<00:34,  1.01s/it]\n",
      "Epoch 10/10:  48%|████▊     | 30/63 [00:30<00:34,  1.05s/it]\n",
      "Epoch 10/10:  49%|████▉     | 31/63 [00:31<00:33,  1.06s/it]\n",
      "Epoch 10/10:  51%|█████     | 32/63 [00:32<00:32,  1.05s/it]\n",
      "Epoch 10/10:  52%|█████▏    | 33/63 [00:33<00:31,  1.04s/it]\n",
      "Epoch 10/10:  54%|█████▍    | 34/63 [00:34<00:30,  1.04s/it]\n",
      "Epoch 10/10:  56%|█████▌    | 35/63 [00:35<00:28,  1.03s/it]\n",
      "Epoch 10/10:  57%|█████▋    | 36/63 [00:36<00:27,  1.03s/it]\n",
      "Epoch 10/10:  59%|█████▊    | 37/63 [00:37<00:26,  1.03s/it]\n",
      "Epoch 10/10:  60%|██████    | 38/63 [00:38<00:25,  1.03s/it]\n",
      "Epoch 10/10:  62%|██████▏   | 39/63 [00:39<00:24,  1.03s/it]\n",
      "Epoch 10/10:  63%|██████▎   | 40/63 [00:40<00:23,  1.03s/it]\n",
      "Epoch 10/10:  65%|██████▌   | 41/63 [00:41<00:22,  1.03s/it]\n",
      "Epoch 10/10:  67%|██████▋   | 42/63 [00:42<00:21,  1.03s/it]\n",
      "Epoch 10/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.03s/it]\n",
      "Epoch 10/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.03s/it]\n",
      "Epoch 10/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.03s/it]\n",
      "Epoch 10/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.03s/it]\n",
      "Epoch 10/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 10/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.02s/it]\n",
      "Epoch 10/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.02s/it]\n",
      "Epoch 10/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.02s/it]\n",
      "Epoch 10/10:  81%|████████  | 51/63 [00:52<00:12,  1.02s/it]\n",
      "Epoch 10/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.02s/it]\n",
      "Epoch 10/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.02s/it]\n",
      "Epoch 10/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 10/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 10/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.04s/it]\n",
      "Epoch 10/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 10/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 10/10:  94%|█████████▎| 59/63 [01:00<00:04,  1.03s/it]\n",
      "Epoch 10/10:  95%|█████████▌| 60/63 [01:01<00:03,  1.03s/it]\n",
      "Epoch 10/10:  97%|█████████▋| 61/63 [01:02<00:02,  1.03s/it]\n",
      "Epoch 10/10:  98%|█████████▊| 62/63 [01:03<00:01,  1.03s/it]\n",
      "Epoch 10/10: 100%|██████████| 63/63 [01:04<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Epoch 10/10, Training Loss: 1.3843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 11:06:53,831\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'optimiser': ('__ref_ph', '5e5b722a')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=7692)\u001b[0m Validation Loss: 1.3833\n",
      "\u001b[36m(train_model pid=22184)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 1/10:   2%|▏         | 1/63 [00:01<01:22,  1.33s/it]\n",
      "Epoch 1/10:   3%|▎         | 2/63 [00:02<01:11,  1.17s/it]\n",
      "Epoch 1/10:   5%|▍         | 3/63 [00:03<01:05,  1.10s/it]\n",
      "Epoch 1/10:   6%|▋         | 4/63 [00:04<01:03,  1.07s/it]\n",
      "Epoch 1/10:   8%|▊         | 5/63 [00:05<01:01,  1.06s/it]\n",
      "Epoch 1/10:  10%|▉         | 6/63 [00:06<00:59,  1.05s/it]\n",
      "Epoch 1/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 1/10:  13%|█▎        | 8/63 [00:08<00:56,  1.03s/it]\n",
      "Epoch 1/10:  14%|█▍        | 9/63 [00:09<00:55,  1.03s/it]\n",
      "Epoch 1/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 1/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 1/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 1/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 1/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 1/10:  24%|██▍       | 15/63 [00:15<00:49,  1.02s/it]\n",
      "Epoch 1/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 1/10:  27%|██▋       | 17/63 [00:17<00:47,  1.02s/it]\n",
      "Epoch 1/10:  29%|██▊       | 18/63 [00:18<00:46,  1.03s/it]\n",
      "Epoch 1/10:  30%|███       | 19/63 [00:19<00:45,  1.03s/it]\n",
      "Epoch 1/10:  32%|███▏      | 20/63 [00:20<00:44,  1.02s/it]\n",
      "Epoch 1/10:  33%|███▎      | 21/63 [00:21<00:43,  1.02s/it]\n",
      "Epoch 1/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 1/10:  37%|███▋      | 23/63 [00:23<00:41,  1.04s/it]\n",
      "Epoch 1/10:  38%|███▊      | 24/63 [00:24<00:40,  1.04s/it]\n",
      "Epoch 1/10:  40%|███▉      | 25/63 [00:26<00:39,  1.03s/it]\n",
      "Epoch 1/10:  41%|████▏     | 26/63 [00:27<00:38,  1.03s/it]\n",
      "Epoch 1/10:  43%|████▎     | 27/63 [00:28<00:36,  1.03s/it]\n",
      "Epoch 1/10:  44%|████▍     | 28/63 [00:29<00:35,  1.03s/it]\n",
      "Epoch 1/10:  46%|████▌     | 29/63 [00:30<00:34,  1.03s/it]\n",
      "Epoch 1/10:  48%|████▊     | 30/63 [00:31<00:33,  1.03s/it]\n",
      "Epoch 1/10:  49%|████▉     | 31/63 [00:32<00:32,  1.03s/it]\n",
      "Epoch 1/10:  51%|█████     | 32/63 [00:33<00:31,  1.03s/it]\n",
      "Epoch 1/10:  52%|█████▏    | 33/63 [00:34<00:30,  1.03s/it]\n",
      "Epoch 1/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.02s/it]\n",
      "Epoch 1/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 1/10:  57%|█████▋    | 36/63 [00:37<00:27,  1.03s/it]\n",
      "Epoch 1/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 1/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 1/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.03s/it]\n",
      "Epoch 1/10:  63%|██████▎   | 40/63 [00:41<00:23,  1.03s/it]\n",
      "Epoch 1/10:  65%|██████▌   | 41/63 [00:42<00:22,  1.03s/it]\n",
      "Epoch 1/10:  67%|██████▋   | 42/63 [00:43<00:21,  1.04s/it]\n",
      "Epoch 1/10:  68%|██████▊   | 43/63 [00:44<00:20,  1.04s/it]\n",
      "Epoch 1/10:  70%|██████▉   | 44/63 [00:45<00:19,  1.04s/it]\n",
      "Epoch 1/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.04s/it]\n",
      "Epoch 1/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.04s/it]\n",
      "Epoch 1/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.03s/it]\n",
      "Epoch 1/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.03s/it]\n",
      "Epoch 1/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 1/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.03s/it]\n",
      "Epoch 1/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 1/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.03s/it]\n",
      "Epoch 1/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 1/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 1/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.03s/it]\n",
      "Epoch 1/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.02s/it]\n",
      "Epoch 1/10:  90%|█████████ | 57/63 [00:58<00:06,  1.03s/it]\n",
      "Epoch 1/10:  92%|█████████▏| 58/63 [00:59<00:05,  1.03s/it]\n",
      "Epoch 1/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 1/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.03s/it]\n",
      "Epoch 1/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 1/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.03s/it]\n",
      "Epoch 1/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=22184)\u001b[0m Epoch 1/10, Training Loss: 1.3873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:   2%|▏         | 1/63 [00:01<01:03,  1.03s/it]\n",
      "Epoch 2/10:   3%|▎         | 2/63 [00:02<01:03,  1.03s/it]\n",
      "Epoch 2/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 2/10:   6%|▋         | 4/63 [00:04<01:01,  1.04s/it]\n",
      "Epoch 2/10:   8%|▊         | 5/63 [00:05<00:59,  1.03s/it]\n",
      "Epoch 2/10:  10%|▉         | 6/63 [00:06<00:58,  1.03s/it]\n",
      "Epoch 2/10:  11%|█         | 7/63 [00:07<00:57,  1.03s/it]\n",
      "Epoch 2/10:  13%|█▎        | 8/63 [00:08<00:56,  1.02s/it]\n",
      "Epoch 2/10:  14%|█▍        | 9/63 [00:09<00:55,  1.02s/it]\n",
      "Epoch 2/10:  16%|█▌        | 10/63 [00:10<00:54,  1.03s/it]\n",
      "Epoch 2/10:  17%|█▋        | 11/63 [00:11<00:53,  1.03s/it]\n",
      "Epoch 2/10:  19%|█▉        | 12/63 [00:12<00:52,  1.03s/it]\n",
      "Epoch 2/10:  21%|██        | 13/63 [00:13<00:51,  1.03s/it]\n",
      "Epoch 2/10:  22%|██▏       | 14/63 [00:14<00:50,  1.03s/it]\n",
      "Epoch 2/10:  24%|██▍       | 15/63 [00:15<00:49,  1.03s/it]\n",
      "Epoch 2/10:  25%|██▌       | 16/63 [00:16<00:48,  1.03s/it]\n",
      "Epoch 2/10:  27%|██▋       | 17/63 [00:17<00:47,  1.03s/it]\n",
      "Epoch 2/10:  29%|██▊       | 18/63 [00:18<00:46,  1.04s/it]\n",
      "Epoch 2/10:  30%|███       | 19/63 [00:19<00:45,  1.04s/it]\n",
      "Epoch 2/10:  32%|███▏      | 20/63 [00:20<00:44,  1.03s/it]\n",
      "Epoch 2/10:  33%|███▎      | 21/63 [00:21<00:43,  1.04s/it]\n",
      "Epoch 2/10:  35%|███▍      | 22/63 [00:22<00:42,  1.03s/it]\n",
      "Epoch 2/10:  37%|███▋      | 23/63 [00:23<00:41,  1.03s/it]\n",
      "Epoch 2/10:  38%|███▊      | 24/63 [00:24<00:40,  1.03s/it]\n",
      "Epoch 2/10:  40%|███▉      | 25/63 [00:25<00:38,  1.02s/it]\n",
      "Epoch 2/10:  41%|████▏     | 26/63 [00:26<00:37,  1.03s/it]\n",
      "Epoch 2/10:  43%|████▎     | 27/63 [00:27<00:37,  1.03s/it]\n",
      "Epoch 2/10:  44%|████▍     | 28/63 [00:28<00:35,  1.03s/it]\n",
      "Epoch 2/10:  46%|████▌     | 29/63 [00:29<00:34,  1.03s/it]\n",
      "Epoch 2/10:  48%|████▊     | 30/63 [00:30<00:33,  1.03s/it]\n",
      "Epoch 2/10:  49%|████▉     | 31/63 [00:31<00:33,  1.04s/it]\n",
      "Epoch 2/10:  51%|█████     | 32/63 [00:32<00:32,  1.04s/it]\n",
      "Epoch 2/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.03s/it]\n",
      "Epoch 2/10:  54%|█████▍    | 34/63 [00:35<00:29,  1.03s/it]\n",
      "Epoch 2/10:  56%|█████▌    | 35/63 [00:36<00:28,  1.03s/it]\n",
      "Epoch 2/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.04s/it]\n",
      "Epoch 2/10:  59%|█████▊    | 37/63 [00:38<00:26,  1.03s/it]\n",
      "Epoch 2/10:  60%|██████    | 38/63 [00:39<00:25,  1.03s/it]\n",
      "Epoch 2/10:  62%|██████▏   | 39/63 [00:40<00:24,  1.04s/it]\n",
      "Epoch 2/10:  63%|██████▎   | 40/63 [00:41<00:24,  1.04s/it]\n",
      "Epoch 2/10:  65%|██████▌   | 41/63 [00:42<00:23,  1.05s/it]\n",
      "Epoch 2/10:  67%|██████▋   | 42/63 [00:43<00:22,  1.08s/it]\n",
      "Epoch 2/10:  68%|██████▊   | 43/63 [00:44<00:21,  1.07s/it]\n",
      "Epoch 2/10:  70%|██████▉   | 44/63 [00:45<00:20,  1.06s/it]\n",
      "Epoch 2/10:  71%|███████▏  | 45/63 [00:46<00:18,  1.05s/it]\n",
      "Epoch 2/10:  73%|███████▎  | 46/63 [00:47<00:17,  1.04s/it]\n",
      "Epoch 2/10:  75%|███████▍  | 47/63 [00:48<00:16,  1.04s/it]\n",
      "Epoch 2/10:  76%|███████▌  | 48/63 [00:49<00:15,  1.04s/it]\n",
      "Epoch 2/10:  78%|███████▊  | 49/63 [00:50<00:14,  1.04s/it]\n",
      "Epoch 2/10:  79%|███████▉  | 50/63 [00:51<00:13,  1.04s/it]\n",
      "Epoch 2/10:  81%|████████  | 51/63 [00:52<00:12,  1.03s/it]\n",
      "Epoch 2/10:  83%|████████▎ | 52/63 [00:53<00:11,  1.04s/it]\n",
      "Epoch 2/10:  84%|████████▍ | 53/63 [00:54<00:10,  1.03s/it]\n",
      "Epoch 2/10:  86%|████████▌ | 54/63 [00:55<00:09,  1.03s/it]\n",
      "Epoch 2/10:  87%|████████▋ | 55/63 [00:56<00:08,  1.04s/it]\n",
      "Epoch 2/10:  89%|████████▉ | 56/63 [00:57<00:07,  1.03s/it]\n",
      "Epoch 2/10:  90%|█████████ | 57/63 [00:59<00:06,  1.03s/it]\n",
      "Epoch 2/10:  92%|█████████▏| 58/63 [01:00<00:05,  1.03s/it]\n",
      "Epoch 2/10:  94%|█████████▎| 59/63 [01:01<00:04,  1.03s/it]\n",
      "Epoch 2/10:  95%|█████████▌| 60/63 [01:02<00:03,  1.02s/it]\n",
      "Epoch 2/10:  97%|█████████▋| 61/63 [01:03<00:02,  1.03s/it]\n",
      "Epoch 2/10:  98%|█████████▊| 62/63 [01:04<00:01,  1.04s/it]\n",
      "Epoch 2/10: 100%|██████████| 63/63 [01:04<00:00,  1.03s/it]\n",
      "Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=22184)\u001b[0m Epoch 2/10, Training Loss: 1.3874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:   2%|▏         | 1/63 [00:01<01:05,  1.05s/it]\n",
      "Epoch 3/10:   3%|▎         | 2/63 [00:02<01:05,  1.07s/it]\n",
      "Epoch 3/10:   5%|▍         | 3/63 [00:03<01:02,  1.04s/it]\n",
      "Epoch 3/10:   6%|▋         | 4/63 [00:04<01:00,  1.03s/it]\n",
      "Epoch 3/10:   8%|▊         | 5/63 [00:05<01:00,  1.04s/it]\n",
      "Epoch 3/10:  10%|▉         | 6/63 [00:06<00:59,  1.04s/it]\n",
      "Epoch 3/10:  11%|█         | 7/63 [00:07<00:58,  1.05s/it]\n",
      "Epoch 3/10:  13%|█▎        | 8/63 [00:08<00:59,  1.07s/it]\n",
      "Epoch 3/10:  14%|█▍        | 9/63 [00:09<00:57,  1.07s/it]\n",
      "Epoch 3/10:  16%|█▌        | 10/63 [00:10<00:56,  1.07s/it]\n",
      "Epoch 3/10:  17%|█▋        | 11/63 [00:11<00:55,  1.07s/it]\n",
      "Epoch 3/10:  19%|█▉        | 12/63 [00:12<00:54,  1.06s/it]\n",
      "Epoch 3/10:  21%|██        | 13/63 [00:13<00:52,  1.06s/it]\n",
      "Epoch 3/10:  22%|██▏       | 14/63 [00:14<00:51,  1.06s/it]\n",
      "Epoch 3/10:  24%|██▍       | 15/63 [00:15<00:50,  1.06s/it]\n",
      "Epoch 3/10:  25%|██▌       | 16/63 [00:16<00:49,  1.06s/it]\n",
      "Epoch 3/10:  27%|██▋       | 17/63 [00:17<00:48,  1.05s/it]\n",
      "Epoch 3/10:  29%|██▊       | 18/63 [00:19<00:47,  1.05s/it]\n",
      "Epoch 3/10:  30%|███       | 19/63 [00:20<00:46,  1.05s/it]\n",
      "Epoch 3/10:  32%|███▏      | 20/63 [00:21<00:45,  1.06s/it]\n",
      "Epoch 3/10:  33%|███▎      | 21/63 [00:22<00:44,  1.06s/it]\n",
      "Epoch 3/10:  35%|███▍      | 22/63 [00:23<00:43,  1.06s/it]\n",
      "Epoch 3/10:  37%|███▋      | 23/63 [00:24<00:41,  1.05s/it]\n",
      "Epoch 3/10:  38%|███▊      | 24/63 [00:25<00:40,  1.04s/it]\n",
      "Epoch 3/10:  40%|███▉      | 25/63 [00:26<00:39,  1.04s/it]\n",
      "Epoch 3/10:  41%|████▏     | 26/63 [00:27<00:38,  1.04s/it]\n",
      "Epoch 3/10:  43%|████▎     | 27/63 [00:28<00:37,  1.04s/it]\n",
      "Epoch 3/10:  44%|████▍     | 28/63 [00:29<00:36,  1.05s/it]\n",
      "Epoch 3/10:  46%|████▌     | 29/63 [00:30<00:35,  1.05s/it]\n",
      "Epoch 3/10:  48%|████▊     | 30/63 [00:31<00:34,  1.05s/it]\n",
      "Epoch 3/10:  49%|████▉     | 31/63 [00:32<00:33,  1.05s/it]\n",
      "2025-04-20 11:09:40,336\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-04-20 11:09:40,345\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'C:/ray_results/ENSE/ENSE' in 0.0090s.\n",
      "Epoch 3/10:  51%|█████     | 32/63 [00:33<00:32,  1.05s/it]\n",
      "Epoch 3/10:  52%|█████▏    | 33/63 [00:34<00:31,  1.05s/it]\n",
      "Epoch 3/10:  54%|█████▍    | 34/63 [00:35<00:30,  1.05s/it]\n",
      "Epoch 3/10:  56%|█████▌    | 35/63 [00:36<00:29,  1.06s/it]\n",
      "Epoch 3/10:  57%|█████▋    | 36/63 [00:37<00:28,  1.05s/it]\n",
      "Epoch 3/10:  59%|█████▊    | 37/63 [00:39<00:27,  1.06s/it]\n",
      "Epoch 3/10:  60%|██████    | 38/63 [00:40<00:26,  1.07s/it]\n",
      "Epoch 3/10:  62%|██████▏   | 39/63 [00:41<00:25,  1.07s/it]\n",
      "Epoch 3/10:  63%|██████▎   | 40/63 [00:42<00:24,  1.06s/it]\n",
      "2025-04-20 11:09:50,415\tERROR tune.py:1037 -- Trials did not complete: [t_c8c52f13]\n",
      "2025-04-20 11:09:50,415\tINFO tune.py:1041 -- Total run time: 2148.38 seconds (2138.23 seconds for the tuning loop).\n",
      "2025-04-20 11:09:50,415\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"C:/ray_results/ENSE/ENSE\", trainable=...)\n",
      "2025-04-20 11:09:50,436\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- t_d9a12698: FileNotFoundError('Could not fetch metrics for t_d9a12698: both result.json and progress.csv were not found at C:/ray_results/ENSE/ENSE/trial_d9a12698')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'lr': 3.818145165896868e-05, 'weight_decay': 1.1585650991012693e-05, 'dropout': 0.393636499344142, 'hidden_sizes': None, 'activation': 'sigmoid', 'unfreeze_blocks': [6, 7], 'num_epochs': 10, 'optimiser': <function <lambda> at 0x00000205801FDBD0>, 'criterion': <function <lambda> at 0x00000205801FD870>}\n",
      "Full best config: {'lr': 3.818145165896868e-05, 'weight_decay': 1.1585650991012693e-05, 'dropout': 0.393636499344142, 'hidden_sizes': None, 'activation': 'sigmoid', 'unfreeze_blocks': [6, 7], 'num_epochs': 10, 'optimiser': <function <lambda> at 0x00000205801FDBD0>, 'criterion': <function <lambda> at 0x00000205801FD870>}\n"
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    max_t=100,           # max training iterations per trial\n",
    "    grace_period=2,     # min iterations before stopping\n",
    "    reduction_factor=2,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"loss\", mode=\"min\", seed=42)\n",
    "\n",
    "# define search space in a config dictionary, i.e what are the values you want to try, this is just example of format\n",
    "\n",
    "# for efficientnetb5 gpu\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 5e-5),    \n",
    "    \"weight_decay\": tune.loguniform(1e-5, 2e-5),\n",
    "    \"dropout\": tune.uniform(0.39, 0.41),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\", \"leakyrelu\"]), #Removed gelu\n",
    "    \"unfreeze_blocks\": tune.choice([[6, 7]]),\n",
    "    \"num_epochs\": 10,\n",
    "    \"optimiser\": tune.choice([\n",
    "        lambda params, lr, wd: optim.Adam(params, lr=lr, weight_decay=wd),\n",
    "        lambda params, lr, wd: optim.SGD(params, lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    ]),\n",
    "    \"criterion\": lambda: nn.CrossEntropyLoss()\n",
    "}\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    # this is meant to create short and unique directory name for each trial bc else too long for windows\n",
    "    # include a unique identifier (like trial.trial_id).\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "def trial_name_creator(trial):\n",
    "    return f\"t_{trial.trial_id}\"\n",
    "\n",
    "# tuner object\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {\"cpu\": 12, \"gpu\": 1}), #NOTE:specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=optuna_search,\n",
    "        num_samples=15,  # number of trials to run\n",
    "        trial_dirname_creator=trial_dirname_creator,\n",
    "        trial_name_creator=trial_name_creator, \n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        name=model_name,\n",
    "        storage_path=F\"C:/ray_results/{model_name}\", #NOTE: not required for macos\n",
    "        log_to_file=True,\n",
    "        verbose=1),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "print(\"Best config:\", results.get_best_result(metric=\"loss\", mode=\"min\", filter_nan_and_inf=False).config)\n",
    "\n",
    "# Extracting the best config from the training from above\n",
    "best_config = results.get_best_result(metric=\"loss\", mode=\"min\").config\n",
    "print(\"Full best config:\", best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1641aeac",
   "metadata": {},
   "source": [
    "## Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4762ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Good configs, saved after various experimentations\n",
    "# This cell can be skipped if hyper parameter tuning was run\n",
    "best_config = {\n",
    "    'lr': 2.5883109107619077e-05,\n",
    "    'weight_decay': 1.5994372155012393e-05,\n",
    "    'dropout': 0.4016587828927856,\n",
    "    'hidden_sizes': None,\n",
    "    'activation': 'leakyrelu',\n",
    "    'unfreeze_blocks': [6, 7],\n",
    "    'num_epochs': 10,\n",
    "    'optimiser': lambda params, lr, wd: optim.Adam(params, lr=lr, weight_decay=wd),\n",
    "    'criterion': lambda: nn.CrossEntropyLoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7018de",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = BaseEfficientNetB5(\n",
    "    num_classes=4,\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    hidden_sizes=best_config[\"hidden_sizes\"],\n",
    "    activation=best_config[\"activation\"],\n",
    "    unfreeze_blocks=best_config[\"unfreeze_blocks\"]\n",
    ")\n",
    "\n",
    "optimiser = best_config[\"optimiser\"](final_model.parameters(), lr=best_config[\"lr\"], wd=best_config[\"weight_decay\"])\n",
    "criterion = best_config[\"criterion\"]()\n",
    "\n",
    "final_model = final_model.to(DEVICE)\n",
    "logging.info(\"Model instantiated on device: %s\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a05c7292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 126/126 [02:08<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.1654, Val Loss: 0.3928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 126/126 [02:08<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Train Loss: 0.1270, Val Loss: 0.3913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 126/126 [02:08<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Train Loss: 0.1013, Val Loss: 0.3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 126/126 [02:13<00:00,  1.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m running_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m FULL_VAL_LOADER:\n\u001b[0;32m     32\u001b[0m         inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), targets\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     33\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m final_model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torchvision\\datasets\\folder.py:231\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yongl\\anaconda3\\envs\\ADLTEAM\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1275\u001b[0m fn_idx, brightness_factor, contrast_factor, saturation_factor, hue_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbrightness, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrast, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaturation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhue\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn_id \u001b[38;5;129;01min\u001b[39;00m fn_idx:\n\u001b[1;32m-> 1280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfn_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;129;01mand\u001b[39;00m brightness_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_brightness(img, brightness_factor)\n\u001b[0;32m   1282\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m contrast_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# early stopping parameters\n",
    "patience = 5  # the number of epochs to wait to see if got improvements\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "num_epochs_full = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop with early stopping\n",
    "for epoch in range(num_epochs_full):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in tqdm(FULL_TRAIN_LOADER, desc=f\"Epoch {epoch+1}/{num_epochs_full}\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimiser.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_loss / len(FULL_TRAIN_LOADER.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    logging.info(\"Epoch %d: Training Loss: %.4f\", epoch+1, epoch_train_loss)\n",
    "    \n",
    "    # evaluate on the validation set\n",
    "    final_model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in FULL_VAL_LOADER:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            outputs = final_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_val_loss = running_val_loss / len(FULL_VAL_LOADER.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    logging.info(\"Epoch %d: Validation Loss: %.4f\", epoch+1, epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_full} - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "    # if no improvement then early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # save the best model\n",
    "        torch.save(final_model.state_dict(), BEST_MODEL_PATH)\n",
    "        logging.info(\"Model saved to: %s\",  {BEST_MODEL_PATH})\n",
    "        logging.info(\"Epoch %d: New best model saved.\", epoch+1)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        logging.info(\"Early stopping triggered at epoch %d\", epoch+1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e01e8",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb3a3b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3373, Test Accuracy: 86.54%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.94      0.86      0.90       140\n",
      "       happy       0.92      0.93      0.92       148\n",
      "     relaxed       0.78      0.86      0.82       146\n",
      "         sad       0.84      0.81      0.82       138\n",
      "\n",
      "    accuracy                           0.87       572\n",
      "   macro avg       0.87      0.86      0.87       572\n",
      "weighted avg       0.87      0.87      0.87       572\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load best model from training \n",
    "final_model.load_state_dict(torch.load(BEST_MODEL_PATH)) \n",
    "final_model.to(DEVICE) \n",
    "final_model.eval() \n",
    " \n",
    "# initialise metrics \n",
    "correct = 0 \n",
    "total = 0 \n",
    "test_loss = 0.0 \n",
    "all_preds = [] \n",
    "all_labels = [] \n",
    " \n",
    "with torch.no_grad(): \n",
    "    for images, labels in FULL_TEST_LOADER: \n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE) \n",
    "        outputs = final_model(images) \n",
    "        # for test loss \n",
    "        loss = criterion(outputs, labels) \n",
    "        test_loss += loss.item() * images.size(0) \n",
    "        # for accuracy \n",
    "        _, predicted = torch.max(outputs, 1) \n",
    "        total += labels.size(0) \n",
    "        correct += (predicted == labels).sum().item() \n",
    " \n",
    "        all_preds.extend(predicted.cpu().numpy()) # move to cpu to ensure compatibility because numpy only operates on cpu \n",
    "        all_labels.extend(labels.cpu().numpy()) \n",
    " \n",
    "test_loss = test_loss / len(FULL_TEST_LOADER.dataset) \n",
    "test_accuracy = correct / total \n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\") \n",
    "logging.info(\"Test Loss: %.4f, Test Accuracy: %.2f%%\", test_loss, test_accuracy * 100) \n",
    " \n",
    "# plotting training and validation loss curves \n",
    "if 'train_losses' in globals() and 'val_losses' in globals(): \n",
    "    plt.figure(figsize=(10, 6)) \n",
    "    plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Training Loss\") \n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Validation Loss\") \n",
    "    plt.xlabel(\"Epoch\") \n",
    "    plt.ylabel(\"Loss\") \n",
    "    plt.title(\"Training and Validation Loss Curves\") \n",
    "    plt.legend() \n",
    "    plt.grid(True) \n",
    "    loss_curve_path = os.path.join(PROJECT_ROOT, \"results\", model_name, \"loss_curves.png\") #NOTE: specify by model name \n",
    "    plt.savefig(loss_curve_path) \n",
    "    plt.show() \n",
    "    logging.info(\"Training and Validation Loss curves saved to %s\", loss_curve_path) \n",
    "else: \n",
    "    logging.warning(\"train_losses and val_losses are not defined. Skipping loss curves plot.\") \n",
    " \n",
    "# generate and log classification report \n",
    "class_names = FULL_TEST_LOADER.dataset.classes \n",
    "report = classification_report(all_labels, all_preds, target_names=class_names) \n",
    "print(\"\\nClassification Report:\\n\", report) \n",
    "logging.info(\"Classification Report:\\n%s\", report) \n",
    " \n",
    "# generate confusion matrix \n",
    "cm = confusion_matrix(all_labels, all_preds) \n",
    " \n",
    "# save confusion matrix as image \n",
    "fig, ax = plt.subplots(figsize=(8, 6)) \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax) \n",
    "plt.xlabel(\"Predicted\") \n",
    "plt.ylabel(\"True\") \n",
    "plt.title(\"Confusion Matrix\") \n",
    "plt.tight_layout() \n",
    " \n",
    "\n",
    "conf_matrix_path = os.path.join(PROJECT_ROOT, \"results\", model_name, \"confusion_matrix.png\") #NOTE: specify by model name \n",
    "fig.savefig(conf_matrix_path) \n",
    "plt.show(fig) \n",
    " \n",
    "logging.info(\"Confusion matrix saved to: %s\", conf_matrix_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e7c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADLTEAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
