{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters in deep learning to tune are \n",
    "- the number of neurons\n",
    "- activation function\n",
    "- optimiser\n",
    "- learning rate\n",
    "- batch size\n",
    "- epochs \n",
    "- number of layers.\n",
    "\n",
    "\n",
    "Reference: \n",
    "- https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported. Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Optional, Callable\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision.models import efficientnet_b5, EfficientNet_B5_Weights\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "print(\"Libraries imported. Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import AutoImageProcessor, Dinov2ForImageClassification\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "This is adapted from SeparatingData.ipynb\n",
    "Download the processed dataset from Google Drive is yet to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(data_dir: str, zip_url: str, zip_filename: str, root_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Download and extract the dataset from Google Drive if it doesn't exist\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset zip file will be stored\n",
    "        zip_url (str): URL of the dataset zip file\n",
    "        zip_filename (str): Name for the downloaded zip file\n",
    "        root_dir (str): Directory where the dataset will be extracted\n",
    "    \"\"\"\n",
    "    # Create the data directory if it doesn't exist.\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "        print(f\"Created directory: {root_dir}\")\n",
    "\n",
    "    # Check if the dataset is already extracted.\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Downloading dataset from {zip_url} to {zip_filename}\")\n",
    "        gdown.download(zip_url, zip_filename, quiet=False)\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(root_dir)\n",
    "        print(f\"Extraction complete. Dataset available at {data_dir}\")\n",
    "    else:\n",
    "        print(f\"Dataset already exists at {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at ../input/final_split.zip\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../input/final_split.zip\"\n",
    "ZIP_URL = \"https://drive.google.com/uc?id=11t8m703wcNss3w5diJSUGBA_vXnCKChr\"\n",
    "ZIP_FILENAME = \"../input/final_split.zip\"\n",
    "ROOT_DIR = \"../input\"\n",
    "\n",
    "download_dataset(DATA_DIR, ZIP_URL, ZIP_FILENAME, ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Data Load\n",
    "For fasting hyperparameter tuning, use a subset of the dataset to find the most optimised set of hyperparameters.\n",
    "Loads dataset from processed dataset which should have been split to train, test, eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuning_data_loaders(\n",
    "    dataset_root: str,\n",
    "    transform: transforms.Compose,\n",
    "    batch_size: int = 32,\n",
    "    subset_fraction: float = 0.1,\n",
    "    random_seed: int = 42\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Creates DataLoaders for hyperparameter tuning using a subset of the dataset.\n",
    "\n",
    "    If the dataset_root directory contains subfolders 'train', 'eval', and 'test',\n",
    "    these are loaded directly. Otherwise, the full dataset is loaded and randomly split.\n",
    "    In either case, a subset of each split is sampled for faster tuning.\n",
    "\n",
    "    Note:\n",
    "      The transform provided is applied to each image as it is loaded by ImageFolder.\n",
    "      Even if the images in the pre-split folders have been preprocessed externally,\n",
    "      it is common to store raw images and apply transforms on the fly.\n",
    "\n",
    "    Args:\n",
    "        dataset_root (str): Root directory of the dataset. This should either be the parent\n",
    "            folder of the split directories or the folder containing all images.\n",
    "        transform (transforms.Compose): Transformations to apply to the dataset.\n",
    "        batch_size (int, optional): Batch size for DataLoaders. Defaults to 32.\n",
    "        subset_fraction (float, optional): Fraction of each split to use for tuning. Defaults to 0.1.\n",
    "        random_seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataLoader, DataLoader, DataLoader]: DataLoaders for training, validation, and test subsets.\n",
    "    \"\"\"\n",
    "    # Check if dataset_root contains pre-split 'train', 'eval', and 'test' subdir\n",
    "    split_dirs = ['train', 'eval', 'test']\n",
    "    if all(os.path.exists(os.path.join(dataset_root, sub)) for sub in split_dirs):\n",
    "        train_dataset = datasets.ImageFolder(root=os.path.join(dataset_root, 'train'), transform=transform)\n",
    "        print(f\"Full train dataset: {len(train_dataset)}\")\n",
    "        val_dataset = datasets.ImageFolder(root=os.path.join(dataset_root, 'eval'), transform=transform)\n",
    "        print(f\"Full val dataset: {len(val_dataset)}\")\n",
    "        test_dataset = datasets.ImageFolder(root=os.path.join(dataset_root, 'test'), transform=transform)\n",
    "        print(f\"Full test dataset: {len(test_dataset)}\")\n",
    "\n",
    "    else:\n",
    "        # If not pre-split, load the full dataset and split it randomly\n",
    "        full_dataset = datasets.ImageFolder(root=dataset_root, transform=transform)\n",
    "        total_len = len(full_dataset)\n",
    "        train_len = int(0.7 * total_len)\n",
    "        val_len = int(0.15 * total_len)\n",
    "        test_len = total_len - train_len - val_len\n",
    "        train_dataset, val_dataset, test_dataset = random_split(\n",
    "            full_dataset, [train_len, val_len, test_len],\n",
    "            generator=torch.Generator().manual_seed(random_seed)\n",
    "        )\n",
    "\n",
    "    def sample_subset(dataset, fraction):\n",
    "        \"\"\"Returns a subset of the dataset with the specified fraction of examples.\"\"\"\n",
    "        dataset_len = len(dataset)\n",
    "        subset_size = max(1, int(fraction * dataset_len))\n",
    "        indices = random.sample(range(dataset_len), subset_size)\n",
    "        return Subset(dataset, indices)\n",
    "\n",
    "    # Sample a subset from each split\n",
    "    if subset_fraction < 1.0:\n",
    "        subset_train_dataset = sample_subset(train_dataset, subset_fraction)\n",
    "        print(f\"Subset-train dataset: {len(train_dataset)}\")\n",
    "        subset_val_dataset = sample_subset(val_dataset, subset_fraction)\n",
    "        print(f\"Subset-val dataset: {len(val_dataset)}\")\n",
    "        subset_test_dataset = sample_subset(test_dataset, subset_fraction)\n",
    "        print(f\"Subset-test dataset: {len(test_dataset)}\")\n",
    "\n",
    "    subset_train_loader = DataLoader(subset_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    subset_val_loader = DataLoader(subset_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    subset_test_loader = DataLoader(subset_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return subset_train_loader, subset_val_loader, subset_test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train dataset: 8025\n",
      "Full val dataset: 579\n",
      "Full test dataset: 572\n",
      "Subset-train dataset: 8025\n",
      "Subset-val dataset: 579\n",
      "Subset-test dataset: 572\n",
      "DataLoaders for hyperparameter tuning are ready.\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATASET = os.path.abspath(\"../input/final_split_training_augmented\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define a transform. \n",
    "# In this notebook, we assume we doing it for EfficientNetB5, so we resize to ~456x456\n",
    "weights = EfficientNet_B5_Weights.DEFAULT\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((456, 456)),\n",
    "        transforms.ToTensor(),\n",
    "        weights.transforms()  # applies normalization as required\n",
    "    ])\n",
    "\n",
    "TRAIN_LOADER, VAL_LOADER, TEST_LOADER = create_tuning_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset_fraction=0.1,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"DataLoaders for hyperparameter tuning are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specifications\n",
    "This is where you should replace with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNetB5 Partial Transfer Learning:\n",
    "- https://discuss.pytorch.org/t/partial-transfer-learning-efficientnet/109689 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "rootPath = \"../input/final_split_training_augmented\"\n",
    "trainPath= rootPath + \"/train\"\n",
    "evalPath= rootPath + \"/eval\"\n",
    "testPath= rootPath + \"/test\"\n",
    "\n",
    "model_name = \"facebook/dinov2-small-imagenet1k-1-layer\"\n",
    "num_classes = 4\n",
    "batch_size = 32\n",
    "learning_rate = 5e-5\n",
    "epochs = 5\n",
    "reduction_ratio = 0.2  # Define the reduction ratio for the dataset\n",
    "\n",
    "\n",
    "# For MacOS MPS\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.backends.mps.is_available()\n",
    "print(\"Using device:\", \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# For CUDA\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Dataset ImageFolder\n",
      "    Number of datapoints: 8025\n",
      "    Root location: ../input/final_split_training_augmented/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           ) size: 1605\n",
      "Reduced Dataset ImageFolder\n",
      "    Number of datapoints: 579\n",
      "    Root location: ../input/final_split_training_augmented/eval\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
      "               ToTensor()\n",
      "           ) size: 115\n",
      "Type of the first element: <class 'tuple'>\n",
      "Type of the image in the first element: <class 'torch.Tensor'>\n",
      "Is the image a PIL Image? False\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def create_reduced_dataset(full_dataset, reduction_ratio):\n",
    "    total_size = len(full_dataset)\n",
    "    reduced_size = int(total_size * reduction_ratio)\n",
    "    indices = list(range(total_size))\n",
    "    random.shuffle(indices)\n",
    "    reduced_indices = indices[:reduced_size]\n",
    "    reduced_dataset = Subset(full_dataset, reduced_indices)\n",
    "    print(f\"Reduced {full_dataset} size: {len(reduced_dataset)}\")\n",
    "    return reduced_dataset\n",
    "\n",
    "trainDataset = datasets.ImageFolder(root=trainPath, transform=transform)\n",
    "evalDataset = datasets.ImageFolder(root=evalPath, transform=transform)\n",
    "\n",
    "# Commenting out the reduced dataset creation for full training (or change above reduction ratio to 1.0)\n",
    "trainDataset = create_reduced_dataset(trainDataset, reduction_ratio)\n",
    "evalDataset = create_reduced_dataset(evalDataset, reduction_ratio)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(trainDataset, batch_size=batch_size, shuffle=True, num_workers=num_classes)\n",
    "eval_dataloader = DataLoader(evalDataset, batch_size=batch_size, shuffle=False, num_workers=num_classes)\n",
    "\n",
    "\n",
    "type(trainDataset)\n",
    "first_element = trainDataset[0]\n",
    "print(f\"Type of the first element: {type(first_element)}\")\n",
    "image, label = first_element\n",
    "print(f\"Type of the image in the first element: {type(image)}\")\n",
    "print(f\"Is the image a PIL Image? {isinstance(image, Image.Image)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseDinov2(nn.Module):\n",
    "    def __init__(\n",
    "            self, model_name: str,\n",
    "            num_classes: int,\n",
    "            learning_rate: float, \n",
    "            freeze_backbone: bool = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.freeze_backbone = freeze_backbone\n",
    "\n",
    "        self.image_processor = AutoImageProcessor.from_pretrained(self.model_name)\n",
    "        self.dinov2 = Dinov2ForImageClassification.from_pretrained(self.model_name, num_labels=self.num_classes, ignore_mismatched_sizes=True)\n",
    "\n",
    "        if self.freeze_backbone:\n",
    "            for name, param in self.dinov2.named_parameters():\n",
    "                if \"classifier\" not in name:\n",
    "                    param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        self.optimizer = optim.AdamW(self.dinov2.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.dinov2(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        loss = self.criterion(logits, labels) if labels is not None else None\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def get_image_processor(self):\n",
    "        return self.image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-small-imagenet1k-1-layer and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated: BaseDinov2\n"
     ]
    }
   ],
   "source": [
    "model = BaseDinov2(model_name=model_name, num_classes=4, learning_rate=learning_rate, freeze_backbone=True)\n",
    "print(\"Model instantiated:\", model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "This is the part where you write the training function and load it to the ray tune scheduler.\n",
    "For this execution, ASHAscheduler is used with Optuna for bayesian optimisation techniques - which should be using the default Tree-Structured Parzen Estimator.\n",
    "If many parameters, this is would be more efficient than grid search and random search.\n",
    "\n",
    "References:\n",
    "- https://docs.ray.io/en/latest/tune/examples/includes/async_hyperband_example.html\n",
    "- https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html \n",
    "- https://docs.ray.io/en/latest/tune/examples/includes/mnist_pytorch.html\n",
    "- https://docs.ray.io/en/latest/tune/api/suggestion.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "\n",
    "CHECKPOINT_DIR = os.path.abspath(\"../models/Dinov2_hyptune\")\n",
    "def train_model(config, checkpoint_dir=CHECKPOINT_DIR, data_dir=None):\n",
    "    \"\"\"Training function for Ray Tune hyperparameter tuning.\n",
    "\n",
    "    This function instantiates the model with hyperparameters\n",
    "    specified in the config dictionary, trains the model on the global TRAIN_LOADER,\n",
    "    evaluates on VAL_LOADER, and reports the validation loss to Ray Tune.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Hyperparameter configuration. Expected keys include:\n",
    "            - lr (float): Learning rate.\n",
    "            - weight_decay (float): Weight decay for the optimizer.\n",
    "            - dropout (float): Dropout rate for the classifier.\n",
    "            - hidden_sizes (list or None): List of hidden layer sizes in the classifier.\n",
    "            - activation (str): Activation function to use ('relu', 'tanh', etc.).\n",
    "            - freeze_backbone (bool): Whether to freeze the model backbone.\n",
    "            - num_epochs (int): Number of training epochs.\n",
    "            - optimiser (callable, optional): Optimiser class. Default is optim.Adam.\n",
    "            - criterion (callable, optional): Loss function instance. Default is nn.CrossEntropyLoss().\n",
    "        checkpoint_dir (str, optional): Directory for checkpointing (if applicable).\n",
    "        data_dir (str, optional): Not used here; included for compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Checkpoint Folder exists\")\n",
    "    \n",
    "    # instantiate model with hyperparameters from config\n",
    "    model = BaseDinov2(\n",
    "        num_classes=num_classes,\n",
    "        # learning_rate=config.get(\"lr\", learning_rate),\n",
    "        # weight_decay=config.get(\"weight_decay\", None),\n",
    "        # dropout=config.get(\"dropout\", None),\n",
    "        # hidden_sizes=config.get(\"hidden_sizes\", None),\n",
    "        # activation=config.get(\"activation\", None),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer_class = config.get(\"optimiser\", optim.Adam)\n",
    "    optimizer = optimizer_class(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    \n",
    "    criterion=config.get(\"criterion\", nn.CrossEntropyLoss())\n",
    "    num_epochs=config.get(\"num_epochs\", 2)  # a low number for quick tuning, but update accordingly\n",
    "    \n",
    "    # image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    # model = Dinov2ForImageClassification.from_pretrained(model_name, num_labels=num_classes, ignore_mismatched_sizes=True)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    eval_losses = []\n",
    "    eval_accuracies = []\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "        # batch: In each iteration of this loop, batch will contain one batch of data loaded by your train_dataloader. Since you created the train_dataloader from a torch.utils.data.Dataset (specifically a Subset of ImageFolder), each batch will typically be a list or tuple containing:\n",
    "        # A tensor of images (the first element, batch[0]). -> PyTorch tensors of shape (batch_size, C, H, W) e.g. (32, 3, 224, 224) for a batch size of 32 and colour images resized to 224x224.\n",
    "        # A tensor of corresponding labels (the second element, batch[1]).\n",
    "        for batch in progress_bar:\n",
    "\n",
    "            inputs = BaseDinov2.image_processor(images=list(batch[0]), return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs['pixel_values'].size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            progress_bar.set_postfix({\"loss\": total_loss / total_samples, \"accuracy\": correct_predictions / total_samples})\n",
    "\n",
    "        epoch_loss = total_loss / total_samples\n",
    "        epoch_accuracy = correct_predictions / total_samples\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}\")\n",
    "        \n",
    "        \n",
    "        # Optionally, checkpoint the model.\n",
    "        if checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "    \n",
    "    # Evaluation on the validation set\n",
    "       # Evaluation loop\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_correct_predictions = 0\n",
    "    eval_total_samples = 0\n",
    "    current_epoch_true_labels = []\n",
    "    current_epoch_predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar_eval = tqdm(eval_dataloader, desc=f\"Evaluating Epoch {epoch+1}\", unit=\"batch\")\n",
    "        for batch in progress_bar_eval:\n",
    "            \n",
    "            inputs = BaseDinov2.image_processor(images=list(batch[0]), return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            labels = batch[1].to(device)\n",
    "\n",
    "            outputs = model(**inputs).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            eval_loss += loss.item() * inputs['pixel_values'].size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            eval_correct_predictions += (predicted == labels).sum().item()\n",
    "            eval_total_samples += labels.size(0)\n",
    "\n",
    "            current_epoch_true_labels.extend(labels.cpu().numpy())\n",
    "            current_epoch_predicted_labels.extend(predicted.cpu().numpy())\n",
    "        \n",
    "\n",
    "            progress_bar_eval.set_postfix({\"loss\": eval_loss / eval_total_samples, \"accuracy\": eval_correct_predictions / eval_total_samples})\n",
    "\n",
    "    eval_epoch_loss = eval_loss / eval_total_samples\n",
    "    eval_epoch_accuracy = eval_correct_predictions / eval_total_samples\n",
    "    eval_losses.append(eval_epoch_loss)\n",
    "    eval_accuracies.append(eval_epoch_accuracy)\n",
    "    print(f\"Epoch {epoch+1} Evaluation Loss: {eval_epoch_loss:.4f}, Evaluation Accuracy: {eval_epoch_accuracy:.4f}\")\n",
    "    \n",
    "    all_true_labels.extend(current_epoch_true_labels)\n",
    "    all_predicted_labels.extend(current_epoch_predicted_labels)\n",
    "    \n",
    "    # Report the metric to Ray Tune.\n",
    "    tune.report({\"loss\": eval_epoch_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-07 15:14:54</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:05.56        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.2/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Logical resource usage: 1.0/11 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                                       </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_7f3f5aec</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-04-07_11-32-55_698095_98916/artifacts/2025-04-07_15-14-48/train_model_2025-04-07_15-14-48/driver_artifacts/train_model_7f3f5aec_1_activation=gelu,criterion=ref_ph_449a4ea2,dropout=0.3928,freeze_backbone=True,hidden_sizes=256_128,lr=0.000_2025-04-07_15-14-48/error.txt</td></tr>\n",
       "<tr><td>train_model_1632e955</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2025-04-07_11-32-55_698095_98916/artifacts/2025-04-07_15-14-48/train_model_2025-04-07_15-14-48/driver_artifacts/train_model_1632e955_2_activation=leaky_relu,criterion=ref_ph_449a4ea2,dropout=0.1727,freeze_backbone=True,hidden_sizes=None,lr=0._2025-04-07_15-14-51/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th>activation  </th><th>criterion           </th><th style=\"text-align: right;\">  dropout</th><th>freeze_backbone  </th><th>hidden_sizes  </th><th style=\"text-align: right;\">         lr</th><th>optimiser           </th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_7f3f5aec</td><td>ERROR   </td><td>127.0.0.1:99836</td><td>gelu        </td><td>&lt;class &#x27;torch.n_e4b0</td><td style=\"text-align: right;\"> 0.392798</td><td>True             </td><td>[256, 128]    </td><td style=\"text-align: right;\">0.000132929</td><td>&lt;class &#x27;torch.o_ebe0</td><td style=\"text-align: right;\">   0.00635122 </td></tr>\n",
       "<tr><td>train_model_1632e955</td><td>ERROR   </td><td>127.0.0.1:99839</td><td>leaky_relu  </td><td>&lt;class &#x27;torch.n_e4b0</td><td style=\"text-align: right;\"> 0.17273 </td><td>True             </td><td>              </td><td style=\"text-align: right;\">0.00314288 </td><td>&lt;class &#x27;torch.o_b190</td><td style=\"text-align: right;\">   7.06897e-06</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 15:14:51,742\tERROR tune_controller.py:1331 -- Trial task failed for trial train_model_7f3f5aec\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=99836, ip=127.0.0.1, actor_id=48350b97aef620727f422f3501000000, repr=train_model)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/var/folders/k6/3tf4d74975j0pjkbm28_ct3c0000gn/T/ipykernel_98916/2968766.py\", line 35, in train_model\n",
      "TypeError: BaseDinov2.__init__() got an unexpected keyword argument 'activation'\n",
      "2025-04-07 15:14:54,011\tERROR tune_controller.py:1331 -- Trial task failed for trial train_model_1632e955\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=99839, ip=127.0.0.1, actor_id=9a79e873b439811650cf9b2b01000000, repr=train_model)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/air/_internal/util.py\", line 107, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/trainable/function_trainable.py\", line 261, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/var/folders/k6/3tf4d74975j0pjkbm28_ct3c0000gn/T/ipykernel_98916/2968766.py\", line 35, in train_model\n",
      "TypeError: BaseDinov2.__init__() got an unexpected keyword argument 'activation'\n",
      "2025-04-07 15:14:54,019\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/valencia/ray_results/train_model_2025-04-07_15-14-48' in 0.0040s.\n",
      "2025-04-07 15:14:54,023\tERROR tune.py:1037 -- Trials did not complete: [train_model_7f3f5aec, train_model_1632e955]\n",
      "2025-04-07 15:14:54,024\tINFO tune.py:1041 -- Total run time: 5.59 seconds (5.55 seconds for the tuning loop).\n",
      "2025-04-07 15:14:54,027\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 54\u001b[0m\n\u001b[1;32m     42\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[1;32m     43\u001b[0m     tune\u001b[38;5;241m.\u001b[39mwith_resources(train_model, {}), \u001b[38;5;66;03m# specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     tune_config\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mTuneConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/rose/lib/python3.10/site-packages/ray/tune/result_grid.py:161\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    150\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    max_t=100,           # max training iterations per trial\n",
    "    grace_period=10,     # min iterations before stopping\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"loss\", mode=\"min\", seed=42)\n",
    "\n",
    "# define search space in a config dictionary, i.e what are the values you want to try, this is just example of format\n",
    "'''\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"freeze_backbone\": tune.choice([True, False]),\n",
    "    \"num_epochs\": 2, \n",
    "    \"optimiser\": tune.choice([optim.Adam, optim.SGD]),\n",
    "    \"criterion\": tune.choice([nn.CrossEntropyLoss, nn.NLLLoss]),\n",
    "}\n",
    "'''\n",
    "\n",
    "# this is what i specified for the example because i am running on cpu\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\":tune.choice([\"leaky_relu\",\"gelu\"]),\n",
    "    \"freeze_backbone\": tune.choice([True]),\n",
    "    \"num_epochs\": 2,\n",
    "    \"optimiser\": tune.choice([optim.AdamW, optim.SGD]),\n",
    "    \"criterion\": tune.choice([nn.CrossEntropyLoss, nn.NLLLoss]),\n",
    "}\n",
    "\n",
    "# tuner object\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {}), # specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=optuna_search,\n",
    "        num_samples=2,  # number of hyperparameter combinations to try. Not the same as epochs\n",
    "    ),\n",
    "    run_config=tune.RunConfig(verbose=1),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "print(\"Best config:\", results.get_best_result(metric=\"loss\", mode=\"min\").config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
