{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters in deep learning to tune are \n",
    "- the number of neurons\n",
    "- activation function\n",
    "- optimiser\n",
    "- learning rate\n",
    "- batch size\n",
    "- epochs \n",
    "- number of layers.\n",
    "\n",
    "\n",
    "Reference: \n",
    "- https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune import CLIReporter\n",
    "\n",
    "# determine the project root - required to import DataHandler from utils folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# configuring log file\n",
    "log_dir = os.path.join(project_root, \"logs\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_filename = os.path.join(log_dir, \"dinov2_training_log.txt\") #NOTE: specify based on model training\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    filemode=\"w\",  #NOTE: previous logs would be overwritten\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "MODEL_SAVE_DIR = os.path.join(project_root, \"models\", \"dinov2\")\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True) \n",
    "\n",
    "RESULTS_SAVE_DIR = os.path.join(project_root, \"results\", \"dinov2\")\n",
    "os.makedirs(RESULTS_SAVE_DIR, exist_ok=True) \n",
    "\n",
    "from utils.DataHandler import download_dataset, create_full_data_loaders, create_tuning_data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Using device: CUDA (GPU)\")\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        print(\"Using device: MPS (Apple Silicon GPU)\")\n",
    "        os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        print(\"Using device: CPU\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "This is adapted from SeparatingData.ipynb\n",
    "Download the processed dataset from Google Drive if yet to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at ../input/final_split_training_augmented\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../input/final_split_training_augmented\"\n",
    "ZIP_URL = \"https://drive.google.com/uc?id=11t8m703wcNss3w5diJSUGBA_vXnCKChr\"\n",
    "ZIP_FILENAME = \"../input/final_split.zip\"\n",
    "ROOT_DIR = \"../input\"\n",
    "\n",
    "download_dataset(DATA_DIR, ZIP_URL, ZIP_FILENAME, ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Data Load\n",
    "For faster hyperparameter tuning, use a subset of the dataset to find the most optimised set of hyperparameters.\n",
    "Loads dataset from processed dataset which should have been split to train, test, eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-split datasets: train 8025, val 579, test 572\n",
      "Created tuning data loaders with subset fraction: 0.5\n",
      "Created subset datasets for hyperparameter tuning: train 4012, val 289, test 572\n",
      "Class Distribution for Subset Training:\n",
      "  angry     : 983\n",
      "  happy     : 1039\n",
      "  relaxed   : 1024\n",
      "  sad       : 966\n",
      "Class Distribution for Subset Validation:\n",
      "  angry     : 71\n",
      "  happy     : 75\n",
      "  relaxed   : 74\n",
      "  sad       : 69\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATASET = os.path.abspath(\"../input/final_split_training_augmented\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define data transformation \n",
    "# in this notebook, we are doing it for EfficientNetB5, so we resize to ~456x456\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "])\n",
    "\n",
    "TRAIN_LOADER, VAL_LOADER, TEST_LOADER = create_tuning_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset_fraction=0.5,\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data Load\n",
    "When doing the entire training for the entire dataset (full dataset not used in hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-split datasets: train 8025, val 579, test 572\n"
     ]
    }
   ],
   "source": [
    "FULL_TRAIN_LOADER, FULL_VAL_LOADER, FULL_TEST_LOADER = create_full_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DINOv2 Transfer Learning with Torch Hub:\n",
    "- https://debuggercafe.com/dinov2-for-image-classification-fine-tuning-vs-transfer-learning/\n",
    "- https://github.com/facebookresearch/dinov2/tree/e1277af2ba9496fbadf7aec6eba56e8d882d1e35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDINOv2(nn.Module):\n",
    "    \"\"\"DINOv2 model for transfer learning on the dog emotion dataset\n",
    "    with a configurable classification head for hyperparameter tuning, \n",
    "    i.e parameters you wish to tune need to be specified\n",
    "\n",
    "    This model uses a pretrained DINOv2 backbone and replaces its\n",
    "    classifier with a multi-layer fully connected network whose architecture\n",
    "    can be tuned (number of layers, neurons, and activation function)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model_variant: str = 'dinov2_vits14',\n",
    "                 num_classes: int = 4,\n",
    "                 dropout: float = 0.2,\n",
    "                 freeze_backbone: bool = False,\n",
    "                 hidden_sizes: Optional[List[int]] = None,\n",
    "                 activation: str = 'relu') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "            dropout (float): Dropout rate to apply in the classifier.\n",
    "            freeze_backbone (bool): If True, freeze the backbone layers.\n",
    "            hidden_sizes (Optional[List[int]]): List of sizes for hidden layers in the classifier.\n",
    "                If None, a single linear layer is used.\n",
    "            activation (str): Activation function to use in the classifier ('relu', 'tanh', etc.).\n",
    "        \"\"\"\n",
    "        super(BaseDINOv2, self).__init__()\n",
    "\n",
    "        # load DINOv2 backbone from Torch Hub\n",
    "        self.backbone = torch.hub.load('facebookresearch/dinov2', model_variant)\n",
    "        in_features = self.backbone.embed_dim  # e.g., 768 for vitb14, 384 for vits14\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Build the classifier based on the provided hidden_sizes\n",
    "        layers = []\n",
    "        input_dim = in_features\n",
    "        if hidden_sizes:\n",
    "            for hidden_dim in hidden_sizes:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(self._get_activation(activation))\n",
    "                input_dim = hidden_dim\n",
    "            # final classification layer.\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(nn.Linear(input_dim, num_classes))\n",
    "        else:\n",
    "            # single linear layer if no hidden layers specified\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(nn.Linear(input_dim, num_classes))\n",
    "        \n",
    "        self.classifier = nn.Sequential(*layers)\n",
    "    \n",
    "    def _get_activation(self, activation: str) -> Callable:\n",
    "        \"\"\"Returns an activation function based on the given string.\n",
    "\n",
    "        Args:\n",
    "            activation (str): Name of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            Callable: Activation function module.\n",
    "        \"\"\"\n",
    "        if activation.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == 'leakyrelu':\n",
    "            return nn.LeakyReLU()\n",
    "        elif activation.lower() == 'gelu':\n",
    "            return nn.GELU()\n",
    "        elif activation.lower() == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation.lower() == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated: BaseDINOv2\n",
      "BaseDINOv2(\n",
      "  (backbone): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.3, inplace=False)\n",
      "    (1): Linear(in_features=384, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.3, inplace=False)\n",
      "    (7): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "model = BaseDINOv2(num_classes=4, dropout=0.3, freeze_backbone=True, hidden_sizes=[256, 128], activation='relu').to(device)\n",
    "print(\"Model instantiated:\", model.__class__.__name__)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "This is the part where you write the training function and load it to the ray tune scheduler.\n",
    "For this execution, ASHAscheduler is used with Optuna for bayesian optimisation techniques - which should be using the default Tree-Structured Parzen Estimator.\n",
    "If many parameters, this is would be more efficient than grid search and random search.\n",
    "\n",
    "References:\n",
    "- https://docs.ray.io/en/latest/tune/examples/includes/async_hyperband_example.html\n",
    "- https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html \n",
    "- https://docs.ray.io/en/latest/tune/examples/includes/mnist_pytorch.html\n",
    "- https://docs.ray.io/en/latest/tune/api/suggestion.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = os.path.abspath(\"../models/hyptune\")\n",
    "def train_model( config, device=device, checkpoint_dir=CHECKPOINT_DIR, data_dir=None):\n",
    "    \"\"\"Training function for Ray Tune hyperparameter tuning.\n",
    "\n",
    "    This function instantiates the model with hyperparameters\n",
    "    specified in the config dictionary, trains the model on the global TRAIN_LOADER,\n",
    "    evaluates on VAL_LOADER, and reports the validation loss to Ray Tune.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Hyperparameter configuration. Expected keys include:\n",
    "            - lr (float): Learning rate.\n",
    "            - weight_decay (float): Weight decay for the optimizer.\n",
    "            - dropout (float): Dropout rate for the classifier.\n",
    "            - hidden_sizes (list or None): List of hidden layer sizes in the classifier.\n",
    "            - activation (str): Activation function to use ('relu', 'tanh', etc.).\n",
    "            - freeze_backbone (bool): Whether to freeze the model backbone.\n",
    "            - num_epochs (int): Number of training epochs.\n",
    "            - optimiser (callable, optional): Optimiser class. Default is optim.Adam.\n",
    "            - criterion (callable, optional): Loss function instance. Default is nn.CrossEntropyLoss().\n",
    "        checkpoint_dir (str, optional): Directory for checkpointing (if applicable).\n",
    "        data_dir (str, optional): Not used here; included for compatibility.\n",
    "    \"\"\"\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Checkpoint Folder exists\")\n",
    "    \n",
    "    # instantiate model with hyperparameters from config\n",
    "    model = BaseDINOv2(\n",
    "        num_classes=4,\n",
    "        dropout=config.get(\"dropout\", 0.3),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        hidden_sizes=config.get(\"hidden_sizes\", None),\n",
    "        activation=config.get(\"activation\", \"relu\")\n",
    "    ).to(device)\n",
    "\n",
    "    optimiser = config[\"optimiser\"](model.parameters(), config[\"lr\"], config[\"weight_decay\"])\n",
    "    criterion = config[\"criterion\"]()\n",
    "\n",
    "    num_epochs = config.get(\"num_epochs\", 2)  # a low number for quick tuning, but update accordingly\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in tqdm(TRAIN_LOADER, position=0, leave=True, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(TRAIN_LOADER.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # optionally, can checkpoint the model\n",
    "        if checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "    \n",
    "    # evaluation on the subset validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in VAL_LOADER:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_val_loss = total_loss / len(VAL_LOADER.dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Report the metric to Ray Tune\n",
    "    tune.report({\"loss\": avg_val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-16 01:21:58</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:33.99        </td></tr>\n",
       "<tr><td>Memory:      </td><td>13.9/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Logical resource usage: 2.0/11 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name  </th><th>status    </th><th>loc            </th><th>activation  </th><th>criterion           </th><th style=\"text-align: right;\">  dropout</th><th>hidden_sizes  </th><th style=\"text-align: right;\">         lr</th><th>optimiser           </th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>t_d6a5f33a  </td><td>TERMINATED</td><td>127.0.0.1:54408</td><td>tanh        </td><td>&lt;function &lt;lamb_9c60</td><td style=\"text-align: right;\"> 0.446399</td><td>[256, 128]    </td><td style=\"text-align: right;\">0.000561152</td><td>&lt;function &lt;lamb_9ab0</td><td style=\"text-align: right;\">   0.000796945</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         31.8765</td><td style=\"text-align: right;\">0.892305</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=54408)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=54408)\u001b[0m Using cache found in /Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m /Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m   warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m /Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m   warnings.warn(\"xFormers is not available (Attention)\")\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m /Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m   warnings.warn(\"xFormers is not available (Block)\")\n",
      "Epoch 1/1:   0%|          | 0/63 [00:00<?, ?it/s]\n",
      "Epoch 1/1:   2%|▏         | 1/63 [00:00<00:41,  1.51it/s]\n",
      "Epoch 1/1:   3%|▎         | 2/63 [00:01<00:33,  1.84it/s]\n",
      "Epoch 1/1:   5%|▍         | 3/63 [00:01<00:29,  2.01it/s]\n",
      "Epoch 1/1:   6%|▋         | 4/63 [00:02<00:28,  2.09it/s]\n",
      "Epoch 1/1:   8%|▊         | 5/63 [00:02<00:27,  2.13it/s]\n",
      "Epoch 1/1:  10%|▉         | 6/63 [00:02<00:26,  2.18it/s]\n",
      "Epoch 1/1:  11%|█         | 7/63 [00:03<00:25,  2.19it/s]\n",
      "Epoch 1/1:  13%|█▎        | 8/63 [00:03<00:24,  2.20it/s]\n",
      "Epoch 1/1:  14%|█▍        | 9/63 [00:04<00:24,  2.21it/s]\n",
      "Epoch 1/1:  16%|█▌        | 10/63 [00:04<00:23,  2.22it/s]\n",
      "Epoch 1/1:  17%|█▋        | 11/63 [00:05<00:23,  2.22it/s]\n",
      "Epoch 1/1:  19%|█▉        | 12/63 [00:05<00:22,  2.22it/s]\n",
      "Epoch 1/1:  21%|██        | 13/63 [00:06<00:22,  2.24it/s]\n",
      "Epoch 1/1:  22%|██▏       | 14/63 [00:06<00:21,  2.23it/s]\n",
      "Epoch 1/1:  24%|██▍       | 15/63 [00:06<00:21,  2.23it/s]\n",
      "Epoch 1/1:  25%|██▌       | 16/63 [00:07<00:21,  2.22it/s]\n",
      "Epoch 1/1:  27%|██▋       | 17/63 [00:07<00:20,  2.22it/s]\n",
      "Epoch 1/1:  29%|██▊       | 18/63 [00:08<00:20,  2.22it/s]\n",
      "Epoch 1/1:  30%|███       | 19/63 [00:08<00:19,  2.21it/s]\n",
      "Epoch 1/1:  32%|███▏      | 20/63 [00:09<00:19,  2.22it/s]\n",
      "Epoch 1/1:  33%|███▎      | 21/63 [00:09<00:19,  2.20it/s]\n",
      "Epoch 1/1:  35%|███▍      | 22/63 [00:10<00:18,  2.19it/s]\n",
      "Epoch 1/1:  37%|███▋      | 23/63 [00:10<00:18,  2.19it/s]\n",
      "Epoch 1/1:  38%|███▊      | 24/63 [00:11<00:17,  2.21it/s]\n",
      "Epoch 1/1:  40%|███▉      | 25/63 [00:11<00:17,  2.20it/s]\n",
      "Epoch 1/1:  41%|████▏     | 26/63 [00:11<00:16,  2.22it/s]\n",
      "Epoch 1/1:  43%|████▎     | 27/63 [00:12<00:16,  2.21it/s]\n",
      "Epoch 1/1:  44%|████▍     | 28/63 [00:12<00:15,  2.21it/s]\n",
      "Epoch 1/1:  46%|████▌     | 29/63 [00:13<00:15,  2.21it/s]\n",
      "Epoch 1/1:  48%|████▊     | 30/63 [00:13<00:14,  2.22it/s]\n",
      "Epoch 1/1:  49%|████▉     | 31/63 [00:14<00:14,  2.22it/s]\n",
      "Epoch 1/1:  51%|█████     | 32/63 [00:14<00:13,  2.21it/s]\n",
      "Epoch 1/1:  52%|█████▏    | 33/63 [00:15<00:13,  2.23it/s]\n",
      "Epoch 1/1:  54%|█████▍    | 34/63 [00:15<00:13,  2.22it/s]\n",
      "Epoch 1/1:  56%|█████▌    | 35/63 [00:15<00:12,  2.22it/s]\n",
      "Epoch 1/1:  57%|█████▋    | 36/63 [00:16<00:12,  2.23it/s]\n",
      "Epoch 1/1:  59%|█████▊    | 37/63 [00:16<00:11,  2.22it/s]\n",
      "Epoch 1/1:  60%|██████    | 38/63 [00:17<00:11,  2.22it/s]\n",
      "Epoch 1/1:  62%|██████▏   | 39/63 [00:17<00:10,  2.22it/s]\n",
      "Epoch 1/1:  63%|██████▎   | 40/63 [00:18<00:10,  2.22it/s]\n",
      "Epoch 1/1:  65%|██████▌   | 41/63 [00:18<00:09,  2.22it/s]\n",
      "Epoch 1/1:  67%|██████▋   | 42/63 [00:19<00:09,  2.22it/s]\n",
      "Epoch 1/1:  68%|██████▊   | 43/63 [00:19<00:08,  2.23it/s]\n",
      "Epoch 1/1:  70%|██████▉   | 44/63 [00:20<00:08,  2.22it/s]\n",
      "Epoch 1/1:  71%|███████▏  | 45/63 [00:20<00:08,  2.22it/s]\n",
      "Epoch 1/1:  73%|███████▎  | 46/63 [00:20<00:07,  2.23it/s]\n",
      "Epoch 1/1:  75%|███████▍  | 47/63 [00:21<00:07,  2.22it/s]\n",
      "Epoch 1/1:  76%|███████▌  | 48/63 [00:21<00:06,  2.22it/s]\n",
      "Epoch 1/1:  78%|███████▊  | 49/63 [00:22<00:06,  2.23it/s]\n",
      "Epoch 1/1:  79%|███████▉  | 50/63 [00:22<00:05,  2.23it/s]\n",
      "Epoch 1/1:  81%|████████  | 51/63 [00:23<00:05,  2.23it/s]\n",
      "Epoch 1/1:  83%|████████▎ | 52/63 [00:23<00:04,  2.23it/s]\n",
      "Epoch 1/1:  84%|████████▍ | 53/63 [00:24<00:04,  2.23it/s]\n",
      "Epoch 1/1:  86%|████████▌ | 54/63 [00:24<00:04,  2.22it/s]\n",
      "Epoch 1/1:  87%|████████▋ | 55/63 [00:24<00:03,  2.22it/s]\n",
      "Epoch 1/1:  89%|████████▉ | 56/63 [00:25<00:03,  2.21it/s]\n",
      "Epoch 1/1:  90%|█████████ | 57/63 [00:25<00:02,  2.23it/s]\n",
      "Epoch 1/1:  92%|█████████▏| 58/63 [00:26<00:02,  2.24it/s]\n",
      "Epoch 1/1:  94%|█████████▎| 59/63 [00:26<00:01,  2.23it/s]\n",
      "Epoch 1/1:  95%|█████████▌| 60/63 [00:27<00:01,  2.23it/s]\n",
      "Epoch 1/1:  97%|█████████▋| 61/63 [00:27<00:00,  2.21it/s]\n",
      "Epoch 1/1:  98%|█████████▊| 62/63 [00:28<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=54408)\u001b[0m Epoch 1/1, Training Loss: 1.1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 63/63 [00:28<00:00,  2.21it/s]\n",
      "2025-04-16 01:21:58,784\tINFO tensorboardx.py:308 -- Removed the following hyperparameter values when logging to tensorboard: {'optimiser': ('__ref_ph', '5f8989a2'), 'criterion': ('__ref_ph', 'f6251774')}\n",
      "2025-04-16 01:21:58,799\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/huiningonn/ray_results/basedinov2' in 0.0093s.\n",
      "2025-04-16 01:21:58,803\tINFO tune.py:1041 -- Total run time: 34.02 seconds (33.98 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'lr': 0.0005611516415334506, 'weight_decay': 0.0007969454818643932, 'dropout': 0.446398788362281, 'hidden_sizes': [256, 128], 'activation': 'tanh', 'freeze_backbone': True, 'num_epochs': 1, 'optimiser': <function <lambda> at 0x324ace680>, 'criterion': <function <lambda> at 0x324ace830>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=54408)\u001b[0m /Users/huiningonn/anaconda3/envs/dogemotion/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(train_model pid=54408)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    max_t=100,           # max training iterations per trial\n",
    "    grace_period=10,     # min iterations before stopping\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"loss\", mode=\"min\", seed=42)\n",
    "\n",
    "# define search space in a config dictionary, i.e what are the values you want to try, this is just example of format\n",
    "'''\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"freeze_backbone\": tune.choice([True, False]),\n",
    "    \"num_epochs\": 2, \n",
    "    \"optimiser\": tune.choice([optim.Adam, optim.SGD]),\n",
    "    \"criterion\": tune.choice([nn.CrossEntropyLoss, nn.NLLLoss]),\n",
    "}\n",
    "'''\n",
    "\n",
    "# this is what i specified for the example because i am running on cpu\n",
    "# config = {\n",
    "#     \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "#     \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "#     \"dropout\": tune.uniform(0.1, 0.5),\n",
    "#     \"freeze_backbone\": tune.choice([True]),\n",
    "#     \"num_epochs\": 2,\n",
    "# }\n",
    "\n",
    "# for dinov2 gpu\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),    \n",
    "    \"weight_decay\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"dropout\": tune.uniform(0.3, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\", \"sigmoid\", \"leakyrelu\", \"gelu\"]),\n",
    "    \"freeze_backbone\": True,\n",
    "    \"num_epochs\": 10,\n",
    "    \"optimiser\": tune.choice([\n",
    "        lambda params, lr, wd: optim.Adam(params, lr=lr, weight_decay=wd),\n",
    "        lambda params, lr, wd: optim.SGD(params, lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    ]),\n",
    "    \"criterion\": tune.choice([\n",
    "        lambda: nn.CrossEntropyLoss(),\n",
    "        lambda: nn.NLLLoss()\n",
    "    ])\n",
    "}\n",
    "\n",
    "def trial_dirname_creator(trial):\n",
    "    # this is meant to create short and unique directory name for each trial bc else too long for windows\n",
    "    # include a unique identifier (like trial.trial_id).\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "def trial_name_creator(trial):\n",
    "    return f\"t_{trial.trial_id}\"\n",
    "\n",
    "# tuner object\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {\"cpu\": 2, \"gpu\": 0}), # specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=optuna_search,\n",
    "        num_samples=15,  # number of trials to run\n",
    "        trial_dirname_creator=trial_dirname_creator,\n",
    "        trial_name_creator=trial_name_creator, \n",
    "    ),\n",
    "    run_config=tune.RunConfig(\n",
    "        name=\"basedinov2\",\n",
    "        storage_path=\"C:/ray_results/baseefficientnetb5\", #not required for mac\n",
    "        log_to_file=True,\n",
    "        verbose=1,\n",
    "        ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "print(\"Best config:\", results.get_best_result(metric=\"loss\", mode=\"min\", filter_nan_and_inf=False).config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=54408)\u001b[0m Validation Loss: 0.8923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full best config: {'lr': 0.0005611516415334506, 'weight_decay': 0.0007969454818643932, 'dropout': 0.446398788362281, 'hidden_sizes': [256, 128], 'activation': 'tanh', 'freeze_backbone': True, 'num_epochs': 1, 'optimiser': <function <lambda> at 0x324ace680>, 'criterion': <function <lambda> at 0x324ace830>}\n"
     ]
    }
   ],
   "source": [
    "best_config = results.get_best_result(metric=\"loss\", mode=\"min\").config\n",
    "print(\"Full best config:\", best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/huiningonn/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "final_model = BaseDINOv2(\n",
    "    num_classes=4,\n",
    "    dropout=best_config[\"dropout\"],\n",
    "    freeze_backbone=best_config[\"freeze_backbone\"],\n",
    "    hidden_sizes=best_config[\"hidden_sizes\"],\n",
    "    activation=best_config[\"activation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = final_model.to(device)\n",
    "logging.info(\"Model instantiated on device: %s\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = best_config[\"optimiser\"](final_model.parameters(), lr=best_config[\"lr\"], wd=best_config[\"weight_decay\"])\n",
    "criterion = best_config[\"criterion\"]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc50f9e2d5346af819848ede8085905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - Train Loss: 1.0588, Val Loss: 0.6893\n"
     ]
    }
   ],
   "source": [
    "# early stopping parameters\n",
    "patience = 5  # the number of epochs to wait to see if got improvements\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "num_epochs_full = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# training loop with early stopping\n",
    "for epoch in range(num_epochs_full):\n",
    "    final_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in tqdm(FULL_TRAIN_LOADER, desc=f\"Epoch {epoch+1}/{num_epochs_full}\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimiser.zero_grad()\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_train_loss = running_loss / len(FULL_TRAIN_LOADER.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    logging.info(\"Epoch %d: Training Loss: %.4f\", epoch+1, epoch_train_loss)\n",
    "    \n",
    "    # evaluate on the validation set\n",
    "    final_model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in FULL_VAL_LOADER:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = final_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "    epoch_val_loss = running_val_loss / len(FULL_VAL_LOADER.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    logging.info(\"Epoch %d: Validation Loss: %.4f\", epoch+1, epoch_val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs_full} - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
    "    \n",
    "    # if no improvement then early stopping\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        epochs_without_improvement = 0\n",
    "        # save the best model\n",
    "        best_model_path = os.path.join(MODEL_SAVE_DIR, \"dinov2_best_model.pt\")\n",
    "        torch.save(final_model.state_dict(), best_model_path)\n",
    "        logging.info(\"Model saved to: %s\",  {best_model_path})\n",
    "        logging.info(\"Epoch %d: New best model saved.\", epoch+1)\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        logging.info(\"Early stopping triggered at epoch %d\", epoch+1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n",
      "Test Loss: 0.6916, Test Accuracy: 72.73%\n"
     ]
    }
   ],
   "source": [
    "# load best model from training\n",
    "final_model.load_state_dict(torch.load(best_model_path))\n",
    "final_model.eval()\n",
    "\n",
    "print(len(FULL_TEST_LOADER.dataset))\n",
    "\n",
    "# Evaluate on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in FULL_TEST_LOADER:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "test_loss = test_loss / len(FULL_TEST_LOADER.dataset)\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "logging.info(\"Test Loss: %.4f, Test Accuracy: %.2f%%\", test_loss, test_accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAIjCAYAAADvBuGTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAT8pJREFUeJzt3QmcjXX///HPWMYYDEIYZEu2LEXclqLsSWiTCqm4uVMiyb6Gu01KpJQUddMid3eL9U6laAotbku27HtqYhjMnP/j8/39r/M4Z+bMmGt8x1nm9Xw8rsy5znWuc13n+tJ5z/f7/VxRHo/HIwAAAACAi5Ln4l4OAAAAAFCEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAuAj333+/VKpUKVuvHTdunERFRUkk++2338w5zp0795K/t76vfsYOPQZdp8d0IXpN9dqGSlsBAIQHwhWAiKRforOyrFq1KtiHmus9+uij5lps3749w21Gjhxptvn5558llB04cMAEuh9//FFCLeA+99xzEg4OHz4sQ4YMkRo1akhsbKwUKlRIGjRoIE899ZT88ccfwT48AMhUvsyfBoDwNG/ePL/Hb7/9tixfvjzd+po1a17U+8yePVtSU1Oz9dpRo0bJsGHDJLe79957Zfr06fLuu+/KmDFjAm7zr3/9S+rUqSN169bN9vv06NFD7r77bilQoIDkZLgaP3686aGqX7++tbaSW3z//fdy8803y8mTJ+W+++4zoUr98MMP8s9//lO++uorWbZsWbAPEwAyRLgCEJH0i5mvtWvXmnCVdn1aSUlJ5rflWZU/f/5sH2O+fPnMkts1btxYrrzyShOgAoWrNWvWyK5du8yX64uRN29eswTLxbSV3EB7pbp27Wqu0YYNG0zPla9JkyaZgGrDqVOnTI8YANjGsEAAuVbLli3l6quvlnXr1skNN9xgQtWIESPMc//+97+lY8eOEh8fb3o6qlatKhMnTpSUlJRM59H4DsF67bXXzOv09dddd535rfyF5lzp4wEDBsjixYvNselra9euLUuWLEl3/DqksWHDhhITE2Pe59VXX83yPK6vv/5a7rzzTrniiivMe1SoUEEGDRokp0+fTnd+hQsXlv3790uXLl3Mz6VKlTLDttJ+FvrlWLcvWrSoFCtWTHr16pXlYVzae7VlyxZZv359uue0R0vPqXv37nL27FkTwLRHQ99HvyBff/318sUXX1zwPQLNufJ4PGa4Wfny5c31v/HGG+V///tfutf+/vvv5py190w/g7i4OOnQoYP89NNPftdDr7Pq3bu3d+ipM98s0Jwr/ZL/+OOPm89fr0P16tVN29Hjym67yK4jR47Igw8+KKVLlzZtql69evLWW2+l227BggXm8y9SpIj5HPQzefHFF73Pnzt3zvTeVatWzeynRIkS0rx5c/PLjcxo+9V2NnXq1HTBSulxaW9vRnPqMpov51z3L7/8Uv7xj3/I5Zdfbq73Bx984F0f6Fj0uY0bN3rXafu844475LLLLjPnpX/3Pv74Y7/XZffcAUQOfmUKIFc7fvy4+ZKsw8W0V0u/wDlfyPRL9ODBg82f//3vf82X+sTERHn22WcvuF8NBH/99Zf8/e9/N1/SnnnmGbnttttk586dF+zBWL16tSxatMh8EdQvsC+99JLcfvvtsmfPHvNlTelv9tu3by9ly5Y1X+Y06EyYMMEEn6x4//33TS9d//79zT4TEhLM0Lx9+/aZ53zpvtu1a2d6mPSL/4oVK+T55583gU5frzQMdO7c2Rx7v379zHDLjz76yASsrIYrPQ/93K699lq/937vvfdMgNIgeOzYMXn99ddN0OrTp4/5jN944w1zfHoOaYfiXYheUw1XOhRNFw13bdu2NSHOl143DTYaSCtXrmzmBekX8BYtWsimTZtMCNdz1mug++zbt685ZtW0adOA762f2a233mqCoYYaPfalS5fKE088YULGCy+84LpdZJeGav1lg8570xCn56jtQEOKBuSBAwea7TQk6GffqlUrefrpp826zZs3yzfffOPdRgPPlClT5KGHHpJGjRqZvzM6rE8/2zZt2mR4DBpUChYsaAJMTtDPTf9+6PXRUKu/PNG/29q+9Dr6WrhwoQmvGmSVBu5mzZpJuXLlzFBeDfX6Ov2Fw4cffmh63C7m3AFEEA8A5AIPP/ywdgX4rWvRooVZN2vWrHTbJyUlpVv397//3RMbG+s5c+aMd12vXr08FStW9D7etWuX2WeJEiU8v//+u3f9v//9b7P+P//5j3fd2LFj0x2TPo6OjvZs377du+6nn34y66dPn+5d16lTJ3Ms+/fv967btm2bJ1++fOn2GUig85syZYonKirKs3v3br/z0/1NmDDBb9trrrnG06BBA+/jxYsXm+2eeeYZ77rz5897rr/+erP+zTffvOAxXXfddZ7y5ct7UlJSvOuWLFliXv/qq69695mcnOz3uhMnTnhKly7teeCBB/zW6+v0M3boMeg6vUbqyJEj5rPu2LGjJzU11bvdiBEjzHZ67g695r7HpXQ/BQoU8Ptsvv/++wzPN21bcT6zp556ym+7O+64w1wH3zaQ1XYRiNMmn3322Qy3mTZtmtlm/vz53nVnz571NGnSxFO4cGFPYmKiWTdw4EBPXFycuQ4ZqVevnvlM3SpevLh5bValvb4O/Yx9r51z3Zs3b57uuLt37+65/PLL/dYfPHjQkydPHr/r2qpVK0+dOnX8/u5rm2natKmnWrVqF33uACIHwwIB5Go6vEqHcKWlv0F3aO+I9phoT4T29ujwoAvp1q2bFC9e3PvY6cXQHpALad26tekVcmgRBx1+5bxWe3O090h/a649Jg6dt6S9cFnhe376W3w9P+1h0e+s2iuWlvZG+dLz8T2Xzz77zMwfc3qylM6deeSRRySrtOdQe860aIFDe7Kio6NNj5GzT32stDiEDtc7f/68GaIVaEhhZvQz1B4qPUbfoZSPPfZYwHaSJ08e7+evPZ7a66HD+Ny+r+9npuej1RJ96TBBvQ6ff/65q3ZxMfRYypQpY3qlHNrDqsemxSWcoXM63FPbS2bD3HQb7enZtm2bq2PQXh7tkcsp2tOZds6d/j3V4ZC+VUN1uKC2LX1OaRvTnuu77rrL+2+BLtoGtMdUz1N7Gi/m3AFEDsIVgFxNh/k4X9Z96RckHeqj83r0C6wOJ3KKYfz5558X3K8OYfPlBK0TJ064fq3zeue1+mVQh3FpmEor0LpAdCiZDvnS+SPOPCpnaFTa89O5I2mHG/oej9q9e7cZoqj78qXhI6t0aKZ++dVApc6cOWOGFmpg9A2qOg9Ig4Uzp0WP7dNPP83SdfGlx6x0fowv3Z/v+yn9sq3D9HRbDVolS5Y022lpeLfv6/v+Go7TBgqngqVzfFltFxdD30vPzQmQGR2LDq276qqrzDXReUsPPPBAunlfOjRShxLqdjofS4c5ZqWEvv490/CSU3SoY1o6tFb/juswQIf+rEM09fiVDpXUsDt69GhzzX2XsWPHev9OXsy5A4gchCsAuZpvD45Dvxxp0NBiBfpl6T//+Y/5Tb0zxyQr5bQzqkqXtlCB7ddmhfa86PwPDSRPPvmkmUuk5+cUXkh7fpeqwp4WGtDj0jksWhhAP3f9sq3zsRzz5883oVB7cHSulX6x12O/6aabcrTM+eTJk838Oy18osegc6P0fXVezqUqr57T7SKr10jv4aXzo5z5Yhq0fOfW6We0Y8cOmTNnjpmzpHPkdB6d/pkZLWLx66+/ppvv5lbaQiuZ/V3XoKw9wBritQdUe6B0/pjTa6Wc66sFTfSaB1qcX2pk99wBRA4KWgBAGjpESIf8aPEA/bLk0HLgoUC/4GqvTaCb7mZ2I17HL7/8Yr7Eag9Qz549vesvpqJZxYoVZeXKlWYImW/v1datW13tR4OUBiYdEqc9WNqb0alTJ78hW1WqVDHXxncon9OD4PaYlQ7h0n06jh49mq43SN9XKwlqoEsbxLUXy5GVSo2+769DEzVA+vZeOcNOneO7FPS9tIdFg4Rv71WgY9GeXr0muuj22pulxT20Z8cJGdojqsNtddE2oX+PtNiDFnrIiO5Py+5ruPYdnpgR7bVLW41Sg9nBgwddnbsGKf27oO1Xi3NoWPUNV07b0GGSOjTzQrJz7gAiBz1XAJBBD4Fvj4B+aZs5c6aEyvHplzztcdKb1voGq7TzdDJ6fdrz0599y2m7pZX29Df/r7zyil8PglYgdEN7EbQkun7Wei5aYVGDZGbH/t1335kv5W7pZ6hfmPUYffc3bdq0dNvq+6btIdJqes5cG4dz76SslKDXz0w/o5dfftlvvQ4/1JCW1flzNuixHDp0yG94nF5P/Ww0LDtDRvWXDr40iDk3dk5OTg64jb5eQ5fzfEZ0Xp8OLdU5Zxr+09Khd1rZ0aG9l77z85Te/iCjnqvM2oEGIj13XbTKn+8QQv1lhlZS1AAZKLhpGHdk99wBRA56rgAgDS3soL8V16FOOqFfv+jOmzfvkg6/uhD9TfiyZctMeWgtIuF8SdehSDps60LDr/SLqQ5z0nCgvUPaW3Axc3e010GPRctU632katWqZXqX3M5H0i+jGrCceVe+QwLVLbfcYvar8+G0lLb2Js6aNcu8n/YSuOHcr0tLZ+t+NWBoMQ8Ndb69Uc776hBR7Y3Q9qG9f++8845fj5fSz1WLGugxaW+Uhi0tYR9ovo9+ZtobNnLkSPOZ6X2l9JrqPda0qIZv8QobtGdG57GlpZ+3lo7X8KBDLvW+b3qvKO2t0yFyGjadnjXtfdECDzoMU+dc6VwsDWA6R8mZn6XXQsOI3gtLQ4uWItd9aYn3zOjfOR2ep9dB96dzHHUfSouG6E2mmzRp4t1ej0UDmZaj1+GkOoxXh2umvXYXogFbQ7zev0uLdejtBtKaMWOGuV+VzqPSwhh63bUcv4Z6LcLi3O8su+cOIIIEu1whAASzFHvt2rUDbv/NN994/va3v3kKFizoiY+P9wwdOtSzdOlSs48vvvjigqXYA5W9Tls6OqNS7HqsFyovrVauXGlKomuJ7qpVq3pef/11z+OPP+6JiYm54OexadMmT+vWrU2Z7ZIlS3r69OnjLe3tW0Zc37NQoULpXh/o2I8fP+7p0aOHKdVdtGhR8/OGDRuyXIrd8emnn5rXlC1bNl35cy1/PXnyZPN5aBl0Pf9PPvkk3XXISil2pfsfP368eS+91i1btvRs3Lgx3eetJbj1s3W2a9asmWfNmjWmDeniS8vu16pVy1sW3zn3QMf4119/eQYNGmTaWP78+U1Zb207vqXh3baLtJw2mdEyb948s93hw4c9vXv3Nu1B25SWHk973T744ANP27ZtTfly3eaKK64wtyjQ8uUOLS3fqFEjT7FixcxnVaNGDc+kSZNMafesOHDggPlMrrrqKtOW9ZYDWvZf9/Hnn3/6Xbsnn3zSHK9u065dO1OqPqNS7FomPyPLly8322gJ/L179wbcZseOHZ6ePXt6ypQpY65VuXLlPLfccov5TGydO4DwF6X/CXbAAwDYob0QlIIGACA4mHMFAGFKy7H70kCl9yvSYUkAAODSo+cKAMKUTv7XOTI6/0PnvmgxCZ04r/OG0t67CQAA5DwKWgBAmNIboOokf63ypvfr0cn+ej8mghUAAMFBzxUAAAAAWMCcKwAAAACwgHAFAAAAABYw5yqA1NRUOXDggLlpot48FAAAAEDu5PF45K+//pL4+HjJkyfzvinCVQAarCpUqBDswwAAAAAQIvbu3Svly5fPdBvCVQDaY+V8gHFxccE+HGTg3LlzsmzZMmnbtq3kz58/2IeDMECbgVu0GbhBe4FbtJnwkJiYaDpenIyQGcJVAM5QQA1WhKvQ/gcpNjbWXCP+QUJW0GbgFm0GbtBe4BZtJrxkZboQBS0AAAAAwALCFQAAAABYQLgCAAAAAAuYcwUAAICwkJKSYuYpRQo9l3z58smZM2fMuSE48ubNa66DjVswEa4AAAAQ8k6ePCn79u0z9xyKFHouZcqUMRWqubdqcGlhkbJly0p0dPRF7YdwBQAAgJCmvToarPQLcKlSpSImiKSmpprQWLhw4QvenBY5F3DPnj0rR48elV27dkm1atUu6loQrgAAABDyw+f0S7AGq4IFC0qk0HClX+xjYmIIV0GkbUpL4e/evdt7PbKLqwgAAICwECk9Vgg9tsIt4QoAAAAALCBcAQAAAIAFhCsAAAAgTFSqVEmmTZuW5e1XrVplhlP+8ccfOXpc+D+EKwAAAMAyDTSZLePGjcvWfr///nvp27dvlrdv2rSpHDx4UIoWLSo5iRD3f6gWCAAAAFimgcaxcOFCGTNmjGzdutW7TsuvO7QS4vnz582NbC9EKya6ofdt0ntp4dKg5woAAABhRcNI0tnzQVmyehNjDTTOor1G2qvjPN6yZYsUKVJEPv/8c2nZsqUpBb569WrZsWOHdO7cWUqXLm3C13XXXScrVqzIdFig7vf111+Xrl27mvuA6X2aPv744wx7lObOnSvFihWTpUuXSs2aNc37tG/f3i8MatB79NFHzXYlSpSQJ598Unr16iVdunTJ9jU7ceKE9OzZU4oXL26Os0OHDrJt2zbv81oGvVOnTub5QoUKSe3ateWzzz7zvvbee+/1luLXc3zzzTclFNFzBQAAgLBy+lyK1BqzNCjvvWlCO4mNtvMVesSIEWZ44NVXX21CzN69e+Xmm2+WSZMmSYECBeTtt982gUN7vK644ooM9zN+/Hh55pln5Nlnn5Xp06ebIKJh5bLLLgu4fVJSkjz33HMyb948U4L8vvvukyFDhsg777xjnn/66afNzxpgNIC9+OKLsnjxYrnxxhuzfa7333+/CVMa/OLi4kxg03PdtGmTucfUww8/bO4x9dVXX5lwpeud3r3Ro0ebxxpGS5YsKdu3b5fTp09LKCJcAQAAAEGgwUoDi4YNDTkahurVq+d9fuLEifLRRx+ZQDJgwIBMg0v37t3Nz5MnT5aXXnpJEhISTI9URjdlnjVrllStWtU81n1PmDDB+7wGtOHDh5veMPXyyy97e5GyY9v/D1XffPONmQOmNLxVqFDBhLY777xT9uzZI7fffrvUqVPHPF+lShXv6/W5a665Rho2bOjtvQtVhCsAAACElYL585oepGC9ty1OWHCcPHnSBK5PP/3UDNPT4XnaQ6PhIjN169b1/qy9PhrWjhw5kuH2OizPCVaqbNmy3u3//PNPOXz4sDRq1Mj7fN68eaVBgwaSmpqarfPcvHmzmU/WuHFj7zrtqatevbp5TukwxP79+8uyZcukdevWJmg556Xr9fH69eulbdu2ZniiE9JCDXOuAAAAEFZ0DpEOzQvGou9tiwYhXzo0T3uqtPfp66+/lh9//NH05OhwuczosLq0n09mQSjQ9lmdS5ZTHnroIdm5c6f06NFDfvnlFxM8tQdN6fwsHeY4aNAgOXDggLRq1cp8VqGIcAUAAACEAB02p0P8dDiehiotfvHbb79d0mPQ4htaUENLvjtSUlJMr1F21axZ0/TCfffdd951x48fN3PJatWq5V2nwwT79esnixYtkscff1xmz57tfU6LWWhRjfnz55uCHq+99pqEIoYFAgAAACFAq+BpsNAiFtqbpIUcsjsU72I88sgjMmXKFLnyyiulRo0apgdJK/Zlpdful19+MZUQHfoanUemVRD79Okjr776qnl+2LBhUq5cObNePfbYY6aH6qqrrjLv9cUXX5hQprSMvQ5L1AqCycnJ8sknn3ifCzWEKwAAACAETJ06VR544AEzn0ir4mlFvcTExEt+HPq+hw4dMqXTdb6V3rS4Xbt25ucLueGGG/we62u010orDw4cOFBuueUWM8xRt9MiGc4QRe0d04qB+/btM3PGtBjHCy+84L1XlxbY0F48LcV+/fXXy4IFCyQURXmCPcAyBGkj1i5RndCnFxehSSvd6F9KLeOZduwwEAhtBm7RZuAG7SXnnDlzRnbt2iWVK1eWmJgYiRTaK6XfO51qgaF8nNpTdNddd5kKhrmtjSW6yAb0XAEAAADw0uIRWrWvRYsWZhielmLX4HHPPfcE+9BCXuhGZAAAAACXnPaizZ07V6677jpp1qyZmUe1YsWKkJ3nFEqCGq70Dsw6YS8+Pt5MdtObiF3IqlWr5NprrzV3rdZJdnrhfem9AXRfvotOxAMAAABwYVq1TysX6jA4HRL37bffpptLhRAMV6dOnTLVQ2bMmJGl7bU7smPHjuZO1lr3X6uKaE38pUuX+m2nlUT0xmvOsnr16hw6AwAAAAAIgTlXWm5Rl6yaNWuWmWT2/PPPm8faNanBSSuJaAUTh94BWu8LAAAAAACXSlgVtFizZo20bt3ab52GKu3B8rVt2zYz1FArfTRp0sTU6b/iiisy3K9O1NPF4ZS81Ko/uiA0OdeGa4Ssos3ALdoM3KC95Bz9TLXAtVatC8Z9n3KKU7TbOTcEj37+eh20raUtOe/m73RYhSutt693jPaljzUMnT592tS9b9y4sZmHVb16dTMkcPz48aYW/saNG/1uaOZLw5dul5ZWSYmNjc2x84Edy5cvD/YhIMzQZuAWbQZu0F7sc0YlnTx50twjKdL89ddfwT6EXO/s2bMmT2hNCL0vl6+kpKTIDFdZ4TvMsG7duiZsVaxYUd577z158MEHA75Gb0o2ePBg72MNazqRr23bttznKoTpbxH0f2Bt2rThfiLIEtoM3KLNwA3aS87eg2jv3r1SuHDhiLrPlfaUaLDSDgAtwobgtjHtqNHCHYHucxWR4Up/Y3H48GG/dfpYA5B+GIEUK1ZMrrrqKtm+fXuG+9XKg7qkpf8w8o9j6OM6wS3aDNyizcAN2ot9KSkpJnxoifBQvtmuW85QQOfcEDz6+et1CPT3183f57C6ijp/auXKlX7r9DdEuj4j2n28Y8cOKVu27CU4QgAAAMCeli1b+tUXqFSpkkybNi3T12T1FkcXYms/uUlQw5UGHy2protTal1/3rNnj3e4Xs+ePb3b9+vXT3bu3ClDhw6VLVu2yMyZM81wv0GDBnm3GTJkiHz55Zfy22+/mZr8Xbt2NZPSunfvHoQzBAAAQG6k93Jt3759wOe+/vprE1x+/vln1/v9/vvvpW/fvmKT3ie2fv366dZr/QI3lb2zQ2sl6EizSBHUYYE//PCDuWeVw5n31KtXL/NB6wV1gpbSMuyffvqpCVMvvviilC9fXl5//XW/Muz79u0zQer48eNSqlQpad68uaxdu9b8DAAAAFwKOtf/9ttvN99N9TurrzfffFMaNmxo6gO4mc+jLuV3Wm5tFGY9V9rNqRP50i4arJT+uWrVqnSv2bBhgymdrsP97r//fr/nFyxYIAcOHDDPa2PWx1WrVr2k5wUAAIAcpCXMz54KzvL/y6dfyC233GKCkPO91nfk1vvvv2/Cl3YG6J9aSE0rVNepU0f+9a9/ZbrftMMC9RZEThGGWrVqBaxW+eSTT5oaBPoeVapUkdGjR3vLi+vxadXsn376yfSm6eIcc9phgb/88ovcdNNNptZBiRIlTA+ano/j/vvvly5dushzzz1npuToNg8//PBF3Z5AO1o6d+5siplonYW77rrLrwaDHrd21mhREH2+QYMGpgNH7d692/QgFi9eXAoVKiS1a9eWzz77THJSWBW0AAAAAORcksjk+OC894gDItGFslQ+Xqe3aFAZOXKktxqgBist0KEjrbTXSofj6fM6NE5HaPXo0cN0DDRq1ChLBTFuu+02c2ui7777Tv788890939VGjz0OPQ+sBqQ+vTpY9bpVJtu3bqZWxYtWbJEVqxYYbYvWrRoun2cOnXKjBbTWgc6NPHIkSPy0EMPyYABA/wC5BdffGGClf6pBeV0/3qO+p5u6fk5wUqn/WiJdA1ruk+nA+bee++Va665Rl555RUzFUinGDkFKHRbLbGu5dU1XG3atMnsKycRrgAAAIAc8MADD8izzz5rgoGOvnKGBOpwQQ0wGnAeeeQR0+Oi1er056VLl5qaAlkJVxqGtA6BvkaDk5o8eXK6eVKjRo3y6/nSGgU6ukvDlfZCaeBw7iWWkXfffdeUK3/77bdNUFEvv/yy6Rl6+umnvfeiLV68uFmvQadGjRrSsWNHU5AuO+FKX6dhUOsyaO+e0vfXHigNeNddd53p2XriiSfMe6lq1ap5X6/P6WetPYJKe+1yGuEKAAAA4SV/7P/1IAXrvbNIv/A3bdpU5syZY8KV9uRoMYsJEyaY57UHS8PXxx9/LPv37ze9LDq1RYfvZcXmzZtN6HCClQpURXvhwoXy0ksvmSk1OoxPe4Dc3stV36tevXreYKWaNWtmepe2bt3qDVe1a9c2wcqhvVgakLLDOT8nWCkd+qi9fPqchiut2aA9aPPmzZPWrVvLnXfe6Z0S9Oijj0r//v1l2bJl5jkNWjrPLSeFVSl2AAAAQHSInQ7NC8bi8ma/Oqfqww8/NDcL1l4r/eLfokUL85zOTZo1a5bpedFhdDqkTYfeaciyZc2aNWbo3M033yyffPKJqV2gwxBtvkdm94SKiory3s8rJ2ilw//973+mh+y///2vCV8fffSReU5Dl1Ya16GWGvC0iMj06dMlJxGuAAAAgByiBRh0yJ8Oq9MhbTpU0Jl/9c0335jQc99995leIR229uuvv2Z53zVr1pS9e/eaCtsOrZLtS29NVLFiRROoNFzosDkt9OArOjra9KJd6L20eITOvXLo8eu5Va9eXXJCzf9/fro4dN7UH3/8YUKUQ4t1aDVx7aHSOWgaYh3a66W3c1q0aJE8/vjjMnv2bMlJhCsAAAAgh+h8Ji3AoPdv1RDkW+lag472WGkA0mFuf//73/0q4V2IDnXTYKG3MdLgo0MONUT50vfQuUc6x0qHBerwQKdnx3celnO/2WPHjpmhiWlp75dWJNT30gIYetw6R0x7hZwhgdmlwc65962z6Oeh56fzpfS9169fLwkJCaZIiPb8aVA8ffq0KaihxS00MGrY07lYGsqUFvfQ+Wh6bvp6PWbnuZxCuAIAAABykA4NPHHihBny5zs/SoOQ9lhpAQqdk6UFJbSUeVZpr5EGJQ0ZWgBDh8FNmjTJb5tbb73V9OpoCNGqfRrktBS7L52LpDc81pLmWj4+UDl4nQemQeX33383c53uuOMOadWqlSlecbFOnjxpKv75LlooQ3v4/v3vf5siGVpuXsOW9u7pHDKlc7u0nL0GLg2Z2kuon6WWlndCm1YM1ECl56fbzJw5U3JSlEdvLAU/WhZTK7hoOUu3k/1w6eg9E/ReBdqdnnZ8LxAIbQZu0WbgBu0l52iVOu19qFy5suk9iRQ6F0m/dzrVAhGabcxNNuAqAgAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAMICddgQ6m2LcAUAAICQpiW31dmzZ4N9KIhQSUlJ5s+LrfSZz9LxAAAAADkiX7585j5LR48eNV9+I6VsuZZi18CoZcAj5ZzCscdKg9WRI0ekWLFi3iCfXYQrAAAAhDS9mWzZsmXNfYh2794tkfTFXm8AXLBgQXOOCB4NVnoT54tFuAIAAEDIi46OlmrVqkXU0EC98fRXX30lN9xwAzeeDiL97C+2x8pBuAIAAEBY0KFzMTExEin0C/358+fNORGuIgODOwEAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAABDu4eqrr76STp06SXx8vERFRcnixYsv+JpVq1bJtddeKwUKFJArr7xS5s6dm26bGTNmSKVKlSQmJkYaN24sCQkJOXQGAAAAABAC4erUqVNSr149E4ayYteuXdKxY0e58cYb5ccff5THHntMHnroIVm6dKl3m4ULF8rgwYNl7Nixsn79erP/du3ayZEjR3LwTAAAAADkdvmC+eYdOnQwS1bNmjVLKleuLM8//7x5XLNmTVm9erW88MILJkCpqVOnSp8+faR3797e13z66acyZ84cGTZsWA6dCQAAAIDcLqjhyq01a9ZI69at/dZpqNIeLHX27FlZt26dDB8+3Pt8njx5zGv0tRlJTk42iyMxMdH8ee7cObMgNDnXhmuErKLNwC3aDNygvcAt2kx4cHN9wipcHTp0SEqXLu23Th9rGDp9+rScOHFCUlJSAm6zZcuWDPc7ZcoUGT9+fLr1y5Ytk9jYWItngJywfPnyYB8CwgxtBm7RZuAG7QVu0WZCW1JSUmSGq5yiPV06T8uhYa1ChQrStm1biYuLC+qxIfPfIug/Rm3atJH8+fMH+3AQBmgzcIs2AzdoL3CLNhMenFFtEReuypQpI4cPH/Zbp481ABUsWFDy5s1rlkDb6GszopUHdUlLGzkNPfRxneAWbQZu0WbgBu0FbtFmQpubaxNW97lq0qSJrFy50m+dpn1dr6Kjo6VBgwZ+26SmpprHzjYAAAAAkBOCGq5OnjxpSqrr4pRa15/37NnjHa7Xs2dP7/b9+vWTnTt3ytChQ80cqpkzZ8p7770ngwYN8m6jw/tmz54tb731lmzevFn69+9vSr471QMBAAAAICcEdVjgDz/8YO5Z5XDmPfXq1cvcHPjgwYPeoKW0DLuWVdcw9eKLL0r58uXl9ddf95ZhV926dZOjR4/KmDFjTAGM+vXry5IlS9IVuQAAAACAiAlXLVu2FI/Hk+HzGrACvWbDhg2Z7nfAgAFmAQAAAIBLJazmXAEAAABAqCJcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAERCuJoxY4ZUqlRJYmJipHHjxpKQkJDhtufOnZMJEyZI1apVzfb16tWTJUuW+G0zbtw4iYqK8ltq1KhxCc4EAAAAQG4W1HC1cOFCGTx4sIwdO1bWr19vwlK7du3kyJEjAbcfNWqUvPrqqzJ9+nTZtGmT9OvXT7p27SobNmzw26527dpy8OBB77J69epLdEYAAAAAcqt8wXzzqVOnSp8+faR3797m8axZs+TTTz+VOXPmyLBhw9JtP2/ePBk5cqTcfPPN5nH//v1lxYoV8vzzz8v8+fO92+XLl0/KlCmT5eNITk42iyMxMdHbU6YLQpNzbbhGyCraDNyizcAN2gvcos2EBzfXJ2jh6uzZs7Ju3ToZPny4d12ePHmkdevWsmbNmoCv0QCkwwF9FSxYMF3P1LZt2yQ+Pt5s26RJE5kyZYpcccUVGR6LPj9+/Ph065ctWyaxsbHZODtcSsuXLw/2ISDM0GbgFm0GbtBe4BZtJrQlJSVledsoj8fjkSA4cOCAlCtXTr799lsTgBxDhw6VL7/8Ur777rt0r7nnnnvkp59+ksWLF5t5VytXrpTOnTtLSkqKt+fp888/l5MnT0r16tXNkEANTfv375eNGzdKkSJFstxzVaFCBTl27JjExcXlyPnDzm8R9B+jNm3aSP78+YN9OAgDtBm4RZuBG7QXuEWbCQ+aDUqWLCl//vnnBbNBUIcFuvXiiy+aYYRaoEILVWjA0iGFOozQ0aFDB+/PdevWNUUyKlasKO+99548+OCDAfdboEABs6SljZyGHvq4TnCLNgO3aDNwg/YCt2gzoc3NtQlaQQtNf3nz5pXDhw/7rdfHGc2XKlWqlOm1OnXqlOzevVu2bNkihQsXlipVqmT4PsWKFZOrrrpKtm/fbv0cAAAAACDo4So6OloaNGhghvY5UlNTzWPfYYKB6FwqHVJ4/vx5+fDDD83QwIzoEMEdO3ZI2bJlrR4/AAAAAIRMKXYtwz579mx56623ZPPmzab6n/ZKOdUDe/bs6VfwQudhLVq0SHbu3Clff/21tG/f3gQynaflGDJkiJmz9dtvv5n5XFqqXXvIunfvHpRzBAAAAJA7BHXOVbdu3eTo0aMyZswYOXTokNSvX9/cFLh06dLm+T179pgKgo4zZ86Ye11puNLhgFqSXcuz69A/x759+0yQOn78uBlG2Lx5c1m7dq35GQAAAAByStALWgwYMMAsgaxatcrvcYsWLczNgzOzYMECq8cHAAAAACE/LBAAAAAAIgXhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAkRCuZsyYIZUqVZKYmBhp3LixJCQkZLjtuXPnZMKECVK1alWzfb169WTJkiUXtU8AAAAACPtwtXDhQhk8eLCMHTtW1q9fb8JSu3bt5MiRIwG3HzVqlLz66qsyffp02bRpk/Tr10+6du0qGzZsyPY+AQAAACDsw9XUqVOlT58+0rt3b6lVq5bMmjVLYmNjZc6cOQG3nzdvnowYMUJuvvlmqVKlivTv39/8/Pzzz2d7nwAAAABgQz4JkrNnz8q6detk+PDh3nV58uSR1q1by5o1awK+Jjk52Qz181WwYEFZvXp1tvfp7FcXR2JioncYoi4ITc614Rohq2gzcIs2AzdoL3CLNhMe3FyfoIWrY8eOSUpKipQuXdpvvT7esmVLwNfo8D7tmbrhhhvMvKuVK1fKokWLzH6yu081ZcoUGT9+fLr1y5YtM71eCG3Lly8P9iEgzNBm4BZtBm7QXuAWbSa0JSUlhX64yo4XX3zRDPmrUaOGREVFmYClw/8udsif9nTpPC3fnqsKFSpI27ZtJS4uzsKRI6d+i6D/GLVp00by588f7MNBGKDNwC3aDNygvcAt2kx4cEa1hXS4KlmypOTNm1cOHz7st14flylTJuBrSpUqJYsXL5YzZ87I8ePHJT4+XoYNG2bmX2V3n6pAgQJmSUsbOQ099HGd4BZtBm7RZuAG7QVu0WZCm5trE7SCFtHR0dKgQQMztM+RmppqHjdp0iTT1+q8q3Llysn58+flww8/lM6dO1/0PgEAAADgYgR1WKAOxevVq5c0bNhQGjVqJNOmTZNTp06ZoX6qZ8+eJkTpnCj13Xffyf79+6V+/frmz3HjxpnwNHTo0CzvEwAAAAAiLlx169ZNjh49KmPGjJFDhw6Z0KQ3BXYKUuzZs8dU+3PocEC919XOnTulcOHCpgy7lmcvVqxYlvcJAAAAADkh6AUtBgwYYJZAVq1a5fe4RYsW5ubBF7NPAAAAAIi4mwgDAAAAQKQgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAAAIVrjau3ev7Nu3z/s4ISFBHnvsMXnttddsHBMAAAAA5I5wdc8998gXX3xhfj506JC0adPGBKyRI0fKhAkTbB8jAAAAAERmuNq4caM0atTI/Pzee+/J1VdfLd9++6288847MnfuXNvHCAAAAACRGa7OnTsnBQoUMD+vWLFCbr31VvNzjRo15ODBg3aPEAAAAAAiNVzVrl1bZs2aJV9//bUsX75c2rdvb9YfOHBASpQoYfsYAQAAACAyw9XTTz8tr776qrRs2VK6d+8u9erVM+s//vhj73BBAAAAAMhN8mXnRRqqjh07JomJiVK8eHHv+r59+0psbKzN4wMAAACAyO25On36tCQnJ3uD1e7du2XatGmydetWufzyy20fIwAAAABEZrjq3LmzvP322+bnP/74Qxo3bizPP/+8dOnSRV555RXbxwgAAAAAkRmu1q9fL9dff735+YMPPpDSpUub3isNXC+99JLtYwQAAACAyAxXSUlJUqRIEfPzsmXL5LbbbpM8efLI3/72NxOyAAAAACC3yVa4uvLKK2Xx4sWyd+9eWbp0qbRt29asP3LkiMTFxdk+RgAAAACIzHA1ZswYGTJkiFSqVMmUXm/SpIm3F+uaa66xfYwAAAAAEJml2O+44w5p3ry5HDx40HuPK9WqVSvp2rWrzeMDAAAAgMgNV6pMmTJm2bdvn3lcvnx5biAMAAAAINfK1rDA1NRUmTBhghQtWlQqVqxolmLFisnEiRPNcwAAAACQ22Sr52rkyJHyxhtvyD//+U9p1qyZWbd69WoZN26cnDlzRiZNmmT7OAEAAAAg8sLVW2+9Ja+//rrceuut3nV169aVcuXKyT/+8Q/CFQAAAIBcJ1vDAn///XepUaNGuvW6Tp8DAAAAgNwmW+FKKwS+/PLL6dbrOu3BAgAAAIDcJlvDAp955hnp2LGjrFixwnuPqzVr1pibCn/22We2jxEAAAAAIrPnqkWLFvLrr7+ae1r98ccfZrntttvkf//7n8ybN8/+UQIAAABApN7nKj4+Pl3hip9++slUEXzttddsHBsAAAAARHbPFQAAAADAH+EKAAAAACwgXAEAAADApZ5zpUUrMqOFLQAAAAAgN3IVrooWLXrB53v27HmxxwQAAAAAkR2u3nzzzZw7EgAAAAAIY8y5AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAAJEQrmbMmCGVKlWSmJgYady4sSQkJGS6/bRp06R69epSsGBBqVChggwaNEjOnDnjfX7cuHESFRXlt9SoUeMSnAkAAACA3MzVfa5sW7hwoQwePFhmzZplgpUGp3bt2snWrVvl8ssvT7f9u+++K8OGDZM5c+ZI06ZN5ddff5X777/fBKipU6d6t6tdu7asWLHC+zhfvqCeJgAAAIBcIKg9VxqI+vTpI71795ZatWqZkBUbG2vCUyDffvutNGvWTO655x7T29W2bVvp3r17ut4uDVNlypTxLiVLlrxEZwQAAAAgtwpal87Zs2dl3bp1Mnz4cO+6PHnySOvWrWXNmjUBX6O9VfPnzzdhqlGjRrJz50757LPPpEePHn7bbdu2TeLj481QwyZNmsiUKVPkiiuuyPBYkpOTzeJITEw0f547d84sCE3OteEaIatoM3CLNgM3aC9wizYTHtxcn6CFq2PHjklKSoqULl3ab70+3rJlS8DXaI+Vvq558+bi8Xjk/Pnz0q9fPxkxYoR3Gx1eOHfuXDMv6+DBgzJ+/Hi5/vrrZePGjVKkSJGA+9XwpdultWzZMtOThtC2fPnyYB8CwgxtBm7RZuAG7QVu0WZCW1JSUpa3DavJSKtWrZLJkyfLzJkzTYjavn27DBw4UCZOnCijR48223To0MG7fd26dc12FStWlPfee08efPDBgPvV3jOd++Xbc6XFMnTYYVxc3CU4M2T3twj6j1GbNm0kf/78wT4chAHaDNyizcAN2gvcos2EB2dUW0iHK50HlTdvXjl8+LDfen2s86QC0QClQwAfeugh87hOnTpy6tQp6du3r4wcOdIMK0yrWLFictVVV5kglpECBQqYJS1t5DT00Md1glu0GbhFm4EbtBe4RZsJbW6uTdAKWkRHR0uDBg1k5cqV3nWpqanmsc6TyqhLLm2A0oCmdJhgICdPnpQdO3ZI2bJlrR4/AAAAAITMsEAditerVy9p2LChKVChpdi1J0qrB6qePXtKuXLlzJwo1alTJ1Nh8JprrvEOC9TeLF3vhKwhQ4aYxzoU8MCBAzJ27FjznFYVBAAAAICIDFfdunWTo0ePypgxY+TQoUNSv359WbJkibfIxZ49e/x6qkaNGmXuaaV/7t+/X0qVKmWC1KRJk7zb7Nu3zwSp48ePm+e1+MXatWvNzwAAAACQU4Je0GLAgAFmyaiARdr7V2lPlC4ZWbBggfVjBAAAAICQvokwAAAAAEQKwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAABAJISrGTNmSKVKlSQmJkYaN24sCQkJmW4/bdo0qV69uhQsWFAqVKgggwYNkjNnzlzUPgEAAAAgrMPVwoULZfDgwTJ27FhZv3691KtXT9q1aydHjhwJuP27774rw4YNM9tv3rxZ3njjDbOPESNGZHufAAAAABD24Wrq1KnSp08f6d27t9SqVUtmzZolsbGxMmfOnIDbf/vtt9KsWTO55557TM9U27ZtpXv37n49U273CQAAAAA25JMgOXv2rKxbt06GDx/uXZcnTx5p3bq1rFmzJuBrmjZtKvPnzzdhqlGjRrJz50757LPPpEePHtnep0pOTjaLIzEx0fx57tw5syA0OdeGa4Ssos3ALdoM3KC9wC3aTHhwc32CFq6OHTsmKSkpUrp0ab/1+njLli0BX6M9Vvq65s2bi8fjkfPnz0u/fv28wwKzs081ZcoUGT9+fLr1y5YtM71eCG3Lly8P9iEgzNBm4BZtBm7QXuAWbSa0JSUlhX64yo5Vq1bJ5MmTZebMmaZQxfbt22XgwIEyceJEGT16dLb3qz1dOk/Lt+dKi2XosMO4uDhLR4+c+C2C/mPUpk0byZ8/f7APB2GANgO3aDNwg/YCt2gz4cEZ1RbS4apkyZKSN29eOXz4sN96fVymTJmAr9EApUMAH3roIfO4Tp06curUKenbt6+MHDkyW/tUBQoUMEta2shp6KGP6wS3aDNwizYDN2gvcIs2E9rcXJugFbSIjo6WBg0ayMqVK73rUlNTzeMmTZpk2CWnc6h8aZhSOkwwO/sEAAAAABuCOixQh+L16tVLGjZsaApU6D2stCdKK/2pnj17Srly5cycKNWpUydTDfCaa67xDgvU3ixd74SsC+0TAAAAACIuXHXr1k2OHj0qY8aMkUOHDkn9+vVlyZIl3oIUe/bs8eupGjVqlERFRZk/9+/fL6VKlTLBatKkSVneJwAAAADkhKAXtBgwYIBZMipg4Stfvnzm5sC6ZHefAAAAABBxNxEGAAAAgEhBuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACIlHA1Y8YMqVSpksTExEjjxo0lISEhw21btmwpUVFR6ZaOHTt6t7n//vvTPd++fftLdDYAAAAAcqN8wT6AhQsXyuDBg2XWrFkmWE2bNk3atWsnW7dulcsvvzzd9osWLZKzZ896Hx8/flzq1asnd955p992GqbefPNN7+MCBQrk8JkAAAAAyM2C3nM1depU6dOnj/Tu3Vtq1aplQlZsbKzMmTMn4PaXXXaZlClTxrssX77cbJ82XGmY8t2uePHil+iMAAAAAORGQe250h6odevWyfDhw73r8uTJI61bt5Y1a9ZkaR9vvPGG3H333VKoUCG/9atWrTI9XxqqbrrpJnnqqaekRIkSAfeRnJxsFkdiYqL589y5c2ZBaHKuDdcIWUWbgVu0GbhBe4FbtJnw4Ob6BDVcHTt2TFJSUqR06dJ+6/Xxli1bLvh6nZu1ceNGE7DSDgm87bbbpHLlyrJjxw4ZMWKEdOjQwQS2vHnzptvPlClTZPz48enWL1u2zPSKIbRp7yXgBm0GbtFm4AbtBW7RZkJbUlJS+My5uhgaqurUqSONGjXyW689WQ59vm7dulK1alXTm9WqVat0+9GeM5335dtzVaFCBWnbtq3ExcXl8FngYn6LoP8YtWnTRvLnzx/sw0EYoM3ALdoM3KC9wC3aTHhwRrWFfLgqWbKk6Uk6fPiw33p9rPOkMnPq1ClZsGCBTJgw4YLvU6VKFfNe27dvDxiudH5WoIIX2shp6KGP6wS3aDNwizYDN2gvcIs2E9rcXJugFrSIjo6WBg0ayMqVK73rUlNTzeMmTZpk+tr333/fzJO67777Lvg++/btM1UFy5Yta+W4AQAAACDkqgXqcLzZs2fLW2+9JZs3b5b+/fubXimtHqh69uzpV/DCd0hgly5d0hWpOHnypDzxxBOydu1a+e2330xQ69y5s1x55ZWmxDsAAAAA5ISgz7nq1q2bHD16VMaMGSOHDh2S+vXry5IlS7xFLvbs2WMqCPrSe2CtXr3aFJxIS4cZ/vzzzyas/fHHHxIfH2/mTk2cOJF7XQEAAACI3HClBgwYYJZAtAhFWtWrVxePxxNw+4IFC8rSpUutHyMAAAAAhPSwQAAAAACIBIQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAACwhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAgEgJVzNmzJBKlSpJTEyMNG7cWBISEjLctmXLlhIVFZVu6dixo3cbj8cjY8aMkbJly0rBggWldevWsm3btkt0NgAAAAByo6CHq4ULF8rgwYNl7Nixsn79eqlXr560a9dOjhw5EnD7RYsWycGDB73Lxo0bJW/evHLnnXd6t3nmmWfkpZdeklmzZsl3330nhQoVMvs8c+bMJTwzAAAAALlJ0MPV1KlTpU+fPtK7d2+pVauWCUSxsbEyZ86cgNtfdtllUqZMGe+yfPlys70TrrTXatq0aTJq1Cjp3Lmz1K1bV95++205cOCALF68+BKfHQAAAIDcIl8w3/zs2bOybt06GT58uHddnjx5zDC+NWvWZGkfb7zxhtx9992md0rt2rVLDh06ZPbhKFq0qBluqPvUbdNKTk42iyMxMdH8ee7cObMgNDnXhmuErKLNwC3aDNygvcAt2kx4cHN9ghqujh07JikpKVK6dGm/9fp4y5YtF3y9zs3SYYEasBwarJx9pN2n81xaU6ZMkfHjx6dbv2zZMtMrhtCmvZeAG7QZuEWbgRu0F7hFmwltSUlJ4RGuLpaGqjp16kijRo0uaj/ac6bzvnx7ripUqCBt27aVuLg4C0eKnPotgv5j1KZNG8mfP3+wDwdhgDYDt2gzcIP2ArdoM+HBGdUW8uGqZMmSphjF4cOH/dbrY51PlZlTp07JggULZMKECX7rndfpPrRaoO8+69evH3BfBQoUMEta2shp6KGP6wS3aDNwizYDN2gvcIs2E9rcXJugFrSIjo6WBg0ayMqVK73rUlNTzeMmTZpk+tr333/fzJO67777/NZXrlzZBCzffWra1KqBF9onAAAAAGRX0IcF6nC8Xr16ScOGDc3wPq30p71SWj1Q9ezZU8qVK2fmRaUdEtilSxcpUaKE33q959Vjjz0mTz31lFSrVs2ErdGjR0t8fLzZHgAAAAAiMlx169ZNjh49am76qwUndOjekiVLvAUp9uzZYyoI+tq6dausXr3aFJwIZOjQoSag9e3bV/744w9p3ry52afepBgAAAAAIjJcqQEDBpglkFWrVqVbV716dXM/q4xo75XOxUo7HwsAAAAAIvYmwgAAAAAQCQhXAAAAAGAB4QoAAAAALCBcAQAAAIAFhCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAAALCAcAUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAAC/LZ2Emk8Xg85s/ExMRgHwoyce7cOUlKSjLXKX/+/ME+HIQB2gzcos3ADdoL3KLNhAcnEzgZITOEqwD++usv82eFChWCfSgAAAAAQiQjFC1aNNNtojxZiWC5TGpqqhw4cECKFCkiUVFRwT4cZPJbBA3Ae/fulbi4uGAfDsIAbQZu0WbgBu0FbtFmwoPGJQ1W8fHxkidP5rOq6LkKQD+08uXLB/swkEX6jxH/IMEN2gzcos3ADdoL3KLNhL4L9Vg5KGgBAAAAABYQrgAAAADAAsIVwlaBAgVk7Nix5k8gK2gzcIs2AzdoL3CLNhN5KGgBAAAAABbQcwUAAAAAFhCuAAAAAMACwhUAAAAAWEC4AgAAAAALCFcIGTNmzJBKlSpJTEyMNG7cWBISEjLc9ty5czJhwgSpWrWq2b5evXqyZMmSdNvt379f7rvvPilRooQULFhQ6tSpIz/88EMOnwnCtc2kpKTI6NGjpXLlyqa96LYTJ040d2ZH+Pvqq6+kU6dOEh8fL1FRUbJ48eILvmbVqlVy7bXXmkpeV155pcydO/ei2iHCS060mSlTpsh1110nRYoUkcsvv1y6dOkiW7duzcGzQCT8O+P45z//afb72GOPWT5y2EK4QkhYuHChDB482JQjXb9+vfni265dOzly5EjA7UeNGiWvvvqqTJ8+XTZt2iT9+vWTrl27yoYNG7zbnDhxQpo1ayb58+eXzz//3Gz3/PPPS/HixS/hmSGc2szTTz8tr7zyirz88suyefNm8/iZZ54xr0H4O3XqlGknGoayYteuXdKxY0e58cYb5ccffzRfZh566CFZunRpttshwktOtJkvv/xSHn74YVm7dq0sX77c/OKnbdu25r0Q/nKizTi+//578/+xunXr5sCRwxotxQ4EW6NGjTwPP/yw93FKSoonPj7eM2XKlIDbly1b1vPyyy/7rbvttts89957r/fxk08+6WnevHkOHjUirc107NjR88ADD2S6DSKD/u/vo48+ynSboUOHemrXru23rlu3bp527dplux0ifNlqM2kdOXLE7PvLL7+0dqyIvDbz119/eapVq+ZZvny5p0WLFp6BAwfmyDHj4tFzhaA7e/asrFu3Tlq3bu1dlydPHvN4zZo1AV+TnJxshuD40mFcq1ev9j7++OOPpWHDhnLnnXeaoRfXXHONzJ49OwfPBOHeZpo2bSorV66UX3/91Tz+6aefzPMdOnTIsXNB6NK25NvGlPZKOW0sO+0QubvNBPLnn3+aPy+77LIcPz6Eb5vR3k7t4Uq7LUIP4QpBd+zYMTPXpXTp0n7r9fGhQ4cCvkb/4Zk6daps27ZNUlNTzdCKRYsWycGDB73b7Ny50wzxqlatmule79+/vzz66KPy1ltv5fg5ITzbzLBhw+Tuu++WGjVqmOGkGsh1iMa9996b4+eE0KNtKVAbS0xMlNOnT2erHSJ3t5m09N8i/TdGh7BfffXVl/BIEU5tZsGCBWbYsc7XQ+gjXCEsvfjiiyY06Zfg6OhoGTBggPTu3dv81tj3f1o6QXTy5MnmS3Lfvn2lT58+MmvWrKAeO0K3zbz33nvyzjvvyLvvvmv+R6ZB/LnnniOQA8gR2huxceNG8+UZCGTv3r0ycOBA8/+mtKMvEJoIVwi6kiVLSt68eeXw4cN+6/VxmTJlAr6mVKlSpgKPThzdvXu3bNmyRQoXLixVqlTxblO2bFmpVauW3+tq1qwpe/bsyaEzQbi3mSeeeMLbe6WVJXv06CGDBg3it4W5lLalQG0sLi7ODCnNTjtE7m4zvvQXPJ988ol88cUXUr58+Ut8pAiXNqNDj7VAjv6yOF++fGbRoigvvfSS+Vl7zxFaCFcIOu1FaNCggZnr4tvrpI+bNGmS6Wv1tzjlypWT8+fPy4cffiidO3f2PqfDLNKWt9W5NBUrVsyBs0AktJmkpCS/niylX55138h9tC35tjGlw0mdNnYx7RC5s80orXOgweqjjz6S//73v+bWD8i9LtRmWrVqJb/88oupJOgsOp9ch6vrz/r/KIQYC0UxgIu2YMECT4ECBTxz5871bNq0ydO3b19PsWLFPIcOHTLP9+jRwzNs2DDv9mvXrvV8+OGHnh07dni++uorz0033eSpXLmy58SJE95tEhISPPny5fNMmjTJs23bNs8777zjiY2N9cyfPz8o54jQbzO9evXylCtXzvPJJ594du3a5Vm0aJGnZMmSppoTwp9W29qwYYNZ9H9/U6dONT/v3r3bPK/tRduNY+fOnebfjCeeeMKzefNmz4wZMzx58+b1LFmyJMvtEOEtJ9pM//79PUWLFvWsWrXKc/DgQe+SlJQUlHNE6LeZtKgWGNoIVwgZ06dP91xxxRWe6OhoU95Yvwz7/kOiX3wd+j+lmjVrmi81JUqUMP9Q7d+/P90+//Of/3iuvvpqs12NGjU8r7322iU7H4Rfm0lMTDT/w9J9xsTEeKpUqeIZOXKkJzk5+ZKeF3LGF198Yb7spF2cdqJ/artJ+5r69eubNqbt4c0333TVDhHecqLNBNqfLoHaFsJPTv0744twFdqi9D/B7j0DAAAAgHDHnCsAAAAAsIBwBQAAAAAWEK4AAAAAwALCFQAAAABYQLgCAAAAAAsIVwAAAABgAeEKAAAAACwgXAEAAACABYQrAAAsi4qKksWLFwf7MAAAlxjhCgAQUe6//34TbtIu7du3D/ahAQAiXL5gHwAAALZpkHrzzTf91hUoUCBoxwMAyB3ouQIARBwNUmXKlPFbihcvbp7TXqxXXnlFOnToIAULFpQqVarIBx984Pf6X375RW666SbzfIkSJaRv375y8uRJv23mzJkjtWvXNu9VtmxZGTBggN/zx44dk65du0psbKxUq1ZNPv7440tw5gCAYCJcAQByndGjR8vtt98uP/30k9x7771y9913y+bNm81zp06dknbt2pkw9v3338v7778vK1as8AtPGs4efvhhE7o0iGlwuvLKK/3eY/z48XLXXXfJzz//LDfffLN5n99///2SnysA4NKJ8ng8nkv4fgAA5Picq/nz50tMTIzf+hEjRphFe6769etnApLjb3/7m1x77bUyc+ZMmT17tjz55JOyd+9eKVSokHn+s88+k06dOsmBAwekdOnSUq5cOendu7c89dRTAY9B32PUqFEyceJEb2ArXLiwfP7558z9AoAIxpwrAEDEufHGG/3Ck7rsssu8Pzdp0sTvOX38448/mp+1B6tevXreYKWaNWsmqampsnXrVhOcNGS1atUq02OoW7eu92fdV1xcnBw5cuSizw0AELoIVwCAiKNhJu0wPVt0HlZW5M+f3++xhjINaACAyMWcKwBArrN27dp0j2vWrGl+1j91LpYO5XN88803kidPHqlevboUKVJEKlWqJCtXrrzkxw0ACG30XAEAIk5ycrIcOnTIb12+fPmkZMmS5mctUtGwYUNp3ry5vPPOO5KQkCBvvPGGeU4LT4wdO1Z69eol48aNk6NHj8ojjzwiPXr0MPOtlK7XeVuXX365qTr4119/mQCm2wEAci/CFQAg4ixZssSUR/elvU5btmzxVvJbsGCB/OMf/zDb/etf/5JatWqZ57R0+tKlS2XgwIFy3XXXmcdaWXDq1KnefWnwOnPmjLzwwgsyZMgQE9ruuOOOS3yWAIBQQ7VAAECuonOfPvroI+nSpUuwDwUAEGGYcwUAAAAAFhCuAAAAAMAC5lwBAHIVRsMDAHIKPVcAAAAAYAHhCgAAAAAsIFwBAAAAgAWEKwAAAACwgHAFAAAAABYQrgAAAADAAsIVAAAAAFhAuAIAAAAAuXj/DxRAHqVFfHEIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "loss_curve_path = os.path.join(\"..\", \"results\", \"dinov2\", \"loss_curves.png\")\n",
    "plt.savefig(loss_curve_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6996, Test Accuracy: 72.03%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.75      0.64      0.69       140\n",
      "       happy       0.70      0.79      0.74       148\n",
      "     relaxed       0.70      0.58      0.64       146\n",
      "         sad       0.74      0.87      0.80       138\n",
      "\n",
      "    accuracy                           0.72       572\n",
      "   macro avg       0.72      0.72      0.72       572\n",
      "weighted avg       0.72      0.72      0.72       572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_targets = []\n",
    "test_loss = 0.0\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in FULL_TEST_LOADER:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = final_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "test_loss /= len(FULL_TEST_LOADER.dataset)\n",
    "test_accuracy = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "logging.info(\"Test Loss: %.4f, Test Accuracy: %.2f%%\", test_loss, test_accuracy * 100)\n",
    "\n",
    "# plot classification report\n",
    "class_names = FULL_TEST_LOADER.dataset.classes\n",
    "report = classification_report(all_targets, all_preds, target_names=class_names)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "logging.info(\"Classification Report:\\n%s\", report)\n",
    "\n",
    "# plot confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "# save confusion matrix as image\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "\n",
    "conf_matrix_path = os.path.join(\"..\", \"results\", \"dinov2\", \"confusion_matrix.png\")\n",
    "fig.savefig(conf_matrix_path)\n",
    "plt.close(fig)\n",
    "\n",
    "logging.info(\"Confusion matrix saved to: %s\", conf_matrix_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dog_emotions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
