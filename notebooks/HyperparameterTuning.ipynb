{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters in deep learning to tune are \n",
    "- the number of neurons\n",
    "- activation function\n",
    "- optimiser\n",
    "- learning rate\n",
    "- batch size\n",
    "- epochs \n",
    "- number of layers.\n",
    "\n",
    "\n",
    "Reference: \n",
    "- https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported. Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple, List, Optional, Callable\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision.models import efficientnet_b5, EfficientNet_B5_Weights\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# determine the project root - required to import DataHandler from utils folder\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "from utils.DataHandler import download_dataset, create_full_data_loaders, create_tuning_data_loaders\n",
    "\n",
    "print(\"Libraries imported. Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "This is adapted from SeparatingData.ipynb\n",
    "Download the processed dataset from Google Drive if yet to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at ../input/final_split_training_augmented\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../input/final_split_training_augmented\"\n",
    "ZIP_URL = \"https://drive.google.com/uc?id=11t8m703wcNss3w5diJSUGBA_vXnCKChr\"\n",
    "ZIP_FILENAME = \"../input/final_split.zip\"\n",
    "ROOT_DIR = \"../input\"\n",
    "\n",
    "download_dataset(DATA_DIR, ZIP_URL, ZIP_FILENAME, ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Data Load\n",
    "For faster hyperparameter tuning, use a subset of the dataset to find the most optimised set of hyperparameters.\n",
    "Loads dataset from processed dataset which should have been split to train, test, eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-split datasets: train 8025, val 579, test 572\n",
      "Created tuning data loaders with subset fraction: 0.1\n",
      "Created subset datasets for hyperparameter tuning: train 802, val 57, test 57\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATASET = os.path.abspath(\"../input/final_split_training_augmented\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Define a transform. \n",
    "# In this notebook, we assume we doing it for EfficientNetB5, so we resize to ~456x456\n",
    "weights = EfficientNet_B5_Weights.DEFAULT\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((456, 456)),\n",
    "        transforms.ToTensor(),\n",
    "        weights.transforms()  # applies normalisation as required\n",
    "    ])\n",
    "\n",
    "TRAIN_LOADER, VAL_LOADER, TEST_LOADER = create_tuning_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset_fraction=0.1,\n",
    "    random_seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data Load\n",
    "When using the function for the entire training, not used in hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-split datasets: train 8025, val 579, test 572\n"
     ]
    }
   ],
   "source": [
    "FULL_TRAIN_LOADER, FULL_VAL_LOADER, FULL_TEST_LOADER = create_full_data_loaders(\n",
    "    dataset_root=SPLIT_DATASET,\n",
    "    transform=transform,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specifications\n",
    "This is where you should replace with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EfficientNetB5 Partial Transfer Learning:\n",
    "- https://discuss.pytorch.org/t/partial-transfer-learning-efficientnet/109689 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEfficientNetB5(nn.Module):\n",
    "    \"\"\"EfficientNetB5 model for transfer learning on the dog emotion dataset\n",
    "    with a configurable classification head for hyperparameter tuning, \n",
    "    i.e parameters you wish to tune need to be specified\n",
    "\n",
    "    This model uses a pretrained EfficientNetB5 backbone and replaces its\n",
    "    classifier with a multi-layer fully connected network whose architecture\n",
    "    can be tuned (number of layers, neurons, and activation function)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_classes: int = 4,\n",
    "                 dropout: float = 0.2,\n",
    "                 freeze_backbone: bool = False,\n",
    "                 hidden_sizes: Optional[List[int]] = None,\n",
    "                 activation: str = 'relu') -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of output classes.\n",
    "            dropout (float): Dropout rate to apply in the classifier.\n",
    "            freeze_backbone (bool): If True, freeze the backbone layers.\n",
    "            hidden_sizes (Optional[List[int]]): List of sizes for hidden layers in the classifier.\n",
    "                If None, a single linear layer is used.\n",
    "            activation (str): Activation function to use in the classifier ('relu', 'tanh', etc.).\n",
    "        \"\"\"\n",
    "        super(BaseEfficientNetB5, self).__init__()\n",
    "        weights = EfficientNet_B5_Weights.DEFAULT\n",
    "        self.backbone = efficientnet_b5(weights=weights)\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        \n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.features.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Build the classifier based on the provided hidden_sizes\n",
    "        layers = []\n",
    "        input_dim = in_features\n",
    "        if hidden_sizes:\n",
    "            for hidden_dim in hidden_sizes:\n",
    "                layers.append(nn.Dropout(p=dropout))\n",
    "                layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                layers.append(self._get_activation(activation))\n",
    "                input_dim = hidden_dim\n",
    "            # final classification layer.\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(nn.Linear(input_dim, num_classes))\n",
    "        else:\n",
    "            # single linear layer if no hidden layers specified\n",
    "            layers.append(nn.Dropout(p=dropout))\n",
    "            layers.append(nn.Linear(input_dim, num_classes))\n",
    "        \n",
    "        self.backbone.classifier[1] = nn.Sequential(*layers)\n",
    "    \n",
    "    def _get_activation(self, activation: str) -> Callable:\n",
    "        \"\"\"Returns an activation function based on the given string.\n",
    "\n",
    "        Args:\n",
    "            activation (str): Name of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            Callable: Activation function module.\n",
    "        \"\"\"\n",
    "        if activation.lower() == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation.lower() == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        elif activation.lower() == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model instantiated: BaseEfficientNetB5\n"
     ]
    }
   ],
   "source": [
    "model = BaseEfficientNetB5(num_classes=4, dropout=0.3, freeze_backbone=True, hidden_sizes=[256, 128], activation='relu')\n",
    "print(\"Model instantiated:\", model.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "This is the part where you write the training function and load it to the ray tune scheduler.\n",
    "For this execution, ASHAscheduler is used with Optuna for bayesian optimisation techniques - which should be using the default Tree-Structured Parzen Estimator.\n",
    "If many parameters, this is would be more efficient than grid search and random search.\n",
    "\n",
    "References:\n",
    "- https://docs.ray.io/en/latest/tune/examples/includes/async_hyperband_example.html\n",
    "- https://docs.ray.io/en/latest/tune/examples/tune-pytorch-cifar.html \n",
    "- https://docs.ray.io/en/latest/tune/examples/includes/mnist_pytorch.html\n",
    "- https://docs.ray.io/en/latest/tune/api/suggestion.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "\n",
    "CHECKPOINT_DIR = os.path.abspath(\"../models/hyptune\")\n",
    "def train_model(config, checkpoint_dir=CHECKPOINT_DIR, data_dir=None):\n",
    "    \"\"\"Training function for Ray Tune hyperparameter tuning.\n",
    "\n",
    "    This function instantiates the model with hyperparameters\n",
    "    specified in the config dictionary, trains the model on the global TRAIN_LOADER,\n",
    "    evaluates on VAL_LOADER, and reports the validation loss to Ray Tune.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Hyperparameter configuration. Expected keys include:\n",
    "            - lr (float): Learning rate.\n",
    "            - weight_decay (float): Weight decay for the optimizer.\n",
    "            - dropout (float): Dropout rate for the classifier.\n",
    "            - hidden_sizes (list or None): List of hidden layer sizes in the classifier.\n",
    "            - activation (str): Activation function to use ('relu', 'tanh', etc.).\n",
    "            - freeze_backbone (bool): Whether to freeze the model backbone.\n",
    "            - num_epochs (int): Number of training epochs.\n",
    "            - optimiser (callable, optional): Optimiser class. Default is optim.Adam.\n",
    "            - criterion (callable, optional): Loss function instance. Default is nn.CrossEntropyLoss().\n",
    "        checkpoint_dir (str, optional): Directory for checkpointing (if applicable).\n",
    "        data_dir (str, optional): Not used here; included for compatibility.\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    if checkpoint_dir:\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        print(f\"Checkpoint Folder exists\")\n",
    "    \n",
    "    # instantiate model with hyperparameters from config\n",
    "    model = BaseEfficientNetB5(\n",
    "        num_classes=4,\n",
    "        dropout=config.get(\"dropout\", 0.2),\n",
    "        freeze_backbone=config.get(\"freeze_backbone\", True),\n",
    "        hidden_sizes=config.get(\"hidden_sizes\", None),\n",
    "        activation=config.get(\"activation\", \"relu\")\n",
    "    ).to(device)\n",
    "    \n",
    "    optimiser_class = config.get(\"optimiser\", optim.Adam)\n",
    "    optimiser = optimiser_class(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "    criterion = config.get(\"criterion\", nn.CrossEntropyLoss())\n",
    "\n",
    "    num_epochs = config.get(\"num_epochs\", 2)  # a low number for quick tuning, but update accordingly\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in tqdm(TRAIN_LOADER, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(TRAIN_LOADER.dataset)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # optionally, can checkpoint the model\n",
    "        if checkpoint_dir:\n",
    "            path = os.path.join(checkpoint_dir, f\"checkpoint_{epoch}.pt\")\n",
    "            torch.save(model.state_dict(), path)\n",
    "    \n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in VAL_LOADER:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    avg_val_loss = total_loss / len(VAL_LOADER.dataset)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Report the metric to Ray Tune.\n",
    "    tune.report({\"loss\": avg_val_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-04-13 23:14:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:04.44        </td></tr>\n",
       "<tr><td>Memory:      </td><td>14.5/18.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 90.000: None | Iter 30.000: None | Iter 10.000: None<br>Logical resource usage: 4.0/11 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  dropout</th><th>freeze_backbone  </th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  weight_decay</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_6dd3691e</td><td>RUNNING </td><td>127.0.0.1:87937</td><td style=\"text-align: right;\"> 0.392798</td><td>True             </td><td style=\"text-align: right;\">0.000132929</td><td style=\"text-align: right;\">   0.00635122 </td></tr>\n",
       "<tr><td>train_model_69f46477</td><td>RUNNING </td><td>127.0.0.1:87939</td><td style=\"text-align: right;\"> 0.162398</td><td>True             </td><td style=\"text-align: right;\">0.000625137</td><td style=\"text-align: right;\">   4.20799e-06</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_model pid=87937)\u001b[0m Checkpoint Folder exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/13 [00:00<?, ?it/s]\n",
      "2025-04-13 23:14:23,793\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-04-13 23:14:23,799\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/huiningonn/ray_results/train_model_2025-04-13_23-14-17' in 0.0050s.\n",
      "2025-04-13 23:14:33,843\tINFO tune.py:1041 -- Total run time: 14.51 seconds (4.44 seconds for the tuning loop).\n",
      "2025-04-13 23:14:33,847\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/Users/huiningonn/ray_results/train_model_2025-04-13_23-14-17\", trainable=...)\n",
      "2025-04-13 23:14:33,861\tWARNING experiment_analysis.py:558 -- Could not find best trial. Did you pass the correct `metric` parameter?\n",
      "\u001b[36m(train_model pid=87937)\u001b[0m /Users/huiningonn/anaconda3/envs/dogemotion/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(train_model pid=87937)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "Epoch 1/2:   0%|          | 0/13 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m\n\u001b[1;32m     38\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[1;32m     39\u001b[0m     tune\u001b[38;5;241m.\u001b[39mwith_resources(train_model, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}), \u001b[38;5;66;03m# specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     tune_config\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mTuneConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     49\u001b[0m results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest config:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/dogemotion/lib/python3.10/site-packages/ray/tune/result_grid.py:161\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    150\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "asha_scheduler = ASHAScheduler(\n",
    "    time_attr='training_iteration',\n",
    "    metric='loss',\n",
    "    mode='min',\n",
    "    max_t=100,           # max training iterations per trial\n",
    "    grace_period=10,     # min iterations before stopping\n",
    "    reduction_factor=3,\n",
    "    brackets=1,\n",
    ")\n",
    "\n",
    "optuna_search = OptunaSearch(metric=\"loss\", mode=\"min\", seed=42)\n",
    "\n",
    "# define search space in a config dictionary, i.e what are the values you want to try, this is just example of format\n",
    "'''\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"dropout\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"freeze_backbone\": tune.choice([True, False]),\n",
    "    \"num_epochs\": 2, \n",
    "    \"optimiser\": tune.choice([optim.Adam, optim.SGD]),\n",
    "    \"criterion\": tune.choice([nn.CrossEntropyLoss, nn.NLLLoss]),\n",
    "}\n",
    "'''\n",
    "\n",
    "# this is what i specified for the example because i am running on cpu\n",
    "# config = {\n",
    "#     \"lr\": tune.loguniform(1e-5, 1e-2),\n",
    "#     \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "#     \"dropout\": tune.uniform(0.1, 0.5),\n",
    "#     \"freeze_backbone\": tune.choice([True]),\n",
    "#     \"num_epochs\": 2,\n",
    "# }\n",
    "\n",
    "\n",
    "# for efficientnetb5 gpu\n",
    "config = {\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-2),    \n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-3),\n",
    "    \"dropout\": tune.uniform(0.2, 0.5),\n",
    "    \"hidden_sizes\": tune.choice([[256, 128], [512, 256], None]),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"]),\n",
    "    \"freeze_backbone\": True,\n",
    "    \"num_epochs\": 5,\n",
    "    \"optimiser\": tune.choice([optim.Adam, optim.SGD]),\n",
    "    \"criterion\": tune.choice([nn.CrossEntropyLoss])\n",
    "}\n",
    "\n",
    "# tuner object\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {\"cpu\": 4, \"gpu\": 1}), # specify based on the device u using because by default it uses all, i.e if u have 4 cpus; it does 4 concurrent trials\n",
    "    tune_config=tune.TuneConfig(\n",
    "        scheduler=asha_scheduler,\n",
    "        search_alg=optuna_search,\n",
    "        num_samples=2,  # number of trials to run\n",
    "    ),\n",
    "    run_config=tune.RunConfig(verbose=1),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "print(\"Best config:\", results.get_best_result(metric=\"loss\", mode=\"min\").config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dogemotion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
