{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c109bc7",
   "metadata": {},
   "source": [
    "# Image Dataset Preperation\n",
    "This notebook contains the code for cleaning and preparing the dataset to be used for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572d508",
   "metadata": {},
   "source": [
    "Configurable Parameters:\n",
    "- **root (str)**: Root directory of the project (commonly set to '..').\n",
    "- **data_set_directory (str)**: Name of the directory containing all datasets (i.e.,'input')\n",
    "- **raw_data_set_name (str)**: Name of the raw ZIP file or raw dataset folder (i.e., 'kaggle_raw_data').\n",
    "- **expanded_folder_name (str)**: Name of the expanded folder after extracting raw ZIP data (i.e., 'Dog Emotion').\n",
    "- **final_data_set (str)**: Folder name of the final processed and augmented dataset used for training.\n",
    "- **google_drive_id (str)**: ID used for Google Drive downloads (if applicable).\n",
    "- **log_file_name (str)**: Filename for saving preprocessing logs or debug information.\n",
    "- **split_ratios (dict)**: Dictionary defining the ratios for splitting the dataset into 'train', 'test', and 'eval' subsets.\n",
    "- **seed**: Ensures reproducibility when shuffling or splitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Panel to adjust variables\n",
    "\n",
    "root = \"..\"\n",
    "data_set_directory = \"input\"\n",
    "raw_data_set_name =\"kaggle_raw_data\"\n",
    "expanded_folder_name =\"Dog Emotion\"\n",
    "final_data_set = \"final_split_15Apr2025\"\n",
    "google_drive_id = \"15vCDXS-3GtNHxgL4EczcxMAGQDfVCYe5\"\n",
    "log_file_name = \"Data_preprocessing_no_dogs_detected\"\n",
    "\n",
    "split_ratios = {'train': 0.7, 'test': 0.15, 'eval': 0.15}\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ac537",
   "metadata": {},
   "source": [
    "## Imports\n",
    "Used to handle the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a458e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import numpy as np\n",
    "import gdown\n",
    "import zipfile\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import random\n",
    "from utils.helperFunctions import is_rgb\n",
    "import sys\n",
    "\n",
    "# Determine the project root - required to import DataHandler from utils folder\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), root))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from utils.DataHandler import download_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ba49d",
   "metadata": {},
   "source": [
    "## File Location Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef3ed1",
   "metadata": {},
   "source": [
    "Global Variables Created:\n",
    "- **RAW_IMAGE_ZIP_NAME (str)**: Name of the Zipfile when downloading the unprocessed dataset from the googledrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\input\\kaggle_raw_data\n",
      "..\\input\\Dog Emotion\n",
      "..\\input\\final_split_training_augmented\n",
      "..\\logs/Data_preprocessing_no_dogs_detected.txt\n"
     ]
    }
   ],
   "source": [
    "RAW_IMAGE_ZIP_NAME = os.path.join(root, data_set_directory, raw_data_set_name)\n",
    "print(RAW_IMAGE_ZIP_NAME)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1874b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at Dog Emotion\n"
     ]
    }
   ],
   "source": [
    "# Downloading dataset from kaggle that is on Gdrive\n",
    "download_dataset(expanded_folder_name,f\"https://drive.google.com/uc?id={google_drive_id}\",  F\"{RAW_IMAGE_ZIP_NAME}.zip\", \"../input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5bdb96",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76061ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(root_dir: str, model_location: str, model_name: str, data_root_dir: str, output_data_dir_name: str, raw_data_dir: str, log_file_dir: str, log_file_name: str, split_ratios: dict):\n",
    "    \"\"\"\n",
    "    Function used to handle the creation of the dataset, handles data cleaning, data augmentation and splitting\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): Directory of the root where this file is stored (used in conjunction with)\n",
    "        model_location (str): Directory of the model location in reference to the root_dir\n",
    "        model_name (str): Name of the model that is used for data augmentation (our team used Yolo v8)\n",
    "        data_root_dir (str): Directory of the dataset\n",
    "        output_data_dir_name (str): Directory for the final dataset (Will be created if not available)\n",
    "        raw_data_dir (str): Directory of the base images that are used for dataset creation\n",
    "        log_file_dir (str): Directory for log files\n",
    "        log_file_name (str): Name of the log_file\n",
    "        split_ratios: dict): Dict of the train, eval and test split proportions\n",
    "    \"\"\"\n",
    "    data_folder_root = os.path.join(root_dir, data_root_dir)\n",
    "    input_data_folder = os.path.join(data_folder_root, raw_data_dir)\n",
    "    if not os.path.exists(input_data_folder):\n",
    "        print(\"Please run the code cell above to download and extract the dataset\")\n",
    "    \n",
    "    else:\n",
    "        output_data_folder = os.path.join(data_folder_root, output_data_dir_name)\n",
    "        # Check if the output dict already exists and skips and tells the users that the final_data_dir already exists\n",
    "        if not os.path.exists(output_data_folder):\n",
    "            # Loading the model which will be used to identify areas of interest and perform subject focusing\n",
    "            print(F\"Loading {model_name} model...\")\n",
    "            model = YOLO(os.path.join(root_dir, model_location, model_name))\n",
    "            model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(f\"{model_name} loaded.\\n\")\n",
    "            \n",
    "            # Deleting any existing log file for the data preprocessing so that the data is clean\n",
    "            log_file_location = os.path.join(root_dir, log_file_dir, f\"{log_file_name}.txt\")\n",
    "            if os.path.exists(log_file_location):\n",
    "                os.remove(log_file_location)\n",
    "\n",
    "            split_counts = {'train': 0, 'test': 0, 'eval': 0}\n",
    "            image_pool = []  # List of (cropped_img, class_label)\n",
    "\n",
    "\n",
    "            #  Iterates for each of the subfolders (classes) within the dataset\n",
    "            for subfolder in os.listdir(input_data_folder):\n",
    "                class_path = os.path.join(input_data_folder, subfolder)\n",
    "\n",
    "                #  Skips over non-folder items\n",
    "                if not os.path.isdir(class_path):\n",
    "                    print(f\"Skipping non-folder: {subfolder}\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"Processing class: {subfolder}\")\n",
    "\n",
    "                #  Iterates through each image, finding the area of the subject and focusing there\n",
    "                for filename in os.listdir(class_path):\n",
    "                    if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        continue\n",
    "\n",
    "                    filepath = os.path.join(class_path, filename)\n",
    "\n",
    "                    try:\n",
    "                        # Removing images that are black and white\n",
    "                        if not is_rgb(filepath):\n",
    "                            with open(log_file_location, \"a\") as f:\n",
    "                                f.write(f\"Grayscale or low-color image skipped: {filepath}\\n\")\n",
    "                            print(f\"Skipped grayscale/low-color image: {filename}\")\n",
    "                            continue\n",
    "\n",
    "                        img = Image.open(filepath).convert(\"RGB\")\n",
    "                        results = model(filepath, conf=0.15)[0]\n",
    "\n",
    "                        for box in results.boxes:\n",
    "                            if int(box.cls) == 16:  # Dog class\n",
    "                                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                                cropped = img.crop((x1, y1, x2, y2)).resize((224, 224))\n",
    "                                image_pool.append((cropped, subfolder, filename))\n",
    "                                print(f\"Cropped dog from {filename} [{subfolder}]\")\n",
    "                                break\n",
    "                        else:\n",
    "                            with open(log_file_location, \"a\") as f:\n",
    "                                f.write(f\"{filepath}\\n\")\n",
    "                            print(f\"No dog detected in {filename}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {filepath}: {e}\")\n",
    "\n",
    "            # Split and save data according to proportions\n",
    "            random.shuffle(image_pool)\n",
    "            total = len(image_pool)\n",
    "            n_train = int(total * split_ratios['train'])\n",
    "            n_test = int(total * split_ratios['test'])\n",
    "\n",
    "            splits = {\n",
    "                'train': image_pool[:n_train],\n",
    "                'test': image_pool[n_train:n_train + n_test],\n",
    "                'eval': image_pool[n_train + n_test:]\n",
    "            }\n",
    "\n",
    "            for split, items in splits.items():\n",
    "                for img, label, original_filename in items:\n",
    "                    save_dir = os.path.join(output_data_folder, split, label)\n",
    "                    os.makedirs(save_dir, exist_ok=True)\n",
    "                    save_path = os.path.join(save_dir, original_filename)\n",
    "                    img.save(save_path)\n",
    "                    split_counts[split] += 1\n",
    "\n",
    "            # Summary\n",
    "            print(\"\\nAll-in-One Summary:\")\n",
    "            print(f\"Total cropped dog images : {total}\")\n",
    "            for split in split_counts:\n",
    "                print(f\"{split.capitalize()} images            : {split_counts[split]}\")\n",
    "            print(f\"Final dataset saved to: {output_data_folder}\")\n",
    "            print(f\"No-dog log file saved at: {log_file_location}\")\n",
    "\n",
    "        \n",
    "        else: \n",
    "            print(f\"Dataset already exists at {output_data_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d697a509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. input Dog Emotion final_split_training_augmented Data_preprocessing_no_dogs_detected\n",
      "Dataset already exists at ..\\input\\final_split_training_augmented\n"
     ]
    }
   ],
   "source": [
    "print(root, data_set_directory, expanded_folder_name, final_data_set, log_file_name)\n",
    "create_dataset(root, \"models\", \"yolov8x.pt\",data_set_directory, final_data_set,expanded_folder_name, \"logs\", log_file_name, split_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe2b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADLTEAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
